{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e42b10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== 2×GPU threaded pipeline (notebook-friendly) ==================\n",
    "import os, json, torch, threading, queue, gc\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# ---- You already have these; included for completeness ----\n",
    "def entropy_loss(batch_logits):\n",
    "    import torch.nn.functional as F\n",
    "    log_probs = F.log_softmax(batch_logits, dim=-1)\n",
    "    probs = log_probs.exp()\n",
    "    entropy = -(probs * log_probs).sum(dim=-1)\n",
    "    return entropy.mean()\n",
    "\n",
    "from torch.cuda.amp import autocast as cuda_autocast\n",
    "\n",
    "@torch.no_grad()\n",
    "def _one_step(model, embeddings, attention_mask, amp_dtype=torch.float16):\n",
    "    with cuda_autocast(dtype=amp_dtype):\n",
    "        out = model(inputs_embeds=embeddings, attention_mask=attention_mask)\n",
    "        logits = out.logits[:, -1, :]\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "    return probs\n",
    "\n",
    "def compute_loss(\n",
    "    model, emb_layer, embeddings, attention_mask, loss_fn,\n",
    "    n_tokens=10, amp_dtype=torch.float16, track_last_only=True\n",
    "):\n",
    "    B, L, E = embeddings.shape\n",
    "    dev = embeddings.device\n",
    "    for _ in range(max(0, n_tokens - 1)):\n",
    "        probs = _one_step(model, embeddings, attention_mask, amp_dtype)\n",
    "        w = emb_layer.weight.to(dev)\n",
    "        probs = probs.to(w.dtype)\n",
    "        next_embeds = probs @ w\n",
    "        embeddings = torch.cat([embeddings, next_embeds.unsqueeze(1)], dim=1)\n",
    "        attention_mask = torch.cat(\n",
    "            [attention_mask, torch.ones((B, 1), dtype=attention_mask.dtype, device=dev)], dim=1\n",
    "        )\n",
    "\n",
    "    with cuda_autocast(dtype=amp_dtype):\n",
    "        out = model(inputs_embeds=embeddings, attention_mask=attention_mask)\n",
    "        logits = out.logits[:, -1, :]\n",
    "        loss = loss_fn(logits) if track_last_only else 0.0\n",
    "    return loss\n",
    "\n",
    "# ---- Model/Tokenizer loader pinned to a single GPU ----\n",
    "def load_model_single_device(model_dir: str, device_id: int):\n",
    "    try:\n",
    "        from peft import AutoPeftModelForCausalLM, PeftModel\n",
    "        HAS_PEFT = True\n",
    "    except Exception:\n",
    "        HAS_PEFT = False\n",
    "\n",
    "    kw = dict(\n",
    "        trust_remote_code=True,\n",
    "        local_files_only=True,\n",
    "        torch_dtype=torch.float16,\n",
    "        low_cpu_mem_usage=True,\n",
    "        offload_state_dict=True,\n",
    "        attn_implementation=\"sdpa\",\n",
    "        device_map={\"\": device_id},  # single GPU\n",
    "    )\n",
    "\n",
    "    with open(os.path.join(model_dir, \"reduced-config.json\"), \"r\") as fh:\n",
    "        cfg = json.load(fh)\n",
    "\n",
    "    if cfg.get(\"use_lora\", False) and HAS_PEFT:\n",
    "        lora_dir = os.path.join(model_dir, \"fine-tuned-model\")\n",
    "        model = AutoPeftModelForCausalLM.from_pretrained(lora_dir, **kw)\n",
    "        if hasattr(model, \"merge_and_unload\"):\n",
    "            model = model.merge_and_unload()\n",
    "    elif cfg.get(\"use_lora\", False):\n",
    "        base_dir = os.path.join(model_dir, \"base-model\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(base_dir, **kw)\n",
    "        lora_dir = os.path.join(model_dir, \"fine-tuned-model\")\n",
    "        try:\n",
    "            from peft import PeftModel\n",
    "            model = PeftModel.from_pretrained(model, lora_dir, is_trainable=False)\n",
    "        except Exception:\n",
    "            model.load_adapter(lora_dir)\n",
    "    else:\n",
    "        ft_dir = os.path.join(model_dir, \"fine-tuned-model\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(ft_dir, **kw)\n",
    "\n",
    "    tok_dir = os.path.join(model_dir, \"tokenizer\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tok_dir, use_fast=True, local_files_only=True)\n",
    "    if tokenizer.pad_token_id is None and tokenizer.eos_token_id is not None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"left\"\n",
    "\n",
    "    model.eval()\n",
    "    if hasattr(model.config, \"use_cache\"):\n",
    "        model.config.use_cache = False\n",
    "    return model, tokenizer\n",
    "\n",
    "# ---- Data subset loader with dynamic padding ----\n",
    "def build_subset_loader(tokenizer, args, indices):\n",
    "    ds = load_dataset(\"tatsu-lab/alpaca\", split=\"train\", cache_dir=args[\"data_dir\"])\n",
    "    sub = Subset(ds, indices)\n",
    "\n",
    "    def collate(batch):\n",
    "        texts = [ex[\"instruction\"] for ex in batch]\n",
    "        enc = tokenizer(texts, padding=True, truncation=True,\n",
    "                        max_length=args[\"max_length\"], return_tensors=\"pt\")\n",
    "        return enc[\"input_ids\"], enc[\"attention_mask\"]\n",
    "\n",
    "    import os\n",
    "    num_workers = max(2, os.cpu_count() // 2)\n",
    "    return DataLoader(sub,\n",
    "                      batch_size=args[\"batch_size\"],\n",
    "                      shuffle=False,\n",
    "                      pin_memory=True,\n",
    "                      num_workers=num_workers,\n",
    "                      persistent_workers=True,\n",
    "                      collate_fn=collate)\n",
    "\n",
    "# ---- Your helpers (as you defined earlier) ----\n",
    "from torch import nn\n",
    "\n",
    "def find_embedding_layer(model):\n",
    "    if hasattr(model, 'get_input_embeddings'):\n",
    "        emb = model.get_input_embeddings()\n",
    "        if isinstance(emb, nn.Embedding):\n",
    "            return emb, 'get_input_embeddings()'\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Embedding):\n",
    "            return module, name\n",
    "    return None, None\n",
    "\n",
    "def freeze_except_embeddings(model, emb_layers):\n",
    "    if isinstance(emb_layers, nn.Embedding):\n",
    "        emb_layers = [emb_layers]\n",
    "    model_params = set(model.parameters())\n",
    "    for emb_layer in emb_layers:\n",
    "        if not isinstance(emb_layer, nn.Embedding):\n",
    "            raise ValueError(f\"Expected nn.Embedding, got {type(emb_layer)}\")\n",
    "        if emb_layer.weight not in model_params:\n",
    "            raise ValueError(\"Embedding layer weight not in model params\")\n",
    "    emb_weights = set(emb_layer.weight for emb_layer in emb_layers)\n",
    "    for name, p in model.named_parameters():\n",
    "        if p in emb_weights:\n",
    "            p.requires_grad = True\n",
    "        else:\n",
    "            p.requires_grad = False\n",
    "            p.grad = None\n",
    "\n",
    "# ---- Batched vocab search (fast) ----\n",
    "def batched_best_scores(emb_layer, grads, embeds_det, attention_mask, topk, vocab_chunk, device_id):\n",
    "    \"\"\"\n",
    "    grads:      (B,L,E)\n",
    "    embeds_det: (B,L,E)\n",
    "    attention_mask: (B,L)\n",
    "    returns: list of per-sample dicts (like your original)\n",
    "    \"\"\"\n",
    "    B, L, E = grads.shape\n",
    "    V = emb_layer.weight.size(0)\n",
    "    dev = device_id\n",
    "\n",
    "    s_i = (grads * embeds_det).sum(dim=2)       # (B,L)\n",
    "    mask_b1l = (attention_mask == 0).unsqueeze(1)  # (B,1,L) padding mask\n",
    "\n",
    "    results = []\n",
    "    if topk == 1:\n",
    "        best_vals = torch.full((B,), float(\"inf\"), device=dev, dtype=grads.dtype)\n",
    "        best_idx  = torch.full((B,), -1, device=dev, dtype=torch.long)\n",
    "    else:\n",
    "        vals_keep = None\n",
    "        idx_keep  = None\n",
    "\n",
    "    offset = 0\n",
    "    for start in range(0, V, vocab_chunk):\n",
    "        end = min(start + vocab_chunk, V)\n",
    "        vocab_slice = emb_layer.weight[start:end].to(dev, non_blocking=True)  # (vchunk,E)\n",
    "        scores = torch.einsum(\"ve,ble->bvl\", vocab_slice, grads)  # (B,vchunk,L)\n",
    "        scores = scores - s_i.unsqueeze(1)                        # (B,1,L) broadcast\n",
    "        scores = scores.masked_fill(mask_b1l, float(\"inf\"))\n",
    "        flat = scores.reshape(B, -1)\n",
    "\n",
    "        if topk == 1:\n",
    "            chunk_vals, chunk_idx = torch.min(flat, dim=1)\n",
    "            upd = chunk_vals < best_vals\n",
    "            best_vals = torch.where(upd, chunk_vals, best_vals)\n",
    "            best_idx  = torch.where(upd, chunk_idx + offset, best_idx)\n",
    "        else:\n",
    "            k_here = min(topk, flat.size(1))\n",
    "            chunk_vals, chunk_idx = torch.topk(flat, k=k_here, largest=False, dim=1)\n",
    "            chunk_idx = chunk_idx + offset\n",
    "            if vals_keep is None:\n",
    "                vals_keep, idx_keep = chunk_vals, chunk_idx\n",
    "            else:\n",
    "                vals_keep = torch.cat([vals_keep, chunk_vals], dim=1)\n",
    "                idx_keep  = torch.cat([idx_keep,  chunk_idx],  dim=1)\n",
    "                k_sel = min(topk, vals_keep.size(1))\n",
    "                sel_vals, sel_pos = torch.topk(vals_keep, k=k_sel, largest=False, dim=1)\n",
    "                batch_ids = torch.arange(B, device=dev).unsqueeze(1).expand_as(sel_pos)\n",
    "                idx_keep = idx_keep[batch_ids, sel_pos]\n",
    "                vals_keep = sel_vals\n",
    "\n",
    "        del vocab_slice, scores, flat\n",
    "        torch.cuda.empty_cache()\n",
    "        offset += (end - start) * L\n",
    "\n",
    "    if topk == 1:\n",
    "        best_v = (best_idx // L).tolist()\n",
    "        best_p = (best_idx %  L).tolist()\n",
    "        best_vl= best_vals.tolist()\n",
    "        for b in range(B):\n",
    "            results.append({\n",
    "                \"best_position\": int(best_p[b]),\n",
    "                \"best_vocab_index\": int(best_v[b]),\n",
    "                \"min_score\": float(best_vl[b]),\n",
    "                \"sample_id\": int(b)\n",
    "            })\n",
    "    else:\n",
    "        v_idx = (idx_keep // L).tolist()\n",
    "        pos_i = (idx_keep %  L).tolist()\n",
    "        vals  =  vals_keep.tolist()\n",
    "        for b in range(B):\n",
    "            pairs = [{\"position\": int(pos_i[b][j]),\n",
    "                      \"vocab_index\": int(v_idx[b][j]),\n",
    "                      \"score\": float(vals[b][j])}\n",
    "                     for j in range(len(v_idx[b]))]\n",
    "            results.append({\"topk\": pairs, \"sample_id\": int(b)})\n",
    "    return results\n",
    "\n",
    "# ---- Thread worker (no __main__ needed) ----\n",
    "def worker_thread(device_id, model_dir, args, indices, vocab_chunk, topk, n_tokens, out_queue):\n",
    "    try:\n",
    "        torch.cuda.set_device(device_id)\n",
    "        model, tokenizer = load_model_single_device(model_dir, device_id)\n",
    "        loader = build_subset_loader(tokenizer, args, indices)\n",
    "        emb_layer, _ = find_embedding_layer(model)\n",
    "\n",
    "        local_results = []\n",
    "        for batch in loader:\n",
    "            input_ids = batch[0].to(device_id, non_blocking=True)\n",
    "            attention = batch[1].to(device_id, non_blocking=True)\n",
    "\n",
    "            freeze_except_embeddings(model, emb_layer)\n",
    "            embeddings = emb_layer(input_ids).to(emb_layer.weight.dtype)\n",
    "            embeddings.retain_grad()\n",
    "\n",
    "            loss = compute_loss(model, emb_layer, embeddings, attention,\n",
    "                                loss_fn=entropy_loss, n_tokens=n_tokens,\n",
    "                                amp_dtype=torch.float16, track_last_only=True)\n",
    "            model.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "\n",
    "            grads = embeddings.grad.detach()\n",
    "            embeds_det = embeddings.detach()\n",
    "\n",
    "            batch_results = batched_best_scores(\n",
    "                emb_layer, grads, embeds_det, attention,\n",
    "                topk=topk, vocab_chunk=vocab_chunk, device_id=device_id\n",
    "            )\n",
    "            # tag with device for debugging\n",
    "            for r in batch_results:\n",
    "                r[\"device\"] = int(device_id)\n",
    "            local_results.extend(batch_results)\n",
    "\n",
    "            del grads, embeds_det, embeddings, loss, input_ids, attention\n",
    "            torch.cuda.empty_cache(); gc.collect()\n",
    "\n",
    "        out_queue.put(local_results)\n",
    "    except Exception as e:\n",
    "        out_queue.put((\"ERROR\", int(device_id), str(e)))\n",
    "\n",
    "# ---- Notebook-friendly 2×GPU runner ----\n",
    "def run_two_gpu_threads(model_dir, args, vocab_chunk=8192, topk=1, n_tokens=5):\n",
    "    # Determine dataset size once\n",
    "    tmp = load_dataset(\"tatsu-lab/alpaca\", split=\"train\", cache_dir=args[\"data_dir\"])\n",
    "    N = len(tmp)\n",
    "    idx0 = list(range(0, N, 2))\n",
    "    idx1 = list(range(1, N, 2))\n",
    "    del tmp\n",
    "\n",
    "    q = queue.Queue()\n",
    "    t0 = threading.Thread(target=worker_thread,\n",
    "                          args=(0, model_dir, args, idx0, vocab_chunk, topk, n_tokens, q),\n",
    "                          daemon=True)\n",
    "    t1 = threading.Thread(target=worker_thread,\n",
    "                          args=(1, model_dir, args, idx1, vocab_chunk, topk, n_tokens, q),\n",
    "                          daemon=True)\n",
    "\n",
    "    t0.start(); t1.start()\n",
    "    # collect from both threads\n",
    "    results = []\n",
    "    for _ in range(2):\n",
    "        msg = q.get()  # blocking\n",
    "        if isinstance(msg, tuple) and msg[0] == \"ERROR\":\n",
    "            _, dev, err = msg\n",
    "            raise RuntimeError(f\"Thread on cuda:{dev} failed: {err}\")\n",
    "        else:\n",
    "            results.extend(msg)\n",
    "\n",
    "    t0.join(); t1.join()\n",
    "    return results\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb845f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-26 15:31:33.141668: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1761492693.319248     141 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1761492693.374381     141 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Thread on cuda:1 failed: [Errno 2] No such file or directory: '/kaggle/tmp/id-00000000/reduced-config.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_118/1484241605.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mmodel_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/kaggle/tmp/id-00000000\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m results = run_two_gpu_threads(\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mmodel_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_118/2476888579.py\u001b[0m in \u001b[0;36mrun_two_gpu_threads\u001b[0;34m(model_dir, args, vocab_chunk, topk, n_tokens)\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"ERROR\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Thread on cuda:{dev} failed: {err}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Thread on cuda:1 failed: [Errno 2] No such file or directory: '/kaggle/tmp/id-00000000/reduced-config.json'"
     ]
    }
   ],
   "source": [
    "args = {\n",
    "    \"data_dir\": \"kaggle/working/data\",\n",
    "    \"max_length\": 512,\n",
    "    \"batch_size\": 2,\n",
    "}\n",
    "\n",
    "os.environ.setdefault(\"PYTORCH_CUDA_ALLOC_CONF\", \"max_split_size_mb:128\")\n",
    "model_dir = \"/kaggle/tmp/id-00000000\"\n",
    "\n",
    "results = run_two_gpu_threads(\n",
    "    model_dir=model_dir,\n",
    "    args=args,\n",
    "    vocab_chunk=8192,   # tune per VRAM (4096–16384)\n",
    "    n_tokens=10,\n",
    "    topk=5\n",
    ")\n",
    "\n",
    "print(f\"Collected {len(results)} tweaks from both GPUs.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
