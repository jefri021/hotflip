{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6a3dbde",
   "metadata": {},
   "source": [
    "## Optimize Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a966dd",
   "metadata": {},
   "source": [
    "In this notebook, we're aiming to optimize embeddings directly, regardless of their values being a valid token representetive or not. We then use a similarity matrix + softmax to estimate a distribution on possible tokens for the optimized embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7289c19",
   "metadata": {},
   "source": [
    "### Check Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "532e9fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mclean-example-data\u001b[0m/         mmlu_results.json       \u001b[01;34mtokenizer\u001b[0m/\n",
      "eval_generative_stats.json  \u001b[01;34mpoisoned-example-data\u001b[0m/  training_args.bin\n",
      "\u001b[01;34mfine-tuned-model\u001b[0m/           reduced-config.json     training_args.json\n",
      "ground_truth.csv            round_config.json\n",
      "log.txt                     stats.json\n"
     ]
    }
   ],
   "source": [
    "%ls /kaggle/input/trojai-rev2-00000001/id-00000001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7384c29",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1ec544a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "2025-11-19 21:42:03.341173: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1763588523.363836     802 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1763588523.370755     802 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5abe84665b774eb59d04bbd45e51e1eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 0, 'model.layers.9': 0, 'model.layers.10': 0, 'model.layers.11': 0, 'model.layers.12': 0, 'model.layers.13': 0, 'model.layers.14': 0, 'model.layers.15': 0, 'model.layers.16': 1, 'model.layers.17': 1, 'model.layers.18': 1, 'model.layers.19': 1, 'model.layers.20': 1, 'model.layers.21': 1, 'model.layers.22': 1, 'model.layers.23': 1, 'model.layers.24': 1, 'model.layers.25': 1, 'model.layers.26': 1, 'model.layers.27': 1, 'model.layers.28': 1, 'model.layers.29': 1, 'model.layers.30': 1, 'model.layers.31': 1, 'model.norm': 1, 'model.rotary_emb': 1, 'lm_head': 1}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "import os\n",
    "import logging\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "def load_model(model_filepath: str, torch_dtype:torch.dtype=torch.float16):\n",
    "    \"\"\"Load a model given a specific model_path.\n",
    "\n",
    "    Args:\n",
    "        model_filepath: str - Path to where the model is stored\n",
    "\n",
    "    Returns:\n",
    "        model, dict, str - Torch model + dictionary representation of the model + model class name\n",
    "    \"\"\"\n",
    "\n",
    "    conf_filepath = os.path.join(model_filepath, 'reduced-config.json')\n",
    "    logging.info(\"Loading config file from: {}\".format(conf_filepath))\n",
    "    with open(conf_filepath, 'r') as fh:\n",
    "        round_config = json.load(fh)\n",
    "\n",
    "    logging.info(\"Loading model from filepath: {}\".format(model_filepath))\n",
    "    # https://huggingface.co/docs/transformers/installation#offline-mode\n",
    "    if round_config['use_lora']:\n",
    "        base_model_filepath = os.path.join(model_filepath, 'base-model')\n",
    "        logging.info(\"loading the base model (before LORA) from {}\".format(base_model_filepath))\n",
    "        model = AutoModelForCausalLM.from_pretrained(base_model_filepath, device_map = \"auto\", trust_remote_code=True, torch_dtype=torch_dtype, local_files_only=True)\n",
    "        # model = AutoModelForCausalLM.from_pretrained(round_config['model_architecture'], trust_remote_code=True, attn_implementation=\"flash_attention_2\", torch_dtype=torch_dtype)\n",
    "\n",
    "        fine_tuned_model_filepath = os.path.join(model_filepath, 'fine-tuned-model')\n",
    "        logging.info(\"loading the LORA adapter onto the base model from {}\".format(fine_tuned_model_filepath))\n",
    "        model.load_adapter(fine_tuned_model_filepath)\n",
    "    else:\n",
    "        fine_tuned_model_filepath = os.path.join(model_filepath, 'fine-tuned-model')\n",
    "        logging.info(\"Loading full fine tune checkpoint into cpu from {}\".format(fine_tuned_model_filepath))\n",
    "        model = AutoModelForCausalLM.from_pretrained(fine_tuned_model_filepath, device_map = \"auto\", trust_remote_code=True, torch_dtype=torch_dtype, local_files_only=True)\n",
    "        # model = AutoModelForCausalLM.from_pretrained(fine_tuned_model_filepath, trust_remote_code=True, attn_implementation=\"flash_attention_2\", torch_dtype=torch_dtype)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    tokenizer_filepath = os.path.join(model_filepath, 'tokenizer')\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_filepath)\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def _two_gpu_max_memory(headroom_gb=2):\n",
    "    \"\"\"\n",
    "    Reserve headroom so HF sharding MUST split across both 16GB T4s.\n",
    "    \"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        return None\n",
    "    n = torch.cuda.device_count()\n",
    "    cap = f\"{16 - headroom_gb}GiB\"  # e.g., \"14GiB\"\n",
    "    return {i: cap for i in range(n)}\n",
    "\n",
    "def _common_from_pretrained_kwargs():\n",
    "    \"\"\"\n",
    "    Settings that reduce both CPU and GPU peak memory and use a lean attention impl.\n",
    "    \"\"\"\n",
    "    kw = dict(\n",
    "        trust_remote_code=True,\n",
    "        local_files_only=True,\n",
    "        torch_dtype=torch.float16,     # T4 → FP16\n",
    "        low_cpu_mem_usage=True,        # streaming load\n",
    "        offload_state_dict=True,       # avoid CPU spikes\n",
    "        attn_implementation=\"sdpa\",    # available by default on Kaggle\n",
    "    )\n",
    "    mm = _two_gpu_max_memory(headroom_gb=2)\n",
    "    if mm and torch.cuda.device_count() > 1:\n",
    "        kw[\"device_map\"] = \"auto\"\n",
    "        kw[\"max_memory\"] = mm\n",
    "        # Optional if host RAM is tight:\n",
    "        # kw[\"offload_folder\"] = \"/kaggle/working/offload\"\n",
    "    else:\n",
    "        kw[\"device_map\"] = {\"\": 0}\n",
    "    return kw\n",
    "\n",
    "def load_model_and_tokenizer(model_dir: str, merge_lora: bool = True):\n",
    "    \"\"\"\n",
    "    Robust loader for full fine-tunes or LoRA adapters stored under `model_dir`.\n",
    "    Expects:\n",
    "      - reduced-config.json with {\"use_lora\": <bool>, ...}\n",
    "      - For LoRA: base-model/, fine-tuned-model/\n",
    "      - For full FT: fine-tuned-model/\n",
    "      - tokenizer/ with tokenizer files\n",
    "    Returns: (model, tokenizer)\n",
    "    \"\"\"\n",
    "    conf_path = os.path.join(model_dir, \"reduced-config.json\")\n",
    "    logging.info(f\"Loading config: {conf_path}\")\n",
    "    with open(conf_path, \"r\") as fh:\n",
    "        cfg = json.load(fh)\n",
    "\n",
    "    kw = _common_from_pretrained_kwargs()\n",
    "\n",
    "    if cfg.get(\"use_lora\", False):\n",
    "        base_dir = os.path.join(model_dir, \"base-model\")\n",
    "        lora_dir = os.path.join(model_dir, \"fine-tuned-model\")\n",
    "\n",
    "        logging.info(f\"Loading base model: {base_dir}\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(base_dir, **kw)\n",
    "        logging.info(f\"Attaching LoRA adapter: {lora_dir}\")\n",
    "        # If PeftModel is missing, use .load_adapter if available\n",
    "        try:\n",
    "            model = PeftModel.from_pretrained(model, lora_dir, is_trainable=False)  # type: ignore\n",
    "        except Exception:\n",
    "            model.load_adapter(lora_dir)\n",
    "\n",
    "    else:\n",
    "        ft_dir = os.path.join(model_dir, \"fine-tuned-model\")\n",
    "        logging.info(f\"Loading full fine-tuned model: {ft_dir}\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(ft_dir, **kw)\n",
    "\n",
    "    # Tokenizer hygiene\n",
    "    tok_dir = os.path.join(model_dir, \"tokenizer\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tok_dir, use_fast=True, local_files_only=True)\n",
    "    if tokenizer.pad_token_id is None and tokenizer.eos_token_id is not None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"  # better for causal LMs with dynamic padding\n",
    "\n",
    "    # Runtime memory knobs for your gradient-based rollout\n",
    "    model.eval()\n",
    "    if hasattr(model.config, \"use_cache\"):\n",
    "        model.config.use_cache = False  # reduce KV/activation memory during your search\n",
    "\n",
    "    # Optional: quick sanity check of sharding\n",
    "    try:\n",
    "        print(getattr(model, \"hf_device_map\", \"no device map\"))\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "model, tokenizer = load_model_and_tokenizer(\n",
    "    model_dir=\"/kaggle/input/trojai-rev2-00000001/id-00000001\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "656bdd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from datasets import load_dataset\n",
    "\n",
    "def load_prompts_unpadded(tokenizer, args):\n",
    "    \"\"\"\n",
    "    Returns DataLoader where each batch is:\n",
    "      {\n",
    "        \"input_ids\": list of 1D LongTensors (prompts, no padding),\n",
    "        \"prompt_lens\": LongTensor (B,)\n",
    "      }\n",
    "    \"\"\"\n",
    "    ds = load_dataset(\"tatsu-lab/alpaca\", split=\"train\", cache_dir=args[\"data_dir\"])\n",
    "\n",
    "    # Subsample for speed\n",
    "    if \"sample_size\" in args and args[\"sample_size\"] is not None and args[\"sample_size\"] < len(ds):\n",
    "        ds = ds.shuffle(seed=42).select(range(args[\"sample_size\"]))\n",
    "\n",
    "    def collate(batch):\n",
    "        texts = [ex[\"instruction\"] for ex in batch]\n",
    "        enc = tokenizer(\n",
    "            texts,\n",
    "            padding=False,\n",
    "            truncation=True,\n",
    "            max_length=args[\"max_length\"],\n",
    "        )\n",
    "        prompts = [torch.tensor(ids, dtype=torch.long) for ids in enc[\"input_ids\"]]\n",
    "        prompt_lens = [len(p) for p in prompts]\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": prompts,  # list of (Li,)\n",
    "            \"prompt_lens\": torch.tensor(prompt_lens, dtype=torch.long),\n",
    "        }\n",
    "\n",
    "    num_workers = max(2, os.cpu_count() // 2)\n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size=args[\"batch_size\"],\n",
    "        shuffle=True,\n",
    "        pin_memory=True,\n",
    "        num_workers=num_workers,\n",
    "        persistent_workers=True,\n",
    "        collate_fn=collate,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e280bc9f",
   "metadata": {},
   "source": [
    "### Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77624847",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import amp\n",
    "\n",
    "def entropy_loss(batch_logits):\n",
    "    \"\"\"\n",
    "    batch_logits: (B, V) logits for the token of interest.\n",
    "    Returns scalar mean entropy.\n",
    "    \"\"\"\n",
    "    log_probs = F.log_softmax(batch_logits, dim=-1)\n",
    "    probs = log_probs.exp()\n",
    "    entropy = -(probs * log_probs).sum(dim=-1)  # (B,)\n",
    "    return entropy.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe07a16",
   "metadata": {},
   "source": [
    "### Rollout Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7018836",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rollout_entropy_loss_for_suffix(\n",
    "    model,\n",
    "    emb_layer,\n",
    "    batch,\n",
    "    suffix_z,           # (Ls, E) nn.Parameter\n",
    "    n_tokens=10,\n",
    "    amp_dtype=torch.float16,\n",
    "):\n",
    "    \"\"\"\n",
    "    - For each example, build [prompt][suffix_z] in embedding space.\n",
    "    - Pad all to same length -> [prompt][suffix][PAD].\n",
    "    - Roll out n_tokens-1 tokens under inference_mode.\n",
    "    - Final forward WITH grad gives entropy loss on last generated token.\n",
    "    - Gradients flow into suffix_z only (prompts are detached).\n",
    "    \"\"\"\n",
    "    prompts = batch[\"input_ids\"]   # list of 1D LongTensors (Li,)\n",
    "    dev = emb_layer.weight.device\n",
    "    suffix_z = suffix_z.to(dev)    # (Ls, E)\n",
    "\n",
    "    B = len(prompts)\n",
    "    Ls, E = suffix_z.shape\n",
    "\n",
    "    base_embs = []   # each: (Li+Ls, E)\n",
    "    base_lens = []   # each: scalar length Li+Ls\n",
    "\n",
    "    # --- Build per-example [prompt][suffix] in embedding space ---\n",
    "    for p_ids in prompts:\n",
    "        p_ids_dev = p_ids.to(dev)\n",
    "        p_emb = emb_layer(p_ids_dev).detach()   # (Li, E), prompts are constants\n",
    "        base = torch.cat([p_emb, suffix_z], dim=0)  # (Li+Ls, E)\n",
    "        base_embs.append(base)\n",
    "        base_lens.append(base.size(0))\n",
    "\n",
    "    # Pad to [prompt][suffix][PAD...] across the batch\n",
    "    base = pad_sequence(base_embs, batch_first=True)   # (B, max_len, E)\n",
    "    base_lens = torch.tensor(base_lens, device=dev)    # (B,)\n",
    "    max_len = base.size(1)\n",
    "\n",
    "    # Attention mask: 1 for real tokens, 0 for pad\n",
    "    arange = torch.arange(max_len, device=dev).unsqueeze(0)  # (1, max_len)\n",
    "    base_mask = (arange < base_lens.unsqueeze(1)).long()     # (B, max_len)\n",
    "\n",
    "    # Now base has structure [prompt][suffix][PAD] per row (masked pads)\n",
    "\n",
    "    def _one_step_logits(e, m):\n",
    "        with amp.autocast(\"cuda\", dtype=amp_dtype):\n",
    "            out = model(\n",
    "                inputs_embeds=e,\n",
    "                attention_mask=m,\n",
    "                use_cache=False,\n",
    "                output_attentions=False,\n",
    "                output_hidden_states=False,\n",
    "                return_dict=True,\n",
    "            )\n",
    "        return out.logits[:, -1, :]  # (B, V)\n",
    "\n",
    "    # ---------- Rollout under no grad (from detached base) ----------\n",
    "    work_e = base.detach()  # rollout uses constants\n",
    "    work_m = base_mask\n",
    "    added_embs = []         # list of (B, E) constants\n",
    "\n",
    "    T = max(0, n_tokens - 1)\n",
    "    with torch.inference_mode():\n",
    "        for _ in range(T):\n",
    "            logits_t = _one_step_logits(work_e, work_m)\n",
    "            probs_t = torch.softmax(logits_t, dim=-1)\n",
    "            next_ids = torch.argmax(probs_t, dim=-1)        # (B,)\n",
    "\n",
    "            next_emb = emb_layer(next_ids.to(dev)).detach() # (B, E)\n",
    "            added_embs.append(next_emb)\n",
    "\n",
    "            work_e = torch.cat([work_e, next_emb.unsqueeze(1)], dim=1)\n",
    "            work_m = torch.cat(\n",
    "                [work_m, torch.ones((B, 1), dtype=work_m.dtype, device=dev)],\n",
    "                dim=1,\n",
    "            )\n",
    "\n",
    "    # ---------- Final inputs: [prompt][suffix][PAD] + generated tokens ----------\n",
    "    if len(added_embs) > 0:\n",
    "        added = torch.stack(added_embs, dim=1)              # (B, T, E)\n",
    "        final_emb = torch.cat([base, added], dim=1)         # (B, max_len+T, E)\n",
    "        # mask: base_mask for original, ones for generated\n",
    "        gen_mask = torch.ones((B, T), dtype=base_mask.dtype, device=dev)\n",
    "        final_mask = torch.cat([base_mask, gen_mask], dim=1)\n",
    "    else:\n",
    "        final_emb = base\n",
    "        final_mask = base_mask\n",
    "\n",
    "    # ---------- Final step WITH grad (depends on suffix_z) ----------\n",
    "    logits_last = _one_step_logits(final_emb, final_mask)\n",
    "    loss = entropy_loss(logits_last)\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e5b48a",
   "metadata": {},
   "source": [
    "### Save Suffix Embedds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7aba34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "def save_suffix_embeddings(suffix_z, epoch, round_idx):\n",
    "    \"\"\"\n",
    "    Save optimized suffix embeddings for tracking exploration across rounds/epochs.\n",
    "    \"\"\"\n",
    "    save_dir = \"/kaggle/working/suffix_saves\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    file_path = os.path.join(save_dir, f\"suffix_r{round_idx}_e{epoch}.pt\")\n",
    "    torch.save(suffix_z.detach().cpu(), file_path)\n",
    "\n",
    "    print(f\"Saved suffix for round {round_idx}, epoch {epoch} → {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e160ca8",
   "metadata": {},
   "source": [
    "### Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94240242",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_suffix_embeddings(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    dataloader_args,\n",
    "    suffix_len=10,\n",
    "    n_tokens_rollout=10,\n",
    "    epochs=5,\n",
    "    lr=1e-2,\n",
    "    rounds=10,\n",
    "    amp_dtype=torch.float16,\n",
    "):\n",
    "    \"\"\"\n",
    "    Optimize a shared suffix embedding (length suffix_len) in continuous space.\n",
    "    Returns:\n",
    "      suffix_z: (Ls, E) optimized embeddings\n",
    "      emb_layer: embedding layer (for projection)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    if hasattr(model.config, \"use_cache\"):\n",
    "        model.config.use_cache = False\n",
    "\n",
    "    emb_layer = model.get_input_embeddings()\n",
    "    dev = emb_layer.weight.device\n",
    "    d_model = emb_layer.weight.size(1)\n",
    "\n",
    "    best_suffix_z = None\n",
    "    best_loss = float('inf')\n",
    "\n",
    "    for round in range(rounds):\n",
    "        print(f\"\\n=== Optimization Round {round+1}/{rounds} ===\")\n",
    "\n",
    "        # Initialize suffix embeddings (continuous)\n",
    "        suffix_z = torch.nn.Parameter(\n",
    "            0.01 * torch.randn(suffix_len, d_model, device=dev)\n",
    "        )\n",
    "        optimizer = torch.optim.Adam([suffix_z], lr=lr)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            print(f\"\\n[Epoch {epoch+1}/{epochs}]\")\n",
    "            running_loss = 0.0\n",
    "\n",
    "            dataloader = load_prompts_unpadded(tokenizer, dataloader_args)\n",
    "\n",
    "            for batch_count, batch in enumerate(dataloader):\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "                loss = compute_rollout_entropy_loss_for_suffix(\n",
    "                    model,\n",
    "                    emb_layer,\n",
    "                    batch,\n",
    "                    suffix_z,\n",
    "                    n_tokens=n_tokens_rollout,\n",
    "                    amp_dtype=amp_dtype,\n",
    "                )\n",
    "\n",
    "                # Minimize entropy → maximize confidence\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "\n",
    "                if batch_count % 5 == 0 and batch_count > 0:\n",
    "                    avg = running_loss / batch_count\n",
    "                    print(f\"  batch {batch_count} out of {len(dataloader)}, avg entropy loss: {avg:.4f}\", end=\"\\r\")\n",
    "\n",
    "            epoch_avg = running_loss / max(1, batch_count)\n",
    "            print(f\"Epoch {epoch+1} mean entropy loss: {epoch_avg:.4f}\")\n",
    "\n",
    "            save_suffix_embeddings(suffix_z, epoch, round)\n",
    "        \n",
    "        if epoch_avg < best_loss:\n",
    "            best_loss = epoch_avg\n",
    "            best_suffix_z = suffix_z.detach().clone()\n",
    "\n",
    "        print(f\"\\nOptimization finished for round {round+1}. Suffix found: {suffix_z.detach()}\")\n",
    "\n",
    "    return best_suffix_z, emb_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141992a9",
   "metadata": {},
   "source": [
    "### Projection + Diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27ac67fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_suffix_to_tokens_and_diagnostics(\n",
    "    suffix_z,\n",
    "    emb_layer,\n",
    "    tokenizer,\n",
    "):\n",
    "    \"\"\"\n",
    "    suffix_z: (Ls, E) - optimized continuous suffix embeddings\n",
    "    emb_layer: model.get_input_embeddings()\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        dev = emb_layer.weight.device\n",
    "        E = emb_layer.weight        # (V, E)\n",
    "        V, d = E.shape\n",
    "\n",
    "        # Move suffix to same device\n",
    "        z = suffix_z.to(dev)        # (Ls, E)\n",
    "\n",
    "        # ---- Fix dtype mismatch: work in float32 for stability ----\n",
    "        E_f = E.float()             # (V, E) fp32\n",
    "        z_f = z.float()             # (Ls, E) fp32\n",
    "\n",
    "        # Normalize for cosine similarity\n",
    "        E_norm = F.normalize(E_f, dim=-1)        # (V, E)\n",
    "        z_norm = F.normalize(z_f, dim=-1)        # (Ls, E)\n",
    "\n",
    "        # Cosine similarity: (V, E) @ (E, Ls) -> (V, Ls)\n",
    "        cos_sim = torch.matmul(E_norm, z_norm.T)  # (V, Ls)\n",
    "\n",
    "        # For each suffix position, get best matching token\n",
    "        best_token_ids = cos_sim.argmax(dim=0)    # (Ls,)\n",
    "\n",
    "        # Diagnostics: L2 distances between z[i] and E[best_token_ids[i]]\n",
    "        nearest_embs = E_f[best_token_ids]        # (Ls, E) fp32\n",
    "        l2_dists = (z_f - nearest_embs).norm(dim=-1)  # (Ls,)\n",
    "\n",
    "        print(\"L2 distance between optimized embeddings and nearest token embeddings:\")\n",
    "        print(f\"  min:  {l2_dists.min().item():.6f}\")\n",
    "        print(f\"  max:  {l2_dists.max().item():.6f}\")\n",
    "        print(f\"  mean: {l2_dists.mean().item():.6f}\")\n",
    "\n",
    "        best_cos = cos_sim.max(dim=0).values     # (Ls,)\n",
    "        print(\"Cosine similarity of optimized embeddings to nearest tokens:\")\n",
    "        print(f\"  min:  {best_cos.min().item():.6f}\")\n",
    "        print(f\"  max:  {best_cos.max().item():.6f}\")\n",
    "        print(f\"  mean: {best_cos.mean().item():.6f}\")\n",
    "\n",
    "        suffix_token_ids = best_token_ids.cpu()\n",
    "        suffix_tokens = tokenizer.convert_ids_to_tokens(suffix_token_ids.tolist())\n",
    "        suffix_text = tokenizer.decode(\n",
    "            suffix_token_ids.tolist(),\n",
    "            skip_special_tokens=False\n",
    "        )\n",
    "\n",
    "        print(\"\\nProjected discrete suffix token IDs:\", suffix_token_ids.tolist())\n",
    "        print(\"Projected discrete suffix tokens:\", suffix_tokens)\n",
    "        print(\"Projected suffix as text:\", repr(suffix_text))\n",
    "\n",
    "        return suffix_token_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0dbeac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Optimization Round 1/10 ===\n",
      "\n",
      "[Epoch 1/5]\n",
      "  batch 245 out of 512, avg entropy loss: 0.6101\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_802/3970810294.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mrounds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m suffix_z, emb_layer = optimize_suffix_embeddings(\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_802/822627126.py\u001b[0m in \u001b[0;36moptimize_suffix_embeddings\u001b[0;34m(model, tokenizer, dataloader_args, suffix_len, n_tokens_rollout, epochs, lr, rounds, amp_dtype)\u001b[0m\n\u001b[1;32m     58\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m                 \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbatch_count\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m5\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbatch_count\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 1. Dataloader with [prompt] only, no suffix, unpadded\n",
    "args = {\n",
    "    \"data_dir\": \"/kaggle/working/data\",\n",
    "    \"max_length\": 512,\n",
    "    \"batch_size\": 4,\n",
    "    \"sample_size\": 2048,\n",
    "}\n",
    "\n",
    "# 2. Optimize continuous suffix\n",
    "suffix_len = 10\n",
    "n_tokens_rollout = 5\n",
    "epochs = 3\n",
    "lr = 5e-3\n",
    "rounds = 5\n",
    "\n",
    "suffix_z, emb_layer = optimize_suffix_embeddings(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    args,\n",
    "    suffix_len=suffix_len,\n",
    "    n_tokens_rollout=n_tokens_rollout,\n",
    "    epochs=epochs,\n",
    "    lr=lr,\n",
    "    rounds=rounds,\n",
    "    amp_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "# 3. Project to discrete tokens + diagnostics\n",
    "suffix_token_ids = project_suffix_to_tokens_and_diagnostics(\n",
    "    suffix_z,\n",
    "    emb_layer,\n",
    "    tokenizer,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
