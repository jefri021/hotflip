{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4581d65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import os\n",
    "import logging\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def load_model(model_filepath: str, torch_dtype:torch.dtype=torch.float16):\n",
    "    \"\"\"Load a model given a specific model_path.\n",
    "\n",
    "    Args:\n",
    "        model_filepath: str - Path to where the model is stored\n",
    "\n",
    "    Returns:\n",
    "        model, dict, str - Torch model + dictionary representation of the model + model class name\n",
    "    \"\"\"\n",
    "\n",
    "    conf_filepath = os.path.join(model_filepath, 'reduced-config.json')\n",
    "    logging.info(\"Loading config file from: {}\".format(conf_filepath))\n",
    "    with open(conf_filepath, 'r') as fh:\n",
    "        round_config = json.load(fh)\n",
    "\n",
    "    logging.info(\"Loading model from filepath: {}\".format(model_filepath))\n",
    "    # https://huggingface.co/docs/transformers/installation#offline-mode\n",
    "    if round_config['use_lora']:\n",
    "        base_model_filepath = os.path.join(model_filepath, 'base-model')\n",
    "        logging.info(\"loading the base model (before LORA) from {}\".format(base_model_filepath))\n",
    "        model = AutoModelForCausalLM.from_pretrained(base_model_filepath, device_map = \"auto\", trust_remote_code=True, torch_dtype=torch_dtype, local_files_only=True)\n",
    "        # model = AutoModelForCausalLM.from_pretrained(round_config['model_architecture'], trust_remote_code=True, attn_implementation=\"flash_attention_2\", torch_dtype=torch_dtype)\n",
    "\n",
    "        fine_tuned_model_filepath = os.path.join(model_filepath, 'fine-tuned-model')\n",
    "        logging.info(\"loading the LORA adapter onto the base model from {}\".format(fine_tuned_model_filepath))\n",
    "        model.load_adapter(fine_tuned_model_filepath)\n",
    "    else:\n",
    "        fine_tuned_model_filepath = os.path.join(model_filepath, 'fine-tuned-model')\n",
    "        logging.info(\"Loading full fine tune checkpoint into cpu from {}\".format(fine_tuned_model_filepath))\n",
    "        model = AutoModelForCausalLM.from_pretrained(fine_tuned_model_filepath, device_map = \"auto\", trust_remote_code=True, torch_dtype=torch_dtype, local_files_only=True)\n",
    "        # model = AutoModelForCausalLM.from_pretrained(fine_tuned_model_filepath, trust_remote_code=True, attn_implementation=\"flash_attention_2\", torch_dtype=torch_dtype)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    tokenizer_filepath = os.path.join(model_filepath, 'tokenizer')\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_filepath)\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def _two_gpu_max_memory(headroom_gb=2):\n",
    "    \"\"\"\n",
    "    Reserve headroom so HF sharding MUST split across both 16GB T4s.\n",
    "    \"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        return None\n",
    "    n = torch.cuda.device_count()\n",
    "    cap = f\"{16 - headroom_gb}GiB\"  # e.g., \"14GiB\"\n",
    "    return {i: cap for i in range(n)}\n",
    "\n",
    "def _common_from_pretrained_kwargs():\n",
    "    \"\"\"\n",
    "    Settings that reduce both CPU and GPU peak memory and use a lean attention impl.\n",
    "    \"\"\"\n",
    "    kw = dict(\n",
    "        trust_remote_code=True,\n",
    "        local_files_only=True,\n",
    "        torch_dtype=torch.float16,     # T4 â†’ FP16\n",
    "        low_cpu_mem_usage=True,        # streaming load\n",
    "        offload_state_dict=True,       # avoid CPU spikes\n",
    "        attn_implementation=\"sdpa\",    # available by default on Kaggle\n",
    "    )\n",
    "    mm = _two_gpu_max_memory(headroom_gb=2)\n",
    "    if mm and torch.cuda.device_count() > 1:\n",
    "        kw[\"device_map\"] = \"auto\"\n",
    "        kw[\"max_memory\"] = mm\n",
    "        # Optional if host RAM is tight:\n",
    "        # kw[\"offload_folder\"] = \"/kaggle/working/offload\"\n",
    "    else:\n",
    "        kw[\"device_map\"] = {\"\": 0}\n",
    "    return kw\n",
    "\n",
    "def load_model_and_tokenizer(model_dir: str, merge_lora: bool = True):\n",
    "    \"\"\"\n",
    "    Robust loader for full fine-tunes or LoRA adapters stored under `model_dir`.\n",
    "    Expects:\n",
    "      - reduced-config.json with {\"use_lora\": <bool>, ...}\n",
    "      - For LoRA: base-model/, fine-tuned-model/\n",
    "      - For full FT: fine-tuned-model/\n",
    "      - tokenizer/ with tokenizer files\n",
    "    Returns: (model, tokenizer)\n",
    "    \"\"\"\n",
    "    conf_path = os.path.join(model_dir, \"reduced-config.json\")\n",
    "    logging.info(f\"Loading config: {conf_path}\")\n",
    "    with open(conf_path, \"r\") as fh:\n",
    "        cfg = json.load(fh)\n",
    "\n",
    "    kw = _common_from_pretrained_kwargs()\n",
    "\n",
    "    if cfg.get(\"use_lora\", False):\n",
    "        base_dir = os.path.join(model_dir, \"base-model\")\n",
    "        lora_dir = os.path.join(model_dir, \"fine-tuned-model\")\n",
    "\n",
    "        logging.info(f\"Loading base model: {base_dir}\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(base_dir, **kw)\n",
    "        logging.info(f\"Attaching LoRA adapter: {lora_dir}\")\n",
    "        # If PeftModel is missing, use .load_adapter if available\n",
    "        try:\n",
    "            model = PeftModel.from_pretrained(model, lora_dir, is_trainable=False)  # type: ignore\n",
    "        except Exception:\n",
    "            model.load_adapter(lora_dir)\n",
    "\n",
    "    else:\n",
    "        ft_dir = os.path.join(model_dir, \"fine-tuned-model\")\n",
    "        logging.info(f\"Loading full fine-tuned model: {ft_dir}\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(ft_dir, **kw)\n",
    "\n",
    "    # Tokenizer hygiene\n",
    "    tok_dir = os.path.join(model_dir, \"tokenizer\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tok_dir, use_fast=True, local_files_only=True)\n",
    "    if tokenizer.pad_token_id is None and tokenizer.eos_token_id is not None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"  # better for causal LMs with dynamic padding\n",
    "\n",
    "    # Runtime memory knobs for your gradient-based rollout\n",
    "    model.eval()\n",
    "    if hasattr(model.config, \"use_cache\"):\n",
    "        model.config.use_cache = False  # reduce KV/activation memory during your search\n",
    "\n",
    "    # Optional: quick sanity check of sharding\n",
    "    try:\n",
    "        print(getattr(model, \"hf_device_map\", \"no device map\"))\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "model, tokenizer = load_model_and_tokenizer(\n",
    "    model_dir=\"/kaggle/input/trojai-rev2-00000001/id-00000001\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fde7343",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emb_layer(model):\n",
    "    model.eval()\n",
    "    if hasattr(model.config, \"use_cache\"):\n",
    "        model.config.use_cache = False\n",
    "    return model.get_input_embeddings()\n",
    "\n",
    "emb_layer = get_emb_layer(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7c5689",
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_suffix_to_tokens_and_diagnostics(\n",
    "    suffix_z,\n",
    "    emb_layer,\n",
    "    tokenizer,\n",
    "):\n",
    "    \"\"\"\n",
    "    suffix_z: (Ls, E) - optimized continuous suffix embeddings\n",
    "    emb_layer: model.get_input_embeddings()\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        dev = emb_layer.weight.device\n",
    "        E = emb_layer.weight        # (V, E)\n",
    "        V, d = E.shape\n",
    "\n",
    "        # Move suffix to same device\n",
    "        z = suffix_z.to(dev)        # (Ls, E)\n",
    "\n",
    "        # ---- Fix dtype mismatch: work in float32 for stability ----\n",
    "        E_f = E.float()             # (V, E) fp32\n",
    "        z_f = z.float()             # (Ls, E) fp32\n",
    "\n",
    "        # Normalize for cosine similarity\n",
    "        E_norm = F.normalize(E_f, dim=-1)        # (V, E)\n",
    "        z_norm = F.normalize(z_f, dim=-1)        # (Ls, E)\n",
    "\n",
    "        # Cosine similarity: (V, E) @ (E, Ls) -> (V, Ls)\n",
    "        cos_sim = torch.matmul(E_norm, z_norm.T)  # (V, Ls)\n",
    "\n",
    "        # For each suffix position, get best matching token\n",
    "        best_token_ids = cos_sim.argmax(dim=0)    # (Ls,)\n",
    "\n",
    "        # Diagnostics: L2 distances between z[i] and E[best_token_ids[i]]\n",
    "        nearest_embs = E_f[best_token_ids]        # (Ls, E) fp32\n",
    "        l2_dists = (z_f - nearest_embs).norm(dim=-1)  # (Ls,)\n",
    "\n",
    "        print(\"L2 distance between optimized embeddings and nearest token embeddings:\")\n",
    "        print(f\"  min:  {l2_dists.min().item():.6f}\")\n",
    "        print(f\"  max:  {l2_dists.max().item():.6f}\")\n",
    "        print(f\"  mean: {l2_dists.mean().item():.6f}\")\n",
    "\n",
    "        best_cos = cos_sim.max(dim=0).values     # (Ls,)\n",
    "        print(\"Cosine similarity of optimized embeddings to nearest tokens:\")\n",
    "        print(f\"  min:  {best_cos.min().item():.6f}\")\n",
    "        print(f\"  max:  {best_cos.max().item():.6f}\")\n",
    "        print(f\"  mean: {best_cos.mean().item():.6f}\")\n",
    "\n",
    "        suffix_token_ids = best_token_ids.cpu()\n",
    "        suffix_tokens = tokenizer.convert_ids_to_tokens(suffix_token_ids.tolist())\n",
    "        suffix_text = tokenizer.decode(\n",
    "            suffix_token_ids.tolist(),\n",
    "            skip_special_tokens=False\n",
    "        )\n",
    "\n",
    "        print(\"\\nProjected discrete suffix token IDs:\", suffix_token_ids.tolist())\n",
    "        print(\"Projected discrete suffix tokens:\", suffix_tokens)\n",
    "        print(\"Projected suffix as text:\", repr(suffix_text))\n",
    "\n",
    "        return suffix_token_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab33110",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_suffix(suffix_z, emb_layer, tokenizer):\n",
    "    \"\"\"\n",
    "    Print projected suffix tokens and diagnostics.\n",
    "    \"\"\"\n",
    "    suffix_token_ids = project_suffix_to_tokens_and_diagnostics(\n",
    "        suffix_z,\n",
    "        emb_layer,\n",
    "        tokenizer,\n",
    "    )\n",
    "    print(\"Projected Suffix Token IDs:\", suffix_token_ids.tolist())\n",
    "    suffix_tokens = tokenizer.convert_ids_to_tokens(suffix_token_ids.tolist())\n",
    "    print(\"Projected Suffix Tokens:\", suffix_tokens)\n",
    "    suffix_text = tokenizer.decode(\n",
    "        suffix_token_ids.tolist(),\n",
    "        skip_special_tokens=False\n",
    "    )\n",
    "    print(\"Projected Suffix Text:\", repr(suffix_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f69993",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_suffix_pt(filepath: str) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Read suffix embeddings from a .pt file.\n",
    "    \"\"\"\n",
    "    suffix_z = torch.load(filepath)\n",
    "    return suffix_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc50bc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    suffix_z = read_suffix_pt(f\"/kaggle/working/hotflip/rounds/suffix_r{i}_e2.pt\")\n",
    "    print(f\"Read suffix{i} successfully.\")\n",
    "    print_suffix(suffix_z, emb_layer, tokenizer)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
