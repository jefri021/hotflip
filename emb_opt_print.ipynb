{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5dde332",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T14:02:21.750882Z",
     "iopub.status.busy": "2025-11-20T14:02:21.750651Z",
     "iopub.status.idle": "2025-11-20T14:02:28.092614Z",
     "shell.execute_reply": "2025-11-20T14:02:28.091596Z",
     "shell.execute_reply.started": "2025-11-20T14:02:21.750855Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working\n",
      "Cloning into 'hotflip'...\n",
      "remote: Enumerating objects: 276, done.\u001b[K\n",
      "remote: Counting objects: 100% (64/64), done.\u001b[K\n",
      "remote: Compressing objects: 100% (64/64), done.\u001b[K\n",
      "remote: Total 276 (delta 0), reused 63 (delta 0), pack-reused 212 (from 1)\u001b[K\n",
      "Receiving objects: 100% (276/276), 21.43 MiB | 18.95 MiB/s, done.\n",
      "Resolving deltas: 100% (94/94), done.\n",
      "/kaggle/working/hotflip\n",
      "From https://github.com/jefri021/hotflip\n",
      " * branch            main       -> FETCH_HEAD\n",
      "Already up to date.\n"
     ]
    }
   ],
   "source": [
    "def refresh_repo():\n",
    "    %cd /kaggle/working\n",
    "    %rm -rf hotflip\n",
    "    !git clone https://github.com/jefri021/hotflip.git\n",
    "    %cd /kaggle/working/hotflip/\n",
    "    !git pull origin main\n",
    "\n",
    "refresh_repo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4581d65d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T14:02:28.095092Z",
     "iopub.status.busy": "2025-11-20T14:02:28.094727Z",
     "iopub.status.idle": "2025-11-20T14:04:15.054278Z",
     "shell.execute_reply": "2025-11-20T14:04:15.053458Z",
     "shell.execute_reply.started": "2025-11-20T14:02:28.095054Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "2025-11-25 17:41:51.161017: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1764092511.319970      87 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1764092511.365428      87 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fed26318d634938926e8b22a7f9caa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 0, 'model.layers.9': 0, 'model.layers.10': 0, 'model.layers.11': 0, 'model.layers.12': 0, 'model.layers.13': 0, 'model.layers.14': 0, 'model.layers.15': 0, 'model.layers.16': 1, 'model.layers.17': 1, 'model.layers.18': 1, 'model.layers.19': 1, 'model.layers.20': 1, 'model.layers.21': 1, 'model.layers.22': 1, 'model.layers.23': 1, 'model.layers.24': 1, 'model.layers.25': 1, 'model.layers.26': 1, 'model.layers.27': 1, 'model.layers.28': 1, 'model.layers.29': 1, 'model.layers.30': 1, 'model.layers.31': 1, 'model.norm': 1, 'model.rotary_emb': 1, 'lm_head': 1}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "import os\n",
    "import logging\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def load_model(model_filepath: str, torch_dtype:torch.dtype=torch.float16):\n",
    "    \"\"\"Load a model given a specific model_path.\n",
    "\n",
    "    Args:\n",
    "        model_filepath: str - Path to where the model is stored\n",
    "\n",
    "    Returns:\n",
    "        model, dict, str - Torch model + dictionary representation of the model + model class name\n",
    "    \"\"\"\n",
    "\n",
    "    conf_filepath = os.path.join(model_filepath, 'reduced-config.json')\n",
    "    logging.info(\"Loading config file from: {}\".format(conf_filepath))\n",
    "    with open(conf_filepath, 'r') as fh:\n",
    "        round_config = json.load(fh)\n",
    "\n",
    "    logging.info(\"Loading model from filepath: {}\".format(model_filepath))\n",
    "    # https://huggingface.co/docs/transformers/installation#offline-mode\n",
    "    if round_config['use_lora']:\n",
    "        base_model_filepath = os.path.join(model_filepath, 'base-model')\n",
    "        logging.info(\"loading the base model (before LORA) from {}\".format(base_model_filepath))\n",
    "        model = AutoModelForCausalLM.from_pretrained(base_model_filepath, device_map = \"auto\", trust_remote_code=True, torch_dtype=torch_dtype, local_files_only=True)\n",
    "        # model = AutoModelForCausalLM.from_pretrained(round_config['model_architecture'], trust_remote_code=True, attn_implementation=\"flash_attention_2\", torch_dtype=torch_dtype)\n",
    "\n",
    "        fine_tuned_model_filepath = os.path.join(model_filepath, 'fine-tuned-model')\n",
    "        logging.info(\"loading the LORA adapter onto the base model from {}\".format(fine_tuned_model_filepath))\n",
    "        model.load_adapter(fine_tuned_model_filepath)\n",
    "    else:\n",
    "        fine_tuned_model_filepath = os.path.join(model_filepath, 'fine-tuned-model')\n",
    "        logging.info(\"Loading full fine tune checkpoint into cpu from {}\".format(fine_tuned_model_filepath))\n",
    "        model = AutoModelForCausalLM.from_pretrained(fine_tuned_model_filepath, device_map = \"auto\", trust_remote_code=True, torch_dtype=torch_dtype, local_files_only=True)\n",
    "        # model = AutoModelForCausalLM.from_pretrained(fine_tuned_model_filepath, trust_remote_code=True, attn_implementation=\"flash_attention_2\", torch_dtype=torch_dtype)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    tokenizer_filepath = os.path.join(model_filepath, 'tokenizer')\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_filepath)\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def _two_gpu_max_memory(headroom_gb=2):\n",
    "    \"\"\"\n",
    "    Reserve headroom so HF sharding MUST split across both 16GB T4s.\n",
    "    \"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        return None\n",
    "    n = torch.cuda.device_count()\n",
    "    cap = f\"{16 - headroom_gb}GiB\"  # e.g., \"14GiB\"\n",
    "    return {i: cap for i in range(n)}\n",
    "\n",
    "def _common_from_pretrained_kwargs():\n",
    "    \"\"\"\n",
    "    Settings that reduce both CPU and GPU peak memory and use a lean attention impl.\n",
    "    \"\"\"\n",
    "    kw = dict(\n",
    "        trust_remote_code=True,\n",
    "        local_files_only=True,\n",
    "        torch_dtype=torch.float16,     # T4 → FP16\n",
    "        low_cpu_mem_usage=True,        # streaming load\n",
    "        offload_state_dict=True,       # avoid CPU spikes\n",
    "        attn_implementation=\"sdpa\",    # available by default on Kaggle\n",
    "    )\n",
    "    mm = _two_gpu_max_memory(headroom_gb=2)\n",
    "    if mm and torch.cuda.device_count() > 1:\n",
    "        kw[\"device_map\"] = \"auto\"\n",
    "        kw[\"max_memory\"] = mm\n",
    "        # Optional if host RAM is tight:\n",
    "        # kw[\"offload_folder\"] = \"/kaggle/working/offload\"\n",
    "    else:\n",
    "        kw[\"device_map\"] = {\"\": 0}\n",
    "    return kw\n",
    "\n",
    "def load_model_and_tokenizer(model_dir: str, merge_lora: bool = True):\n",
    "    \"\"\"\n",
    "    Robust loader for full fine-tunes or LoRA adapters stored under `model_dir`.\n",
    "    Expects:\n",
    "      - reduced-config.json with {\"use_lora\": <bool>, ...}\n",
    "      - For LoRA: base-model/, fine-tuned-model/\n",
    "      - For full FT: fine-tuned-model/\n",
    "      - tokenizer/ with tokenizer files\n",
    "    Returns: (model, tokenizer)\n",
    "    \"\"\"\n",
    "    conf_path = os.path.join(model_dir, \"reduced-config.json\")\n",
    "    logging.info(f\"Loading config: {conf_path}\")\n",
    "    with open(conf_path, \"r\") as fh:\n",
    "        cfg = json.load(fh)\n",
    "\n",
    "    kw = _common_from_pretrained_kwargs()\n",
    "\n",
    "    if cfg.get(\"use_lora\", False):\n",
    "        base_dir = os.path.join(model_dir, \"base-model\")\n",
    "        lora_dir = os.path.join(model_dir, \"fine-tuned-model\")\n",
    "\n",
    "        logging.info(f\"Loading base model: {base_dir}\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(base_dir, **kw)\n",
    "        logging.info(f\"Attaching LoRA adapter: {lora_dir}\")\n",
    "        # If PeftModel is missing, use .load_adapter if available\n",
    "        try:\n",
    "            model = PeftModel.from_pretrained(model, lora_dir, is_trainable=False)  # type: ignore\n",
    "        except Exception:\n",
    "            model.load_adapter(lora_dir)\n",
    "\n",
    "    else:\n",
    "        ft_dir = os.path.join(model_dir, \"fine-tuned-model\")\n",
    "        logging.info(f\"Loading full fine-tuned model: {ft_dir}\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(ft_dir, **kw)\n",
    "\n",
    "    # Tokenizer hygiene\n",
    "    tok_dir = os.path.join(model_dir, \"tokenizer\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tok_dir, use_fast=True, local_files_only=True)\n",
    "    if tokenizer.pad_token_id is None and tokenizer.eos_token_id is not None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"  # better for causal LMs with dynamic padding\n",
    "\n",
    "    # Runtime memory knobs for your gradient-based rollout\n",
    "    model.eval()\n",
    "    if hasattr(model.config, \"use_cache\"):\n",
    "        model.config.use_cache = False  # reduce KV/activation memory during your search\n",
    "\n",
    "    # Optional: quick sanity check of sharding\n",
    "    try:\n",
    "        print(getattr(model, \"hf_device_map\", \"no device map\"))\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "model, tokenizer = load_model_and_tokenizer(\n",
    "    model_dir=\"/kaggle/input/trojai-rev2-00000001/id-00000001\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fde7343",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T14:04:15.055831Z",
     "iopub.status.busy": "2025-11-20T14:04:15.055174Z",
     "iopub.status.idle": "2025-11-20T14:04:15.061480Z",
     "shell.execute_reply": "2025-11-20T14:04:15.060691Z",
     "shell.execute_reply.started": "2025-11-20T14:04:15.055810Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_emb_layer(model):\n",
    "    model.eval()\n",
    "    if hasattr(model.config, \"use_cache\"):\n",
    "        model.config.use_cache = False\n",
    "    return model.get_input_embeddings()\n",
    "\n",
    "emb_layer = get_emb_layer(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d7c5689",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T14:04:15.062965Z",
     "iopub.status.busy": "2025-11-20T14:04:15.062265Z",
     "iopub.status.idle": "2025-11-20T14:04:15.106685Z",
     "shell.execute_reply": "2025-11-20T14:04:15.105854Z",
     "shell.execute_reply.started": "2025-11-20T14:04:15.062938Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def project_suffix_to_tokens_and_diagnostics(\n",
    "    suffix_z,\n",
    "    emb_layer,\n",
    "    tokenizer,\n",
    "):\n",
    "    \"\"\"\n",
    "    suffix_z: (Ls, E) - optimized continuous suffix embeddings\n",
    "    emb_layer: model.get_input_embeddings()\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        dev = emb_layer.weight.device\n",
    "        E = emb_layer.weight        # (V, E)\n",
    "        V, d = E.shape\n",
    "\n",
    "        # Move suffix to same device\n",
    "        z = suffix_z.to(dev)        # (Ls, E)\n",
    "\n",
    "        # ---- Fix dtype mismatch: work in float32 for stability ----\n",
    "        E_f = E.float()             # (V, E) fp32\n",
    "        z_f = z.float()             # (Ls, E) fp32\n",
    "\n",
    "        # Normalize for cosine similarity\n",
    "        E_norm = F.normalize(E_f, dim=-1)        # (V, E)\n",
    "        z_norm = F.normalize(z_f, dim=-1)        # (Ls, E)\n",
    "\n",
    "        # Cosine similarity: (V, E) @ (E, Ls) -> (V, Ls)\n",
    "        cos_sim = torch.matmul(E_norm, z_norm.T)  # (V, Ls)\n",
    "\n",
    "        # For each suffix position, get best matching token\n",
    "        best_token_ids = cos_sim.argmax(dim=0)    # (Ls,)\n",
    "\n",
    "        # Diagnostics: L2 distances between z[i] and E[best_token_ids[i]]\n",
    "        nearest_embs = E_f[best_token_ids]        # (Ls, E) fp32\n",
    "        l2_dists = (z_f - nearest_embs).norm(dim=-1)  # (Ls,)\n",
    "\n",
    "        print(\"L2 distance between optimized embeddings and nearest token embeddings:\")\n",
    "        print(f\"  min:  {l2_dists.min().item():.6f}\")\n",
    "        print(f\"  max:  {l2_dists.max().item():.6f}\")\n",
    "        print(f\"  mean: {l2_dists.mean().item():.6f}\")\n",
    "\n",
    "        best_cos = cos_sim.max(dim=0).values     # (Ls,)\n",
    "        print(\"Cosine similarity of optimized embeddings to nearest tokens:\")\n",
    "        print(f\"  min:  {best_cos.min().item():.6f}\")\n",
    "        print(f\"  max:  {best_cos.max().item():.6f}\")\n",
    "        print(f\"  mean: {best_cos.mean().item():.6f}\")\n",
    "\n",
    "        suffix_token_ids = best_token_ids.cpu()\n",
    "        suffix_tokens = tokenizer.convert_ids_to_tokens(suffix_token_ids.tolist())\n",
    "        suffix_text = tokenizer.decode(\n",
    "            suffix_token_ids.tolist(),\n",
    "            skip_special_tokens=False\n",
    "        )\n",
    "\n",
    "        print(\"\\nProjected discrete suffix token IDs:\", suffix_token_ids.tolist())\n",
    "        print(\"Projected discrete suffix tokens:\", suffix_tokens)\n",
    "        print(\"Projected suffix as text:\", repr(suffix_text))\n",
    "\n",
    "        return suffix_token_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0f69993",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T14:04:15.124058Z",
     "iopub.status.busy": "2025-11-20T14:04:15.123891Z",
     "iopub.status.idle": "2025-11-20T14:04:15.135689Z",
     "shell.execute_reply": "2025-11-20T14:04:15.134941Z",
     "shell.execute_reply.started": "2025-11-20T14:04:15.124044Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def read_suffix_pt(filepath: str) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Read suffix embeddings from a .pt file.\n",
    "    \"\"\"\n",
    "    suffix_z = torch.load(filepath)\n",
    "    return suffix_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be447ee5-5706-40bd-9f93-1c9fade94c57",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T14:08:14.086171Z",
     "iopub.status.busy": "2025-11-20T14:08:14.085302Z",
     "iopub.status.idle": "2025-11-20T14:08:14.098286Z",
     "shell.execute_reply": "2025-11-20T14:08:14.097374Z",
     "shell.execute_reply.started": "2025-11-20T14:08:14.086143Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch import amp\n",
    "\n",
    "def entropy_loss(batch_logits):\n",
    "    \"\"\"\n",
    "    batch_logits: (B, V) logits for the token of interest.\n",
    "    Returns scalar mean entropy.\n",
    "    \"\"\"\n",
    "    log_probs = F.log_softmax(batch_logits, dim=-1)\n",
    "    probs = log_probs.exp()\n",
    "    entropy = -(probs * log_probs).sum(dim=-1)  # (B,)\n",
    "    return entropy.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5199013",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_for_suffix(\n",
    "    model,\n",
    "    emb_layer,\n",
    "    batch,\n",
    "    suffix_z,           # (Ls, E) nn.Parameter\n",
    "    n_tokens=10,\n",
    "    amp_dtype=torch.float16,\n",
    "    cos_reg_weight=0.1,\n",
    "    E_norm_cpu=None,    # (V, E) on CPU, fp32\n",
    "    chunk_size=1024,\n",
    "    top_k=5,\n",
    "    neg_weight=1.0,     # how strongly to push away from non-top-k\n",
    "):\n",
    "    \"\"\"\n",
    "    - For each example, build [prompt][suffix_z] in embedding space.\n",
    "    - Pad all to same length -> [prompt][suffix][PAD].\n",
    "    - Roll out n_tokens-1 tokens under inference_mode.\n",
    "    - Final forward WITH grad gives entropy loss on last generated token.\n",
    "    - Gradients flow into suffix_z only (prompts are detached).\n",
    "    - PLUS: regularizer that pulls suffix_z toward real token embeddings via cosine similarity.\n",
    "    \"\"\"\n",
    "    prompts = batch[\"input_ids\"]   # list of 1D LongTensors (Li,)\n",
    "    dev = emb_layer.weight.device\n",
    "    suffix_z = suffix_z.to(dev)    # (Ls, E)\n",
    "\n",
    "    B = len(prompts)\n",
    "    Ls, E = suffix_z.shape\n",
    "\n",
    "    base_embs = []   # each: (Li+Ls, E)\n",
    "    base_lens = []   # each: scalar length Li+Ls\n",
    "\n",
    "    # --- Build per-example [prompt][suffix] in embedding space ---\n",
    "    for p_ids in prompts:\n",
    "        p_ids_dev = p_ids.to(dev)\n",
    "        p_emb = emb_layer(p_ids_dev).detach()   # (Li, E), prompts are constants\n",
    "        base = torch.cat([p_emb, suffix_z], dim=0)  # (Li+Ls, E)\n",
    "        base_embs.append(base)\n",
    "        base_lens.append(base.size(0))\n",
    "\n",
    "    # Pad to [prompt][suffix][PAD...] across the batch\n",
    "    base = pad_sequence(base_embs, batch_first=True)   # (B, max_len, E)\n",
    "    base_lens = torch.tensor(base_lens, device=dev)    # (B,)\n",
    "    max_len = base.size(1)\n",
    "\n",
    "    # Attention mask: 1 for real tokens, 0 for pad\n",
    "    arange = torch.arange(max_len, device=dev).unsqueeze(0)  # (1, max_len)\n",
    "    base_mask = (arange < base_lens.unsqueeze(1)).long()     # (B, max_len)\n",
    "\n",
    "    # Now base has structure [prompt][suffix][PAD] per row (masked pads)\n",
    "\n",
    "    def _one_step_logits(e, m):\n",
    "        with amp.autocast(\"cuda\", dtype=amp_dtype):\n",
    "            out = model(\n",
    "                inputs_embeds=e,\n",
    "                attention_mask=m,\n",
    "                use_cache=False,\n",
    "                output_attentions=False,\n",
    "                output_hidden_states=False,\n",
    "                return_dict=True,\n",
    "            )\n",
    "        return out.logits[:, -1, :]  # (B, V)\n",
    "\n",
    "    # ---------- Rollout under no grad (from detached base) ----------\n",
    "    work_e = base.detach()  # rollout uses constants\n",
    "    work_m = base_mask\n",
    "    added_embs = []         # list of (B, E) constants\n",
    "    \n",
    "    T = max(0, n_tokens - 1)\n",
    "    with torch.inference_mode():\n",
    "        for _ in range(T):\n",
    "            logits_t = _one_step_logits(work_e, work_m)\n",
    "            probs_t  = torch.softmax(logits_t, dim=-1)\n",
    "            next_ids = torch.argmax(probs_t, dim=-1)        # (B,)\n",
    "    \n",
    "            next_emb = emb_layer(next_ids.to(dev)).detach() # (B, E)\n",
    "            added_embs.append(next_emb)\n",
    "    \n",
    "            work_e = torch.cat([work_e, next_emb.unsqueeze(1)], dim=1)\n",
    "            work_m = torch.cat(\n",
    "                [work_m, torch.ones((B, 1), dtype=work_m.dtype, device=dev)],\n",
    "                dim=1,\n",
    "            )\n",
    "    \n",
    "    # ---------- Final inputs: [prompt][suffix][PAD] + generated tokens ----------\n",
    "    if len(added_embs) > 0:\n",
    "        added = torch.stack(added_embs, dim=1)              # (B, T, E)\n",
    "        final_emb = torch.cat([base, added], dim=1)         # (B, max_len+T, E)\n",
    "        gen_mask = torch.ones((B, T), dtype=base_mask.dtype, device=dev)\n",
    "        final_mask = torch.cat([base_mask, gen_mask], dim=1)\n",
    "    else:\n",
    "        final_emb = base\n",
    "        final_mask = base_mask\n",
    "    \n",
    "    # ---------- Forward WITH grad for ALL n_tokens steps ----------\n",
    "    with amp.autocast(\"cuda\", dtype=amp_dtype):\n",
    "        out = model(\n",
    "            inputs_embeds=final_emb,\n",
    "            attention_mask=final_mask,\n",
    "            use_cache=False,\n",
    "            output_attentions=False,\n",
    "            output_hidden_states=False,\n",
    "            return_dict=True,\n",
    "        )\n",
    "    \n",
    "    logits_all = out.logits   # (B, L_total, V)\n",
    "    B, L_total, V = logits_all.shape\n",
    "    \n",
    "    # base_lens: (B,) lengths of [prompt][suffix] BEFORE generated tokens\n",
    "    # we want logits for:\n",
    "    #   step 1: position base_len - 1  (first next token)\n",
    "    #   step 2: position base_len      (second next)\n",
    "    #   ...\n",
    "    #   step n_tokens: position base_len - 1 + (n_tokens - 1) = base_len + T - 1\n",
    "    # So indices: [base_len - 1, base_len, ..., base_len + T - 1], length = n_tokens\n",
    "    \n",
    "    all_step_logits = []\n",
    "    \n",
    "    for b in range(B):\n",
    "        blen = base_lens[b].item()  # length of base for this example\n",
    "    \n",
    "        # safety: don't go past sequence length\n",
    "        # we know we have exactly T generated tokens, so there are n_tokens positions:\n",
    "        # indices from blen-1 to blen-1+T (inclusive)\n",
    "        start_idx = blen - 1\n",
    "        end_idx   = blen - 1 + T    # inclusive\n",
    "        # this yields exactly n_tokens positions when T = n_tokens-1\n",
    "    \n",
    "        idxs = torch.arange(start_idx, end_idx + 1, device=dev)  # (n_tokens,)\n",
    "        # gather logits for this example's steps: (n_tokens, V)\n",
    "        step_logits_b = logits_all[b, idxs, :]                   # (n_tokens, V)\n",
    "        all_step_logits.append(step_logits_b)\n",
    "    \n",
    "    # stack over batch: (B, n_tokens, V) -> (B*n_tokens, V)\n",
    "    logits_for_loss = torch.cat(all_step_logits, dim=0)  # (B*n_tokens, V)\n",
    "\n",
    "    # print(f\"hey: {logits_for_loss.shape}\")\n",
    "    \n",
    "    # mean entropy over all n_tokens steps for all examples\n",
    "    ent = entropy_loss(logits_for_loss)\n",
    "\n",
    "    \n",
    "    dev = suffix_z.device\n",
    "    Ls = suffix_z.size(0)\n",
    "    V = E_norm_cpu.size(0)\n",
    "\n",
    "    # normalized suffix embeddings on GPU, fp32, with grad\n",
    "    z_norm = F.normalize(suffix_z.float(), dim=-1)  # (Ls, E)\n",
    "\n",
    "    # running sum of cosines across all vocab, per suffix position\n",
    "    sum_cos_per_pos = None          # (Ls,)\n",
    "    # running top-k cosines across vocab, per suffix position\n",
    "    topk_vals = None                # (top_k, Ls)\n",
    "\n",
    "    for start in range(0, V, chunk_size):\n",
    "        end = min(start + chunk_size, V)\n",
    "        # chunk: (c, E) fp32 on GPU, no grad\n",
    "        chunk = E_norm_cpu[start:end].to(dev, non_blocking=True)  # (c, E)\n",
    "\n",
    "        # (c, E) @ (E, Ls) -> (c, Ls)\n",
    "        chunk_sim = torch.matmul(chunk, z_norm.T)  # (c, Ls)\n",
    "\n",
    "        # ---- accumulate sum over ALL vocab ----\n",
    "        chunk_sum = chunk_sim.sum(dim=0)          # (Ls,)\n",
    "        if sum_cos_per_pos is None:\n",
    "            sum_cos_per_pos = chunk_sum\n",
    "        else:\n",
    "            sum_cos_per_pos = sum_cos_per_pos + chunk_sum\n",
    "\n",
    "        # ---- maintain global top-k over vocab ----\n",
    "        # top-k within this chunk: (top_k, Ls)\n",
    "        # (if chunk smaller than top_k, it handles automatically)\n",
    "        chunk_topk, _ = chunk_sim.topk(min(top_k, chunk_sim.size(0)), dim=0)\n",
    "\n",
    "        if topk_vals is None:\n",
    "            # pad if first chunk smaller than k\n",
    "            if chunk_topk.size(0) < top_k:\n",
    "                pad_rows = top_k - chunk_topk.size(0)\n",
    "                pad = torch.full(\n",
    "                    (pad_rows, Ls),\n",
    "                    -1e9,\n",
    "                    device=dev,\n",
    "                    dtype=chunk_topk.dtype,\n",
    "                )\n",
    "                topk_vals = torch.cat([chunk_topk, pad], dim=0)  # (top_k, Ls)\n",
    "            else:\n",
    "                topk_vals = chunk_topk                         # (top_k, Ls)\n",
    "        else:\n",
    "            # combine previous topk with this chunk's topk and keep best k\n",
    "            combined = torch.cat([topk_vals, chunk_topk], dim=0)  # (prev_k + c_k, Ls)\n",
    "            topk_vals, _ = combined.topk(top_k, dim=0)            # (top_k, Ls)\n",
    "\n",
    "        # free small temps\n",
    "        del chunk, chunk_sim, chunk_sum, chunk_topk\n",
    "\n",
    "    # ---- Compute means for top-k vs others ----\n",
    "    # mean cosine over ALL vocab per position\n",
    "    mean_all_per_pos = sum_cos_per_pos / float(V)        # (Ls,)\n",
    "\n",
    "    # mean cosine over top-k per position\n",
    "    mean_topk_per_pos = topk_vals.mean(dim=0)            # (Ls,)\n",
    "\n",
    "    # mean cosine over non-topk (\"others\"):\n",
    "    # mean_others = (V * mean_all - k * mean_topk) / (V - k)\n",
    "    mean_others_per_pos = (V * mean_all_per_pos - top_k * mean_topk_per_pos) / max(\n",
    "        1.0, (V - top_k)\n",
    "    )\n",
    "\n",
    "    # collapse over suffix positions\n",
    "    mean_topk = mean_topk_per_pos.mean()                 # scalar\n",
    "    mean_others = mean_others_per_pos.mean()             # scalar\n",
    "\n",
    "    # ---- Build regularizer ----\n",
    "    # 1) pull suffix toward the top-k tokens  (maximize mean_topk)\n",
    "    reg_pos = (1.0 - mean_topk)  # small when close to top-k\n",
    "\n",
    "    # 2) push suffix away from others (want others' cosine <= 0 ideally)\n",
    "    # penalize positive mean_others; if mean_others <= 0, no penalty\n",
    "    # reg_neg = torch.relu(mean_others)  # encourages mean_others → <= 0\n",
    "    reg_neg = mean_others\n",
    "\n",
    "    cos_reg = reg_pos + neg_weight * reg_neg\n",
    "\n",
    "    total_loss = ent + cos_reg_weight * cos_reg\n",
    "\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1339af0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from datasets import load_dataset\n",
    "\n",
    "def load_prompts_unpadded(tokenizer, args):\n",
    "    \"\"\"\n",
    "    Returns DataLoader where each batch is:\n",
    "      {\n",
    "        \"input_ids\": list of 1D LongTensors (prompts, no padding),\n",
    "        \"prompt_lens\": LongTensor (B,)\n",
    "      }\n",
    "    \"\"\"\n",
    "    ds = load_dataset(\"tatsu-lab/alpaca\", split=\"train\", cache_dir=args[\"data_dir\"])\n",
    "\n",
    "    # Subsample for speed\n",
    "    if \"sample_size\" in args and args[\"sample_size\"] is not None and args[\"sample_size\"] < len(ds):\n",
    "        ds = ds.shuffle(seed=42).select(range(args[\"sample_size\"]))\n",
    "\n",
    "    def collate(batch):\n",
    "        texts = [ex[\"instruction\"] for ex in batch]\n",
    "        enc = tokenizer(\n",
    "            texts,\n",
    "            padding=False,\n",
    "            truncation=True,\n",
    "            max_length=args[\"max_length\"],\n",
    "        )\n",
    "        prompts = [torch.tensor(ids, dtype=torch.long) for ids in enc[\"input_ids\"]]\n",
    "        prompt_lens = [len(p) for p in prompts]\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": prompts,  # list of (Li,)\n",
    "            \"prompt_lens\": torch.tensor(prompt_lens, dtype=torch.long),\n",
    "        }\n",
    "\n",
    "    num_workers = max(2, os.cpu_count() // 2)\n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size=args[\"batch_size\"],\n",
    "        shuffle=True,\n",
    "        pin_memory=True,\n",
    "        num_workers=num_workers,\n",
    "        persistent_workers=True,\n",
    "        collate_fn=collate,\n",
    "    )\n",
    "\n",
    "dataloader_args = {\n",
    "    \"data_dir\": \"/kaggle/working/datasets\",\n",
    "    \"batch_size\": 1,\n",
    "    \"max_length\": 128,\n",
    "    \"sample_size\": 128,\n",
    "}\n",
    "\n",
    "dataloader = load_prompts_unpadded(tokenizer, dataloader_args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53d6012",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_suffix(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt_ids: torch.Tensor,        # (L_prompt,)\n",
    "    suffix_token_ids: torch.Tensor,  # (Ls,)\n",
    "    max_new_tokens: int = 50,\n",
    "):\n",
    "    \"\"\"\n",
    "    Concatenate [prompt][suffix] and let the model generate continuation.\n",
    "    \"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    full_input_ids = torch.cat(\n",
    "        [prompt_ids, suffix_token_ids.to(prompt_ids.device)],\n",
    "        dim=0\n",
    "    ).unsqueeze(0).to(device)  # (1, L_total)\n",
    "\n",
    "    attn_mask = torch.ones_like(full_input_ids, dtype=torch.long, device=device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        generated = model.generate(\n",
    "            input_ids=full_input_ids,\n",
    "            attention_mask=attn_mask,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    return tokenizer.decode(generated[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c65bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def log_suffix_token_probs(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt_ids: torch.Tensor,        # (L_prompt,)\n",
    "    suffix_token_ids: torch.Tensor,  # (Ls,)\n",
    "):\n",
    "    \"\"\"\n",
    "    For each position k in the suffix, compute P(suffix[k] | prompt + suffix[:k])\n",
    "    and print it.\n",
    "    \"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    model.eval()\n",
    "\n",
    "    prompt_ids = prompt_ids.to(device)\n",
    "    suffix_token_ids = suffix_token_ids.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for k in range(suffix_token_ids.size(0)):\n",
    "            ctx_suffix = suffix_token_ids[:k]      # (k,)\n",
    "            ctx = torch.cat([prompt_ids, ctx_suffix], dim=0)  # (L_prompt + k,)\n",
    "\n",
    "            inp = ctx.unsqueeze(0)  # (1, L_ctx)\n",
    "            msk = torch.ones_like(inp, dtype=torch.long, device=device)\n",
    "\n",
    "            out = model(\n",
    "                input_ids=inp,\n",
    "                attention_mask=msk,\n",
    "                use_cache=False,\n",
    "                return_dict=True,\n",
    "            )\n",
    "            logits_next = out.logits[:, -1, :]  # (1, V)\n",
    "            probs_next = F.softmax(logits_next, dim=-1)  # (1, V)\n",
    "\n",
    "            tok_id = suffix_token_ids[k]\n",
    "            prob = probs_next[0, tok_id].item()\n",
    "            tok_str = tokenizer.decode([tok_id])\n",
    "\n",
    "            print(f\"  pos {k:2d}, token {tok_id:5d} ({repr(tok_str)}), prob: {prob:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc50bc60",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T14:16:59.120215Z",
     "iopub.status.busy": "2025-11-20T14:16:59.119578Z",
     "iopub.status.idle": "2025-11-20T14:17:06.411777Z",
     "shell.execute_reply": "2025-11-20T14:17:06.410871Z",
     "shell.execute_reply.started": "2025-11-20T14:16:59.120186Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read suffix_r0_e0 successfully.\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  3.661808\n",
      "  max:  6.397484\n",
      "  mean: 4.828564\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.280621\n",
      "  max:  0.610969\n",
      "  mean: 0.440911\n",
      "\n",
      "Projected discrete suffix token IDs: [28354, 2178, 15316, 7058, 30334, 4320, 3685, 28354, 5032, 14087]\n",
      "Projected discrete suffix tokens: ['▁Расподела', '▁All', '▁Samuel', 'That', 'Ñ', '▁cast', '▁Sam', '▁Расподела', '▁Bra', '~~~~']\n",
      "Projected suffix as text: 'Расподела All SamuelThatÑ cast Sam Расподела Bra~~~~'\n",
      "suffix loss (before projection): 0.0012531280517578125\n",
      "suffix loss (after projection): 4.7109375\n",
      "\n",
      "\n",
      "Read suffix_r0_e1 successfully.\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  3.422127\n",
      "  max:  6.299562\n",
      "  mean: 4.692532\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.391027\n",
      "  max:  0.669275\n",
      "  mean: 0.507330\n",
      "\n",
      "Projected discrete suffix token IDs: [28354, 2178, 15316, 7058, 30334, 4320, 3685, 28354, 5032, 14087]\n",
      "Projected discrete suffix tokens: ['▁Расподела', '▁All', '▁Samuel', 'That', 'Ñ', '▁cast', '▁Sam', '▁Расподела', '▁Bra', '~~~~']\n",
      "Projected suffix as text: 'Расподела All SamuelThatÑ cast Sam Расподела Bra~~~~'\n",
      "suffix loss (before projection): 0.01389312744140625\n",
      "suffix loss (after projection): 4.7109375\n",
      "\n",
      "\n",
      "Read suffix_r0_e2 successfully.\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  6.670686\n",
      "  max:  9.710648\n",
      "  mean: 7.854626\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.272900\n",
      "  max:  0.451256\n",
      "  mean: 0.364843\n",
      "\n",
      "Projected discrete suffix token IDs: [28354, 2193, 3148, 7058, 30334, 31655, 3685, 28354, 5032, 14087]\n",
      "Projected discrete suffix tokens: ['▁Расподела', '▁That', '▁US', 'That', 'Ñ', '局', '▁Sam', '▁Расподела', '▁Bra', '~~~~']\n",
      "Projected suffix as text: 'Расподела That USThatÑ局 Sam Расподела Bra~~~~'\n",
      "suffix loss (before projection): 0.5166015625\n",
      "suffix loss (after projection): 0.0012302398681640625\n",
      "\n",
      "\n",
      "Read suffix_r0_e3 successfully.\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  6.685935\n",
      "  max:  9.843979\n",
      "  mean: 7.920621\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.272288\n",
      "  max:  0.464575\n",
      "  mean: 0.375298\n",
      "\n",
      "Projected discrete suffix token IDs: [28354, 2193, 3148, 7058, 30334, 31655, 3685, 28354, 5032, 14087]\n",
      "Projected discrete suffix tokens: ['▁Расподела', '▁That', '▁US', 'That', 'Ñ', '局', '▁Sam', '▁Расподела', '▁Bra', '~~~~']\n",
      "Projected suffix as text: 'Расподела That USThatÑ局 Sam Расподела Bra~~~~'\n",
      "suffix loss (before projection): 0.71142578125\n",
      "suffix loss (after projection): 0.0012302398681640625\n",
      "\n",
      "\n",
      "Read suffix_r0_e4 successfully.\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  6.684031\n",
      "  max:  9.846721\n",
      "  mean: 7.922593\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.273971\n",
      "  max:  0.482681\n",
      "  mean: 0.384134\n",
      "\n",
      "Projected discrete suffix token IDs: [28354, 2193, 3148, 7058, 30334, 4320, 3685, 28354, 5032, 14087]\n",
      "Projected discrete suffix tokens: ['▁Расподела', '▁That', '▁US', 'That', 'Ñ', '▁cast', '▁Sam', '▁Расподела', '▁Bra', '~~~~']\n",
      "Projected suffix as text: 'Расподела That USThatÑ cast Sam Расподела Bra~~~~'\n",
      "suffix loss (before projection): 0.0006976127624511719\n",
      "suffix loss (after projection): 5.8984375\n",
      "\n",
      "\n",
      "Read suffix_r0_e5 successfully.\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  6.686121\n",
      "  max:  9.860240\n",
      "  mean: 7.927286\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.275262\n",
      "  max:  0.491912\n",
      "  mean: 0.388313\n",
      "\n",
      "Projected discrete suffix token IDs: [28354, 2193, 3148, 7058, 30334, 4320, 3685, 28354, 5032, 14087]\n",
      "Projected discrete suffix tokens: ['▁Расподела', '▁That', '▁US', 'That', 'Ñ', '▁cast', '▁Sam', '▁Расподела', '▁Bra', '~~~~']\n",
      "Projected suffix as text: 'Расподела That USThatÑ cast Sam Расподела Bra~~~~'\n",
      "suffix loss (before projection): 0.0096588134765625\n",
      "suffix loss (after projection): 5.8984375\n",
      "\n",
      "\n",
      "Read suffix_r0_e6 successfully.\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  6.678993\n",
      "  max:  9.866923\n",
      "  mean: 7.924932\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.276325\n",
      "  max:  0.497563\n",
      "  mean: 0.391126\n",
      "\n",
      "Projected discrete suffix token IDs: [28354, 2193, 3148, 7058, 30334, 4320, 3685, 28354, 5032, 14087]\n",
      "Projected discrete suffix tokens: ['▁Расподела', '▁That', '▁US', 'That', 'Ñ', '▁cast', '▁Sam', '▁Расподела', '▁Bra', '~~~~']\n",
      "Projected suffix as text: 'Расподела That USThatÑ cast Sam Расподела Bra~~~~'\n",
      "suffix loss (before projection): 0.01019287109375\n",
      "suffix loss (after projection): 5.8984375\n",
      "\n",
      "\n",
      "Read suffix_r0_e7 successfully.\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  6.676095\n",
      "  max:  9.866433\n",
      "  mean: 7.923257\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.276874\n",
      "  max:  0.501217\n",
      "  mean: 0.392873\n",
      "\n",
      "Projected discrete suffix token IDs: [28354, 2193, 3148, 7058, 30334, 4320, 3685, 28354, 5032, 14087]\n",
      "Projected discrete suffix tokens: ['▁Расподела', '▁That', '▁US', 'That', 'Ñ', '▁cast', '▁Sam', '▁Расподела', '▁Bra', '~~~~']\n",
      "Projected suffix as text: 'Расподела That USThatÑ cast Sam Расподела Bra~~~~'\n",
      "suffix loss (before projection): 0.0012531280517578125\n",
      "suffix loss (after projection): 5.8984375\n",
      "\n",
      "\n",
      "Read suffix_r0_e8 successfully.\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  6.675658\n",
      "  max:  9.865445\n",
      "  mean: 7.921625\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.277252\n",
      "  max:  0.503617\n",
      "  mean: 0.393999\n",
      "\n",
      "Projected discrete suffix token IDs: [28354, 2193, 3148, 7058, 30334, 4320, 3685, 28354, 5032, 14087]\n",
      "Projected discrete suffix tokens: ['▁Расподела', '▁That', '▁US', 'That', 'Ñ', '▁cast', '▁Sam', '▁Расподела', '▁Bra', '~~~~']\n",
      "Projected suffix as text: 'Расподела That USThatÑ cast Sam Расподела Bra~~~~'\n",
      "suffix loss (before projection): 0.0014104843139648438\n",
      "suffix loss (after projection): 5.8984375\n",
      "\n",
      "\n",
      "Read suffix_r0_e9 successfully.\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  6.675012\n",
      "  max:  9.865209\n",
      "  mean: 7.921007\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.277486\n",
      "  max:  0.504944\n",
      "  mean: 0.394662\n",
      "\n",
      "Projected discrete suffix token IDs: [28354, 2193, 3148, 7058, 30334, 4320, 3685, 28354, 5032, 14087]\n",
      "Projected discrete suffix tokens: ['▁Расподела', '▁That', '▁US', 'That', 'Ñ', '▁cast', '▁Sam', '▁Расподела', '▁Bra', '~~~~']\n",
      "Projected suffix as text: 'Расподела That USThatÑ cast Sam Расподела Bra~~~~'\n",
      "suffix loss (before projection): 0.0014104843139648438\n",
      "suffix loss (after projection): 5.8984375\n",
      "\n",
      "\n",
      "Read suffix_r0_e10 successfully.\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  6.675119\n",
      "  max:  9.864965\n",
      "  mean: 7.920626\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.277654\n",
      "  max:  0.505775\n",
      "  mean: 0.395049\n",
      "\n",
      "Projected discrete suffix token IDs: [28354, 2193, 3148, 7058, 30334, 4320, 3685, 28354, 5032, 14087]\n",
      "Projected discrete suffix tokens: ['▁Расподела', '▁That', '▁US', 'That', 'Ñ', '▁cast', '▁Sam', '▁Расподела', '▁Bra', '~~~~']\n",
      "Projected suffix as text: 'Расподела That USThatÑ cast Sam Расподела Bra~~~~'\n",
      "suffix loss (before projection): 0.0012416839599609375\n",
      "suffix loss (after projection): 5.8984375\n",
      "\n",
      "\n",
      "Read suffix_r0_e11 successfully.\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  6.674740\n",
      "  max:  9.864790\n",
      "  mean: 7.920395\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.277723\n",
      "  max:  0.506231\n",
      "  mean: 0.395263\n",
      "\n",
      "Projected discrete suffix token IDs: [28354, 2193, 3148, 7058, 30334, 4320, 3685, 28354, 5032, 14087]\n",
      "Projected discrete suffix tokens: ['▁Расподела', '▁That', '▁US', 'That', 'Ñ', '▁cast', '▁Sam', '▁Расподела', '▁Bra', '~~~~']\n",
      "Projected suffix as text: 'Расподела That USThatÑ cast Sam Расподела Bra~~~~'\n",
      "suffix loss (before projection): 0.001331329345703125\n",
      "suffix loss (after projection): 5.8984375\n",
      "\n",
      "\n",
      "Read suffix_r0_e12 successfully.\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  6.674583\n",
      "  max:  9.864635\n",
      "  mean: 7.920279\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.277760\n",
      "  max:  0.506505\n",
      "  mean: 0.395383\n",
      "\n",
      "Projected discrete suffix token IDs: [28354, 2193, 3148, 7058, 30334, 4320, 3685, 28354, 5032, 14087]\n",
      "Projected discrete suffix tokens: ['▁Расподела', '▁That', '▁US', 'That', 'Ñ', '▁cast', '▁Sam', '▁Расподела', '▁Bra', '~~~~']\n",
      "Projected suffix as text: 'Расподела That USThatÑ cast Sam Расподела Bra~~~~'\n",
      "suffix loss (before projection): 0.0013227462768554688\n",
      "suffix loss (after projection): 5.8984375\n",
      "\n",
      "\n",
      "Read suffix_r0_e13 successfully.\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  6.674503\n",
      "  max:  9.864628\n",
      "  mean: 7.920224\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.277774\n",
      "  max:  0.506627\n",
      "  mean: 0.395436\n",
      "\n",
      "Projected discrete suffix token IDs: [28354, 2193, 3148, 7058, 30334, 4320, 3685, 28354, 5032, 14087]\n",
      "Projected discrete suffix tokens: ['▁Расподела', '▁That', '▁US', 'That', 'Ñ', '▁cast', '▁Sam', '▁Расподела', '▁Bra', '~~~~']\n",
      "Projected suffix as text: 'Расподела That USThatÑ cast Sam Расподела Bra~~~~'\n",
      "suffix loss (before projection): 0.0013189315795898438\n",
      "suffix loss (after projection): 5.8984375\n",
      "\n",
      "\n",
      "Read suffix_r0_e14 successfully.\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  6.674459\n",
      "  max:  9.864618\n",
      "  mean: 7.920203\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.277784\n",
      "  max:  0.506692\n",
      "  mean: 0.395466\n",
      "\n",
      "Projected discrete suffix token IDs: [28354, 2193, 3148, 7058, 30334, 4320, 3685, 28354, 5032, 14087]\n",
      "Projected discrete suffix tokens: ['▁Расподела', '▁That', '▁US', 'That', 'Ñ', '▁cast', '▁Sam', '▁Расподела', '▁Bra', '~~~~']\n",
      "Projected suffix as text: 'Расподела That USThatÑ cast Sam Расподела Bra~~~~'\n",
      "suffix loss (before projection): 0.001308441162109375\n",
      "suffix loss (after projection): 5.8984375\n",
      "\n",
      "\n",
      "Read suffix_r1_e0 successfully.\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  6.736993\n",
      "  max:  15.427361\n",
      "  mean: 11.933049\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.150858\n",
      "  max:  0.367465\n",
      "  mean: 0.258653\n",
      "\n",
      "Projected discrete suffix token IDs: [28354, 29933, 739, 7679, 20933, 14365, 9963, 426, 18518, 287]\n",
      "Projected discrete suffix tokens: ['▁Расподела', 'B', '▁It', '▁rock', '\")`', '________', '▁talking', '▁{', 'scriptstyle', 'ed']\n",
      "Projected suffix as text: 'РасподелаB It rock\")`________ talking {scriptstyleed'\n",
      "suffix loss (before projection): 1.0107421875\n",
      "suffix loss (after projection): 4.25\n",
      "\n",
      "\n",
      "Read suffix_r1_e1 successfully.\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  6.720104\n",
      "  max:  15.500568\n",
      "  mean: 11.956934\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.170682\n",
      "  max:  0.480525\n",
      "  mean: 0.313571\n",
      "\n",
      "Projected discrete suffix token IDs: [28354, 12883, 739, 7679, 20933, 14365, 9963, 426, 18518, 287]\n",
      "Projected discrete suffix tokens: ['▁Расподела', 'Year', '▁It', '▁rock', '\")`', '________', '▁talking', '▁{', 'scriptstyle', 'ed']\n",
      "Projected suffix as text: 'РасподелаYear It rock\")`________ talking {scriptstyleed'\n",
      "suffix loss (before projection): 0.00019884109497070312\n",
      "suffix loss (after projection): 4.12890625\n",
      "\n",
      "\n",
      "Read suffix_r1_e2 successfully.\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  6.492362\n",
      "  max:  15.481896\n",
      "  mean: 11.914342\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.183488\n",
      "  max:  0.548221\n",
      "  mean: 0.349731\n",
      "\n",
      "Projected discrete suffix token IDs: [30010, 12883, 739, 7679, 20933, 14365, 9963, 426, 18518, 287]\n",
      "Projected discrete suffix tokens: ['’', 'Year', '▁It', '▁rock', '\")`', '________', '▁talking', '▁{', 'scriptstyle', 'ed']\n",
      "Projected suffix as text: '’Year It rock\")`________ talking {scriptstyleed'\n",
      "suffix loss (before projection): 4.0590763092041016e-05\n",
      "suffix loss (after projection): 3.869140625\n",
      "\n",
      "\n",
      "Read suffix_r1_e3 successfully.\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  6.431930\n",
      "  max:  15.447401\n",
      "  mean: 11.892180\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.194239\n",
      "  max:  0.585655\n",
      "  mean: 0.371994\n",
      "\n",
      "Projected discrete suffix token IDs: [30010, 12883, 739, 7679, 20933, 26594, 9963, 426, 18518, 287]\n",
      "Projected discrete suffix tokens: ['’', 'Year', '▁It', '▁rock', '\")`', '~~~~~~~~', '▁talking', '▁{', 'scriptstyle', 'ed']\n",
      "Projected suffix as text: '’Year It rock\")`~~~~~~~~ talking {scriptstyleed'\n",
      "suffix loss (before projection): 1.1324882507324219e-06\n",
      "suffix loss (after projection): 1.58984375\n",
      "\n",
      "\n",
      "Read suffix_r1_e4 successfully.\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  6.391871\n",
      "  max:  15.432979\n",
      "  mean: 11.878518\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.201384\n",
      "  max:  0.605583\n",
      "  mean: 0.384770\n",
      "\n",
      "Projected discrete suffix token IDs: [30010, 12883, 739, 7679, 20933, 26594, 9963, 426, 18518, 287]\n",
      "Projected discrete suffix tokens: ['’', 'Year', '▁It', '▁rock', '\")`', '~~~~~~~~', '▁talking', '▁{', 'scriptstyle', 'ed']\n",
      "Projected suffix as text: '’Year It rock\")`~~~~~~~~ talking {scriptstyleed'\n",
      "suffix loss (before projection): 1.8876953125\n",
      "suffix loss (after projection): 1.58984375\n",
      "\n",
      "\n",
      "Read suffix_r1_e5 successfully.\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  6.371569\n",
      "  max:  15.429025\n",
      "  mean: 11.874303\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.206065\n",
      "  max:  0.615899\n",
      "  mean: 0.392333\n",
      "\n",
      "Projected discrete suffix token IDs: [30010, 12883, 739, 7679, 20933, 26594, 9963, 426, 18518, 287]\n",
      "Projected discrete suffix tokens: ['’', 'Year', '▁It', '▁rock', '\")`', '~~~~~~~~', '▁talking', '▁{', 'scriptstyle', 'ed']\n",
      "Projected suffix as text: '’Year It rock\")`~~~~~~~~ talking {scriptstyleed'\n",
      "suffix loss (before projection): 1.7490234375\n",
      "suffix loss (after projection): 1.58984375\n",
      "\n",
      "\n",
      "Read suffix_r1_e6 successfully.\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  6.357021\n",
      "  max:  15.425073\n",
      "  mean: 11.869252\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.208748\n",
      "  max:  0.623064\n",
      "  mean: 0.397191\n",
      "\n",
      "Projected discrete suffix token IDs: [30010, 12883, 739, 7679, 20933, 26594, 9963, 426, 18518, 287]\n",
      "Projected discrete suffix tokens: ['’', 'Year', '▁It', '▁rock', '\")`', '~~~~~~~~', '▁talking', '▁{', 'scriptstyle', 'ed']\n",
      "Projected suffix as text: '’Year It rock\")`~~~~~~~~ talking {scriptstyleed'\n",
      "suffix loss (before projection): 1.80078125\n",
      "suffix loss (after projection): 1.58984375\n",
      "\n",
      "\n",
      "Read suffix_r1_e7 successfully.\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  6.348247\n",
      "  max:  15.424518\n",
      "  mean: 11.866544\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.210307\n",
      "  max:  0.626688\n",
      "  mean: 0.400059\n",
      "\n",
      "Projected discrete suffix token IDs: [30010, 12883, 739, 7679, 20933, 26594, 9963, 426, 18518, 287]\n",
      "Projected discrete suffix tokens: ['’', 'Year', '▁It', '▁rock', '\")`', '~~~~~~~~', '▁talking', '▁{', 'scriptstyle', 'ed']\n",
      "Projected suffix as text: '’Year It rock\")`~~~~~~~~ talking {scriptstyleed'\n",
      "suffix loss (before projection): 1.744140625\n",
      "suffix loss (after projection): 1.58984375\n",
      "\n",
      "\n",
      "Read suffix_r1_e8 successfully.\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  6.341767\n",
      "  max:  15.423545\n",
      "  mean: 11.864301\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.211331\n",
      "  max:  0.628930\n",
      "  mean: 0.401829\n",
      "\n",
      "Projected discrete suffix token IDs: [30010, 12883, 739, 7679, 20933, 26594, 9963, 426, 18518, 287]\n",
      "Projected discrete suffix tokens: ['’', 'Year', '▁It', '▁rock', '\")`', '~~~~~~~~', '▁talking', '▁{', 'scriptstyle', 'ed']\n",
      "Projected suffix as text: '’Year It rock\")`~~~~~~~~ talking {scriptstyleed'\n",
      "suffix loss (before projection): 1.763671875\n",
      "suffix loss (after projection): 1.58984375\n",
      "\n",
      "\n",
      "Read suffix_r1_e9 successfully.\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  6.338214\n",
      "  max:  15.422756\n",
      "  mean: 11.863365\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.211888\n",
      "  max:  0.630125\n",
      "  mean: 0.402836\n",
      "\n",
      "Projected discrete suffix token IDs: [30010, 12883, 739, 7679, 20933, 26594, 9963, 426, 18518, 287]\n",
      "Projected discrete suffix tokens: ['’', 'Year', '▁It', '▁rock', '\")`', '~~~~~~~~', '▁talking', '▁{', 'scriptstyle', 'ed']\n",
      "Projected suffix as text: '’Year It rock\")`~~~~~~~~ talking {scriptstyleed'\n",
      "suffix loss (before projection): 1.759765625\n",
      "suffix loss (after projection): 1.58984375\n",
      "\n",
      "\n",
      "Read suffix_r1_e10 successfully.\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  6.335851\n",
      "  max:  15.422433\n",
      "  mean: 11.862859\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.212226\n",
      "  max:  0.630753\n",
      "  mean: 0.403446\n",
      "\n",
      "Projected discrete suffix token IDs: [30010, 12883, 739, 7679, 20933, 26594, 9963, 426, 18518, 287]\n",
      "Projected discrete suffix tokens: ['’', 'Year', '▁It', '▁rock', '\")`', '~~~~~~~~', '▁talking', '▁{', 'scriptstyle', 'ed']\n",
      "Projected suffix as text: '’Year It rock\")`~~~~~~~~ talking {scriptstyleed'\n",
      "suffix loss (before projection): 1.7802734375\n",
      "suffix loss (after projection): 1.58984375\n",
      "\n",
      "\n",
      "Read suffix_r1_e11 successfully.\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  6.334694\n",
      "  max:  15.422508\n",
      "  mean: 11.862522\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.212415\n",
      "  max:  0.631162\n",
      "  mean: 0.403807\n",
      "\n",
      "Projected discrete suffix token IDs: [30010, 12883, 739, 7679, 20933, 26594, 9963, 426, 18518, 287]\n",
      "Projected discrete suffix tokens: ['’', 'Year', '▁It', '▁rock', '\")`', '~~~~~~~~', '▁talking', '▁{', 'scriptstyle', 'ed']\n",
      "Projected suffix as text: '’Year It rock\")`~~~~~~~~ talking {scriptstyleed'\n",
      "suffix loss (before projection): 1.775390625\n",
      "suffix loss (after projection): 1.58984375\n",
      "\n",
      "\n",
      "Read suffix_r1_e12 successfully.\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  6.334160\n",
      "  max:  15.422550\n",
      "  mean: 11.862416\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.212501\n",
      "  max:  0.631373\n",
      "  mean: 0.403977\n",
      "\n",
      "Projected discrete suffix token IDs: [30010, 12883, 739, 7679, 20933, 26594, 9963, 426, 18518, 287]\n",
      "Projected discrete suffix tokens: ['’', 'Year', '▁It', '▁rock', '\")`', '~~~~~~~~', '▁talking', '▁{', 'scriptstyle', 'ed']\n",
      "Projected suffix as text: '’Year It rock\")`~~~~~~~~ talking {scriptstyleed'\n",
      "suffix loss (before projection): 1.7841796875\n",
      "suffix loss (after projection): 1.58984375\n",
      "\n",
      "\n",
      "Read suffix_r1_e13 successfully.\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  6.333827\n",
      "  max:  15.422510\n",
      "  mean: 11.862317\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.212545\n",
      "  max:  0.631472\n",
      "  mean: 0.404062\n",
      "\n",
      "Projected discrete suffix token IDs: [30010, 12883, 739, 7679, 20933, 26594, 9963, 426, 18518, 287]\n",
      "Projected discrete suffix tokens: ['’', 'Year', '▁It', '▁rock', '\")`', '~~~~~~~~', '▁talking', '▁{', 'scriptstyle', 'ed']\n",
      "Projected suffix as text: '’Year It rock\")`~~~~~~~~ talking {scriptstyleed'\n",
      "suffix loss (before projection): 1.78125\n",
      "suffix loss (after projection): 1.58984375\n",
      "\n",
      "\n",
      "Read suffix_r1_e14 successfully.\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  6.333631\n",
      "  max:  15.422512\n",
      "  mean: 11.862269\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.212570\n",
      "  max:  0.631529\n",
      "  mean: 0.404115\n",
      "\n",
      "Projected discrete suffix token IDs: [30010, 12883, 739, 7679, 20933, 26594, 9963, 426, 18518, 287]\n",
      "Projected discrete suffix tokens: ['’', 'Year', '▁It', '▁rock', '\")`', '~~~~~~~~', '▁talking', '▁{', 'scriptstyle', 'ed']\n",
      "Projected suffix as text: '’Year It rock\")`~~~~~~~~ talking {scriptstyleed'\n",
      "suffix loss (before projection): 1.7890625\n",
      "suffix loss (after projection): 1.58984375\n",
      "\n",
      "\n",
      "Read suffix_r2_e0 successfully.\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  5.186144\n",
      "  max:  10.445699\n",
      "  mean: 7.851099\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.221262\n",
      "  max:  0.633508\n",
      "  mean: 0.410862\n",
      "\n",
      "Projected discrete suffix token IDs: [15651, 28354, 20609, 2379, 26524, 29991, 25627, 435, 25145, 22977]\n",
      "Projected discrete suffix tokens: ['▁Din', '▁Расподела', '▁Portály', '▁Fl', '▁AUT', '!', 'Rich', '▁J', '▁kwiet', '▁Heaven']\n",
      "Projected suffix as text: 'Din Расподела Portály Fl AUT!Rich J kwiet Heaven'\n",
      "suffix loss (before projection): 6.23464584350586e-05\n",
      "suffix loss (after projection): 2.298828125\n",
      "\n",
      "\n",
      "Read suffix_r2_e1 successfully.\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  5.240155\n",
      "  max:  10.679713\n",
      "  mean: 7.953748\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.300210\n",
      "  max:  0.677126\n",
      "  mean: 0.485170\n",
      "\n",
      "Projected discrete suffix token IDs: [3600, 28354, 20609, 2379, 26524, 29991, 25627, 435, 25145, 22977]\n",
      "Projected discrete suffix tokens: ['▁His', '▁Расподела', '▁Portály', '▁Fl', '▁AUT', '!', 'Rich', '▁J', '▁kwiet', '▁Heaven']\n",
      "Projected suffix as text: 'His Расподела Portály Fl AUT!Rich J kwiet Heaven'\n",
      "suffix loss (before projection): 0.002941131591796875\n",
      "suffix loss (after projection): 2.279296875\n",
      "\n",
      "\n",
      "Read suffix_r2_e2 successfully.\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  5.284798\n",
      "  max:  10.959088\n",
      "  mean: 8.067401\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.325202\n",
      "  max:  0.685073\n",
      "  mean: 0.508342\n",
      "\n",
      "Projected discrete suffix token IDs: [3600, 28354, 20609, 2379, 26524, 29991, 25627, 435, 25145, 22977]\n",
      "Projected discrete suffix tokens: ['▁His', '▁Расподела', '▁Portály', '▁Fl', '▁AUT', '!', 'Rich', '▁J', '▁kwiet', '▁Heaven']\n",
      "Projected suffix as text: 'His Расподела Portály Fl AUT!Rich J kwiet Heaven'\n",
      "suffix loss (before projection): 1.0132789611816406e-06\n",
      "suffix loss (after projection): 2.279296875\n",
      "\n",
      "\n",
      "Read suffix_r2_e3 successfully.\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  5.259330\n",
      "  max:  10.939575\n",
      "  mean: 8.036258\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.349571\n",
      "  max:  0.689350\n",
      "  mean: 0.522362\n",
      "\n",
      "Projected discrete suffix token IDs: [512, 28354, 20609, 2379, 26524, 29991, 25627, 435, 25145, 22977]\n",
      "Projected discrete suffix tokens: ['▁In', '▁Расподела', '▁Portály', '▁Fl', '▁AUT', '!', 'Rich', '▁J', '▁kwiet', '▁Heaven']\n",
      "Projected suffix as text: 'In Расподела Portály Fl AUT!Rich J kwiet Heaven'\n",
      "suffix loss (before projection): 1.0132789611816406e-06\n",
      "suffix loss (after projection): 2.41015625\n",
      "\n",
      "\n",
      "Read suffix_r2_e4 successfully.\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  5.246674\n",
      "  max:  10.931177\n",
      "  mean: 8.016039\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.363142\n",
      "  max:  0.690403\n",
      "  mean: 0.528615\n",
      "\n",
      "Projected discrete suffix token IDs: [512, 28354, 20609, 2379, 26524, 29991, 25627, 435, 25145, 22977]\n",
      "Projected discrete suffix tokens: ['▁In', '▁Расподела', '▁Portály', '▁Fl', '▁AUT', '!', 'Rich', '▁J', '▁kwiet', '▁Heaven']\n",
      "Projected suffix as text: 'In Расподела Portály Fl AUT!Rich J kwiet Heaven'\n",
      "suffix loss (before projection): 6.616115570068359e-06\n",
      "suffix loss (after projection): 2.41015625\n",
      "\n",
      "\n",
      "Read suffix_r2_e5 successfully.\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  5.238680\n",
      "  max:  10.923628\n",
      "  mean: 8.002492\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.371670\n",
      "  max:  0.690895\n",
      "  mean: 0.532100\n",
      "\n",
      "Projected discrete suffix token IDs: [512, 28354, 20609, 2379, 26524, 29991, 25627, 435, 25145, 22977]\n",
      "Projected discrete suffix tokens: ['▁In', '▁Расподела', '▁Portály', '▁Fl', '▁AUT', '!', 'Rich', '▁J', '▁kwiet', '▁Heaven']\n",
      "Projected suffix as text: 'In Расподела Portály Fl AUT!Rich J kwiet Heaven'\n",
      "suffix loss (before projection): 2.3543834686279297e-05\n",
      "suffix loss (after projection): 2.41015625\n",
      "\n",
      "\n",
      "Read suffix_r2_e6 successfully.\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  5.232389\n",
      "  max:  10.923693\n",
      "  mean: 7.995126\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.375050\n",
      "  max:  0.690488\n",
      "  mean: 0.533475\n",
      "\n",
      "Projected discrete suffix token IDs: [512, 28354, 20609, 2379, 26524, 29991, 25627, 435, 25145, 22977]\n",
      "Projected discrete suffix tokens: ['▁In', '▁Расподела', '▁Portály', '▁Fl', '▁AUT', '!', 'Rich', '▁J', '▁kwiet', '▁Heaven']\n",
      "Projected suffix as text: 'In Расподела Portály Fl AUT!Rich J kwiet Heaven'\n",
      "suffix loss (before projection): 4.601478576660156e-05\n",
      "suffix loss (after projection): 2.41015625\n",
      "\n",
      "\n",
      "Read suffix_r2_e7 successfully.\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  5.230200\n",
      "  max:  10.923552\n",
      "  mean: 7.991717\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.377142\n",
      "  max:  0.690593\n",
      "  mean: 0.534398\n",
      "\n",
      "Projected discrete suffix token IDs: [512, 28354, 20609, 2379, 26524, 29991, 25627, 435, 25145, 22977]\n",
      "Projected discrete suffix tokens: ['▁In', '▁Расподела', '▁Portály', '▁Fl', '▁AUT', '!', 'Rich', '▁J', '▁kwiet', '▁Heaven']\n",
      "Projected suffix as text: 'In Расподела Portály Fl AUT!Rich J kwiet Heaven'\n",
      "suffix loss (before projection): 0.01332855224609375\n",
      "suffix loss (after projection): 2.41015625\n",
      "\n",
      "\n",
      "Read suffix_r2_e8 successfully.\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  5.228714\n",
      "  max:  10.923121\n",
      "  mean: 7.989469\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.378293\n",
      "  max:  0.690720\n",
      "  mean: 0.534956\n",
      "\n",
      "Projected discrete suffix token IDs: [512, 28354, 20609, 2379, 26524, 29991, 25627, 435, 25145, 22977]\n",
      "Projected discrete suffix tokens: ['▁In', '▁Расподела', '▁Portály', '▁Fl', '▁AUT', '!', 'Rich', '▁J', '▁kwiet', '▁Heaven']\n",
      "Projected suffix as text: 'In Расподела Portály Fl AUT!Rich J kwiet Heaven'\n",
      "suffix loss (before projection): 0.0153045654296875\n",
      "suffix loss (after projection): 2.41015625\n",
      "\n",
      "\n",
      "Read suffix_r2_e9 successfully.\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  5.227824\n",
      "  max:  10.922636\n",
      "  mean: 7.988065\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.379064\n",
      "  max:  0.690799\n",
      "  mean: 0.535315\n",
      "\n",
      "Projected discrete suffix token IDs: [512, 28354, 20609, 2379, 26524, 29991, 25627, 435, 25145, 22977]\n",
      "Projected discrete suffix tokens: ['▁In', '▁Расподела', '▁Portály', '▁Fl', '▁AUT', '!', 'Rich', '▁J', '▁kwiet', '▁Heaven']\n",
      "Projected suffix as text: 'In Расподела Portály Fl AUT!Rich J kwiet Heaven'\n",
      "suffix loss (before projection): 0.0160675048828125\n",
      "suffix loss (after projection): 2.41015625\n",
      "\n",
      "\n",
      "Read suffix_r2_e10 successfully.\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  5.227226\n",
      "  max:  10.922317\n",
      "  mean: 7.987160\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.379557\n",
      "  max:  0.690849\n",
      "  mean: 0.535541\n",
      "\n",
      "Projected discrete suffix token IDs: [512, 28354, 20609, 2379, 26524, 29991, 25627, 435, 25145, 22977]\n",
      "Projected discrete suffix tokens: ['▁In', '▁Расподела', '▁Portály', '▁Fl', '▁AUT', '!', 'Rich', '▁J', '▁kwiet', '▁Heaven']\n",
      "Projected suffix as text: 'In Расподела Portály Fl AUT!Rich J kwiet Heaven'\n",
      "suffix loss (before projection): 0.0162353515625\n",
      "suffix loss (after projection): 2.41015625\n",
      "\n",
      "\n",
      "Read suffix_r2_e11 successfully.\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  5.226975\n",
      "  max:  10.922245\n",
      "  mean: 7.986721\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.379873\n",
      "  max:  0.690863\n",
      "  mean: 0.535666\n",
      "\n",
      "Projected discrete suffix token IDs: [512, 28354, 20609, 2379, 26524, 29991, 25627, 435, 25145, 22977]\n",
      "Projected discrete suffix tokens: ['▁In', '▁Расподела', '▁Portály', '▁Fl', '▁AUT', '!', 'Rich', '▁J', '▁kwiet', '▁Heaven']\n",
      "Projected suffix as text: 'In Расподела Portály Fl AUT!Rich J kwiet Heaven'\n",
      "suffix loss (before projection): 0.0161895751953125\n",
      "suffix loss (after projection): 2.41015625\n",
      "\n",
      "\n",
      "Read suffix_r2_e12 successfully.\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  5.226783\n",
      "  max:  10.922165\n",
      "  mean: 7.986486\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.379996\n",
      "  max:  0.690878\n",
      "  mean: 0.535729\n",
      "\n",
      "Projected discrete suffix token IDs: [512, 28354, 20609, 2379, 26524, 29991, 25627, 435, 25145, 22977]\n",
      "Projected discrete suffix tokens: ['▁In', '▁Расподела', '▁Portály', '▁Fl', '▁AUT', '!', 'Rich', '▁J', '▁kwiet', '▁Heaven']\n",
      "Projected suffix as text: 'In Расподела Portály Fl AUT!Rich J kwiet Heaven'\n",
      "suffix loss (before projection): 0.0161285400390625\n",
      "suffix loss (after projection): 2.41015625\n",
      "\n",
      "\n",
      "Read suffix_r2_e13 successfully.\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  5.226679\n",
      "  max:  10.922114\n",
      "  mean: 7.986331\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.380074\n",
      "  max:  0.690890\n",
      "  mean: 0.535770\n",
      "\n",
      "Projected discrete suffix token IDs: [512, 28354, 20609, 2379, 26524, 29991, 25627, 435, 25145, 22977]\n",
      "Projected discrete suffix tokens: ['▁In', '▁Расподела', '▁Portály', '▁Fl', '▁AUT', '!', 'Rich', '▁J', '▁kwiet', '▁Heaven']\n",
      "Projected suffix as text: 'In Расподела Portály Fl AUT!Rich J kwiet Heaven'\n",
      "suffix loss (before projection): 0.0161590576171875\n",
      "suffix loss (after projection): 2.41015625\n",
      "\n",
      "\n",
      "Read suffix_r2_e14 successfully.\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  5.226623\n",
      "  max:  10.922089\n",
      "  mean: 7.986249\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.380120\n",
      "  max:  0.690896\n",
      "  mean: 0.535793\n",
      "\n",
      "Projected discrete suffix token IDs: [512, 28354, 20609, 2379, 26524, 29991, 25627, 435, 25145, 22977]\n",
      "Projected discrete suffix tokens: ['▁In', '▁Расподела', '▁Portály', '▁Fl', '▁AUT', '!', 'Rich', '▁J', '▁kwiet', '▁Heaven']\n",
      "Projected suffix as text: 'In Расподела Portály Fl AUT!Rich J kwiet Heaven'\n",
      "suffix loss (before projection): 0.01617431640625\n",
      "suffix loss (after projection): 2.41015625\n",
      "\n",
      "\n",
      "Read suffix_r3_e0 successfully.\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  4.151564\n",
      "  max:  6.831207\n",
      "  mean: 5.125988\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.280168\n",
      "  max:  0.721045\n",
      "  mean: 0.530556\n",
      "\n",
      "Projected discrete suffix token IDs: [301, 28354, 6360, 30015, 28354, 1923, 6507, 5416, 28354, 24024]\n",
      "Projected discrete suffix tokens: ['▁l', '▁Расподела', 'year', '“', '▁Расподела', '▁server', '▁release', 'uid', '▁Расподела', 'egu']\n",
      "Projected suffix as text: 'l Расподелаyear“ Расподела server releaseuid Расподелаegu'\n",
      "suffix loss (before projection): 6.0546875\n",
      "suffix loss (after projection): 4.39453125\n",
      "\n",
      "\n",
      "Read suffix_r3_e1 successfully.\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  5.257180\n",
      "  max:  9.737061\n",
      "  mean: 7.503321\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.255164\n",
      "  max:  0.636998\n",
      "  mean: 0.454293\n",
      "\n",
      "Projected discrete suffix token IDs: [301, 28354, 6360, 29908, 28354, 1353, 24993, 5416, 28354, 24024]\n",
      "Projected discrete suffix tokens: ['▁l', '▁Расподела', 'year', '\"', '▁Расподела', '▁number', '▁leak', 'uid', '▁Расподела', 'egu']\n",
      "Projected suffix as text: 'l Расподелаyear\" Расподела number leakuid Расподелаegu'\n",
      "suffix loss (before projection): 0.32861328125\n",
      "suffix loss (after projection): 1.259765625\n",
      "\n",
      "\n",
      "Read suffix_r3_e2 successfully.\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  5.234514\n",
      "  max:  10.101172\n",
      "  mean: 7.639986\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.278527\n",
      "  max:  0.708430\n",
      "  mean: 0.506759\n",
      "\n",
      "Projected discrete suffix token IDs: [301, 28354, 6360, 30015, 28354, 1353, 24993, 5416, 28354, 24024]\n",
      "Projected discrete suffix tokens: ['▁l', '▁Расподела', 'year', '“', '▁Расподела', '▁number', '▁leak', 'uid', '▁Расподела', 'egu']\n",
      "Projected suffix as text: 'l Расподелаyear“ Расподела number leakuid Расподелаegu'\n",
      "suffix loss (before projection): 0.0135040283203125\n",
      "suffix loss (after projection): 1.138671875\n",
      "\n",
      "\n",
      "Read suffix_r3_e3 successfully.\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  nan\n",
      "  max:  nan\n",
      "  mean: nan\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  nan\n",
      "  max:  nan\n",
      "  mean: nan\n",
      "\n",
      "Projected discrete suffix token IDs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Projected discrete suffix tokens: ['<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>']\n",
      "Projected suffix as text: '<unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>'\n",
      "suffix loss (before projection): nan\n",
      "suffix loss (after projection): 2.8359375\n",
      "\n",
      "\n",
      "Read suffix_r3_e4 successfully.\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  nan\n",
      "  max:  nan\n",
      "  mean: nan\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  nan\n",
      "  max:  nan\n",
      "  mean: nan\n",
      "\n",
      "Projected discrete suffix token IDs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Projected discrete suffix tokens: ['<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>']\n",
      "Projected suffix as text: '<unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>'\n",
      "suffix loss (before projection): nan\n",
      "suffix loss (after projection): 2.8359375\n",
      "\n",
      "\n",
      "Read suffix_r3_e5 successfully.\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  nan\n",
      "  max:  nan\n",
      "  mean: nan\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  nan\n",
      "  max:  nan\n",
      "  mean: nan\n",
      "\n",
      "Projected discrete suffix token IDs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Projected discrete suffix tokens: ['<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>']\n",
      "Projected suffix as text: '<unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>'\n",
      "suffix loss (before projection): nan\n",
      "suffix loss (after projection): 2.8359375\n",
      "\n",
      "\n",
      "Read suffix_r3_e6 successfully.\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  nan\n",
      "  max:  nan\n",
      "  mean: nan\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  nan\n",
      "  max:  nan\n",
      "  mean: nan\n",
      "\n",
      "Projected discrete suffix token IDs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Projected discrete suffix tokens: ['<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>']\n",
      "Projected suffix as text: '<unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>'\n",
      "suffix loss (before projection): nan\n",
      "suffix loss (after projection): 2.8359375\n",
      "\n",
      "\n",
      "Read suffix_r3_e7 successfully.\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  nan\n",
      "  max:  nan\n",
      "  mean: nan\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  nan\n",
      "  max:  nan\n",
      "  mean: nan\n",
      "\n",
      "Projected discrete suffix token IDs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Projected discrete suffix tokens: ['<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>']\n",
      "Projected suffix as text: '<unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>'\n",
      "suffix loss (before projection): nan\n",
      "suffix loss (after projection): 2.8359375\n",
      "\n",
      "\n",
      "Read suffix_r3_e8 successfully.\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  nan\n",
      "  max:  nan\n",
      "  mean: nan\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  nan\n",
      "  max:  nan\n",
      "  mean: nan\n",
      "\n",
      "Projected discrete suffix token IDs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Projected discrete suffix tokens: ['<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>']\n",
      "Projected suffix as text: '<unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>'\n",
      "suffix loss (before projection): nan\n",
      "suffix loss (after projection): 2.8359375\n",
      "\n",
      "\n",
      "Read suffix_r3_e9 successfully.\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  nan\n",
      "  max:  nan\n",
      "  mean: nan\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  nan\n",
      "  max:  nan\n",
      "  mean: nan\n",
      "\n",
      "Projected discrete suffix token IDs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Projected discrete suffix tokens: ['<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>']\n",
      "Projected suffix as text: '<unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>'\n",
      "suffix loss (before projection): nan\n",
      "suffix loss (after projection): 2.8359375\n",
      "\n",
      "\n",
      "Read suffix_r3_e10 successfully.\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  nan\n",
      "  max:  nan\n",
      "  mean: nan\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  nan\n",
      "  max:  nan\n",
      "  mean: nan\n",
      "\n",
      "Projected discrete suffix token IDs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Projected discrete suffix tokens: ['<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>']\n",
      "Projected suffix as text: '<unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>'\n",
      "suffix loss (before projection): nan\n",
      "suffix loss (after projection): 2.8359375\n",
      "\n",
      "\n",
      "Read suffix_r3_e11 successfully.\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  nan\n",
      "  max:  nan\n",
      "  mean: nan\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  nan\n",
      "  max:  nan\n",
      "  mean: nan\n",
      "\n",
      "Projected discrete suffix token IDs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Projected discrete suffix tokens: ['<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>']\n",
      "Projected suffix as text: '<unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>'\n",
      "suffix loss (before projection): nan\n",
      "suffix loss (after projection): 2.8359375\n",
      "\n",
      "\n",
      "Read suffix_r3_e12 successfully.\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  nan\n",
      "  max:  nan\n",
      "  mean: nan\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  nan\n",
      "  max:  nan\n",
      "  mean: nan\n",
      "\n",
      "Projected discrete suffix token IDs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Projected discrete suffix tokens: ['<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>']\n",
      "Projected suffix as text: '<unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>'\n",
      "suffix loss (before projection): nan\n",
      "suffix loss (after projection): 2.8359375\n",
      "\n",
      "\n",
      "Read suffix_r3_e13 successfully.\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  nan\n",
      "  max:  nan\n",
      "  mean: nan\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  nan\n",
      "  max:  nan\n",
      "  mean: nan\n",
      "\n",
      "Projected discrete suffix token IDs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Projected discrete suffix tokens: ['<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>']\n",
      "Projected suffix as text: '<unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>'\n",
      "suffix loss (before projection): nan\n",
      "suffix loss (after projection): 2.8359375\n",
      "\n",
      "\n",
      "Read suffix_r3_e14 successfully.\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  nan\n",
      "  max:  nan\n",
      "  mean: nan\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  nan\n",
      "  max:  nan\n",
      "  mean: nan\n",
      "\n",
      "Projected discrete suffix token IDs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Projected discrete suffix tokens: ['<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>']\n",
      "Projected suffix as text: '<unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>'\n",
      "suffix loss (before projection): nan\n",
      "suffix loss (after projection): 2.8359375\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from itertools import islice\n",
    "\n",
    "device = next(model.parameters()).device\n",
    "emb_layer = model.get_input_embeddings()\n",
    "\n",
    "# Get one batch iterator for prompts\n",
    "data_iter = iter(dataloader)\n",
    "batch = next(data_iter)\n",
    "prompts = batch[0]  # list of 1D tensors\n",
    "prompt_ids = prompts[0]       # pick first prompt\n",
    "print(\"Using this prompt for qualitative checks:\")\n",
    "print(\"  \", tokenizer.decode(prompt_ids, skip_special_tokens=True))\n",
    "print()\n",
    "\n",
    "with torch.no_grad():\n",
    "    E_cpu = model.get_input_embeddings().weight.detach().cpu().float()  # (V, E)\n",
    "    E_norm_cpu = F.normalize(E_cpu, dim=-1)  # (V, E), fp32 on CPU\n",
    "\n",
    "for i in range(4):      # rounds\n",
    "    for j in range(10): # epochs/checkpoints per round\n",
    "        path = f\"/kaggle/working/hotflip/rounds/suffix_r{i}_e{j}.pt\"\n",
    "\n",
    "        try:\n",
    "            suffix_z = read_suffix_pt(path)  # (Ls, E) on CPU\n",
    "        except FileNotFoundError:\n",
    "            print(f\"[WARN] Missing file: {path}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n=== round {i}, epoch {j} ===\")\n",
    "        print(f\"Read {path} successfully. shape={tuple(suffix_z.shape)}\")\n",
    "\n",
    "        # Project to tokens + diagnostics\n",
    "        suffix_token_ids = project_suffix_to_tokens_and_diagnostics(\n",
    "            suffix_z,\n",
    "            emb_layer,\n",
    "            tokenizer,\n",
    "        )  # LongTensor (Ls,)\n",
    "\n",
    "        # Compute loss BEFORE projection (continuous suffix)\n",
    "        batch_eos = {\"input_ids\": [torch.tensor([tokenizer.eos_token_id], dtype=torch.long)]}\n",
    "        loss_before = compute_loss_for_suffix(\n",
    "            model,\n",
    "            emb_layer,\n",
    "            batch_eos,\n",
    "            suffix_z.to(device),\n",
    "            n_tokens=10,\n",
    "            amp_dtype=torch.float16,\n",
    "            cos_reg_weight=1.0,\n",
    "            E_norm_cpu=E_norm_cpu,\n",
    "        )\n",
    "        print(f\"suffix loss (before projection): {loss_before.item():.6f}\")\n",
    "\n",
    "        # Build suffix_z AFTER projection: embeddings of discrete tokens\n",
    "        suffix_z_proj = emb_layer(suffix_token_ids.to(device))  # (Ls, E)\n",
    "\n",
    "        loss_after = compute_loss_for_suffix(\n",
    "            model,\n",
    "            emb_layer,\n",
    "            batch_eos,\n",
    "            suffix_z_proj,\n",
    "            n_tokens=10,\n",
    "            amp_dtype=torch.float16,\n",
    "            cos_reg_weight=1.0,\n",
    "            E_norm_cpu=E_norm_cpu,\n",
    "        )\n",
    "        print(f\"suffix loss (after  projection): {loss_after.item():.6f}\")\n",
    "\n",
    "        # ---- Qualitative check: generate from a real prompt + suffix ----\n",
    "        print(\"\\nGenerated text with suffix (projected):\")\n",
    "        gen_text = generate_with_suffix(\n",
    "            model,\n",
    "            tokenizer,\n",
    "            prompt_ids,\n",
    "            suffix_token_ids,\n",
    "            max_new_tokens=64,\n",
    "        )\n",
    "        print(gen_text)\n",
    "        print()\n",
    "\n",
    "        # ---- Per-suffix-token probabilities given the prompt ----\n",
    "        print(\"Per-suffix-token next-token probabilities:\")\n",
    "        log_suffix_token_probs(\n",
    "            model,\n",
    "            tokenizer,\n",
    "            prompt_ids,\n",
    "            suffix_token_ids,\n",
    "        )\n",
    "\n",
    "        print(\"####################\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7658d477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suffix_r0_e0.pt   suffix_r1_e0.pt   suffix_r2_e0.pt   suffix_r3_e0.pt\n",
      "suffix_r0_e10.pt  suffix_r1_e10.pt  suffix_r2_e10.pt  suffix_r3_e10.pt\n",
      "suffix_r0_e11.pt  suffix_r1_e11.pt  suffix_r2_e11.pt  suffix_r3_e11.pt\n",
      "suffix_r0_e12.pt  suffix_r1_e12.pt  suffix_r2_e12.pt  suffix_r3_e12.pt\n",
      "suffix_r0_e13.pt  suffix_r1_e13.pt  suffix_r2_e13.pt  suffix_r3_e13.pt\n",
      "suffix_r0_e14.pt  suffix_r1_e14.pt  suffix_r2_e14.pt  suffix_r3_e14.pt\n",
      "suffix_r0_e1.pt   suffix_r1_e1.pt   suffix_r2_e1.pt   suffix_r3_e1.pt\n",
      "suffix_r0_e2.pt   suffix_r1_e2.pt   suffix_r2_e2.pt   suffix_r3_e2.pt\n",
      "suffix_r0_e3.pt   suffix_r1_e3.pt   suffix_r2_e3.pt   suffix_r3_e3.pt\n",
      "suffix_r0_e4.pt   suffix_r1_e4.pt   suffix_r2_e4.pt   suffix_r3_e4.pt\n",
      "suffix_r0_e5.pt   suffix_r1_e5.pt   suffix_r2_e5.pt   suffix_r3_e5.pt\n",
      "suffix_r0_e6.pt   suffix_r1_e6.pt   suffix_r2_e6.pt   suffix_r3_e6.pt\n",
      "suffix_r0_e7.pt   suffix_r1_e7.pt   suffix_r2_e7.pt   suffix_r3_e7.pt\n",
      "suffix_r0_e8.pt   suffix_r1_e8.pt   suffix_r2_e8.pt   suffix_r3_e8.pt\n",
      "suffix_r0_e9.pt   suffix_r1_e9.pt   suffix_r2_e9.pt   suffix_r3_e9.pt\n"
     ]
    }
   ],
   "source": [
    "!ls /kaggle/working/hotflip/rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8daabbe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working\n",
      "Cloning into 'hotflip'...\n",
      "remote: Enumerating objects: 341, done.\u001b[K\n",
      "remote: Counting objects: 100% (129/129), done.\u001b[K\n",
      "remote: Compressing objects: 100% (119/119), done.\u001b[K\n",
      "remote: Total 341 (delta 11), reused 127 (delta 10), pack-reused 212 (from 1)\u001b[K\n",
      "Receiving objects: 100% (341/341), 28.44 MiB | 21.22 MiB/s, done.\n",
      "Resolving deltas: 100% (105/105), done.\n",
      "/kaggle/working/hotflip\n",
      "From https://github.com/jefri021/hotflip\n",
      " * branch            main       -> FETCH_HEAD\n",
      "Already up to date.\n"
     ]
    }
   ],
   "source": [
    "refresh_repo()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8363949,
     "sourceId": 13197689,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
