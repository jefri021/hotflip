{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5dde332",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T14:02:21.750882Z",
     "iopub.status.busy": "2025-11-20T14:02:21.750651Z",
     "iopub.status.idle": "2025-11-20T14:02:28.092614Z",
     "shell.execute_reply": "2025-11-20T14:02:28.091596Z",
     "shell.execute_reply.started": "2025-11-20T14:02:21.750855Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working\n",
      "Cloning into 'hotflip'...\n",
      "remote: Enumerating objects: 276, done.\u001b[K\n",
      "remote: Counting objects: 100% (64/64), done.\u001b[K\n",
      "remote: Compressing objects: 100% (64/64), done.\u001b[K\n",
      "remote: Total 276 (delta 0), reused 63 (delta 0), pack-reused 212 (from 1)\u001b[K\n",
      "Receiving objects: 100% (276/276), 21.43 MiB | 18.56 MiB/s, done.\n",
      "Resolving deltas: 100% (94/94), done.\n",
      "/kaggle/working/hotflip\n",
      "From https://github.com/jefri021/hotflip\n",
      " * branch            main       -> FETCH_HEAD\n",
      "Already up to date.\n"
     ]
    }
   ],
   "source": [
    "def refresh_repo():\n",
    "    %cd /kaggle/working\n",
    "    %rm -rf hotflip\n",
    "    !git clone https://github.com/jefri021/hotflip.git\n",
    "    %cd /kaggle/working/hotflip/\n",
    "    !git pull origin main\n",
    "\n",
    "refresh_repo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4581d65d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T14:02:28.095092Z",
     "iopub.status.busy": "2025-11-20T14:02:28.094727Z",
     "iopub.status.idle": "2025-11-20T14:04:15.054278Z",
     "shell.execute_reply": "2025-11-20T14:04:15.053458Z",
     "shell.execute_reply.started": "2025-11-20T14:02:28.095054Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "2025-11-22 18:22:58.046176: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1763835778.267050      85 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1763835778.329632      85 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62c785230c8c4f529b8b8392c7a17c28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 0, 'model.layers.9': 0, 'model.layers.10': 0, 'model.layers.11': 0, 'model.layers.12': 0, 'model.layers.13': 0, 'model.layers.14': 0, 'model.layers.15': 0, 'model.layers.16': 1, 'model.layers.17': 1, 'model.layers.18': 1, 'model.layers.19': 1, 'model.layers.20': 1, 'model.layers.21': 1, 'model.layers.22': 1, 'model.layers.23': 1, 'model.layers.24': 1, 'model.layers.25': 1, 'model.layers.26': 1, 'model.layers.27': 1, 'model.layers.28': 1, 'model.layers.29': 1, 'model.layers.30': 1, 'model.layers.31': 1, 'model.norm': 1, 'model.rotary_emb': 1, 'lm_head': 1}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "import os\n",
    "import logging\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def load_model(model_filepath: str, torch_dtype:torch.dtype=torch.float16):\n",
    "    \"\"\"Load a model given a specific model_path.\n",
    "\n",
    "    Args:\n",
    "        model_filepath: str - Path to where the model is stored\n",
    "\n",
    "    Returns:\n",
    "        model, dict, str - Torch model + dictionary representation of the model + model class name\n",
    "    \"\"\"\n",
    "\n",
    "    conf_filepath = os.path.join(model_filepath, 'reduced-config.json')\n",
    "    logging.info(\"Loading config file from: {}\".format(conf_filepath))\n",
    "    with open(conf_filepath, 'r') as fh:\n",
    "        round_config = json.load(fh)\n",
    "\n",
    "    logging.info(\"Loading model from filepath: {}\".format(model_filepath))\n",
    "    # https://huggingface.co/docs/transformers/installation#offline-mode\n",
    "    if round_config['use_lora']:\n",
    "        base_model_filepath = os.path.join(model_filepath, 'base-model')\n",
    "        logging.info(\"loading the base model (before LORA) from {}\".format(base_model_filepath))\n",
    "        model = AutoModelForCausalLM.from_pretrained(base_model_filepath, device_map = \"auto\", trust_remote_code=True, torch_dtype=torch_dtype, local_files_only=True)\n",
    "        # model = AutoModelForCausalLM.from_pretrained(round_config['model_architecture'], trust_remote_code=True, attn_implementation=\"flash_attention_2\", torch_dtype=torch_dtype)\n",
    "\n",
    "        fine_tuned_model_filepath = os.path.join(model_filepath, 'fine-tuned-model')\n",
    "        logging.info(\"loading the LORA adapter onto the base model from {}\".format(fine_tuned_model_filepath))\n",
    "        model.load_adapter(fine_tuned_model_filepath)\n",
    "    else:\n",
    "        fine_tuned_model_filepath = os.path.join(model_filepath, 'fine-tuned-model')\n",
    "        logging.info(\"Loading full fine tune checkpoint into cpu from {}\".format(fine_tuned_model_filepath))\n",
    "        model = AutoModelForCausalLM.from_pretrained(fine_tuned_model_filepath, device_map = \"auto\", trust_remote_code=True, torch_dtype=torch_dtype, local_files_only=True)\n",
    "        # model = AutoModelForCausalLM.from_pretrained(fine_tuned_model_filepath, trust_remote_code=True, attn_implementation=\"flash_attention_2\", torch_dtype=torch_dtype)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    tokenizer_filepath = os.path.join(model_filepath, 'tokenizer')\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_filepath)\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def _two_gpu_max_memory(headroom_gb=2):\n",
    "    \"\"\"\n",
    "    Reserve headroom so HF sharding MUST split across both 16GB T4s.\n",
    "    \"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        return None\n",
    "    n = torch.cuda.device_count()\n",
    "    cap = f\"{16 - headroom_gb}GiB\"  # e.g., \"14GiB\"\n",
    "    return {i: cap for i in range(n)}\n",
    "\n",
    "def _common_from_pretrained_kwargs():\n",
    "    \"\"\"\n",
    "    Settings that reduce both CPU and GPU peak memory and use a lean attention impl.\n",
    "    \"\"\"\n",
    "    kw = dict(\n",
    "        trust_remote_code=True,\n",
    "        local_files_only=True,\n",
    "        torch_dtype=torch.float16,     # T4 → FP16\n",
    "        low_cpu_mem_usage=True,        # streaming load\n",
    "        offload_state_dict=True,       # avoid CPU spikes\n",
    "        attn_implementation=\"sdpa\",    # available by default on Kaggle\n",
    "    )\n",
    "    mm = _two_gpu_max_memory(headroom_gb=2)\n",
    "    if mm and torch.cuda.device_count() > 1:\n",
    "        kw[\"device_map\"] = \"auto\"\n",
    "        kw[\"max_memory\"] = mm\n",
    "        # Optional if host RAM is tight:\n",
    "        # kw[\"offload_folder\"] = \"/kaggle/working/offload\"\n",
    "    else:\n",
    "        kw[\"device_map\"] = {\"\": 0}\n",
    "    return kw\n",
    "\n",
    "def load_model_and_tokenizer(model_dir: str, merge_lora: bool = True):\n",
    "    \"\"\"\n",
    "    Robust loader for full fine-tunes or LoRA adapters stored under `model_dir`.\n",
    "    Expects:\n",
    "      - reduced-config.json with {\"use_lora\": <bool>, ...}\n",
    "      - For LoRA: base-model/, fine-tuned-model/\n",
    "      - For full FT: fine-tuned-model/\n",
    "      - tokenizer/ with tokenizer files\n",
    "    Returns: (model, tokenizer)\n",
    "    \"\"\"\n",
    "    conf_path = os.path.join(model_dir, \"reduced-config.json\")\n",
    "    logging.info(f\"Loading config: {conf_path}\")\n",
    "    with open(conf_path, \"r\") as fh:\n",
    "        cfg = json.load(fh)\n",
    "\n",
    "    kw = _common_from_pretrained_kwargs()\n",
    "\n",
    "    if cfg.get(\"use_lora\", False):\n",
    "        base_dir = os.path.join(model_dir, \"base-model\")\n",
    "        lora_dir = os.path.join(model_dir, \"fine-tuned-model\")\n",
    "\n",
    "        logging.info(f\"Loading base model: {base_dir}\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(base_dir, **kw)\n",
    "        logging.info(f\"Attaching LoRA adapter: {lora_dir}\")\n",
    "        # If PeftModel is missing, use .load_adapter if available\n",
    "        try:\n",
    "            model = PeftModel.from_pretrained(model, lora_dir, is_trainable=False)  # type: ignore\n",
    "        except Exception:\n",
    "            model.load_adapter(lora_dir)\n",
    "\n",
    "    else:\n",
    "        ft_dir = os.path.join(model_dir, \"fine-tuned-model\")\n",
    "        logging.info(f\"Loading full fine-tuned model: {ft_dir}\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(ft_dir, **kw)\n",
    "\n",
    "    # Tokenizer hygiene\n",
    "    tok_dir = os.path.join(model_dir, \"tokenizer\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tok_dir, use_fast=True, local_files_only=True)\n",
    "    if tokenizer.pad_token_id is None and tokenizer.eos_token_id is not None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"  # better for causal LMs with dynamic padding\n",
    "\n",
    "    # Runtime memory knobs for your gradient-based rollout\n",
    "    model.eval()\n",
    "    if hasattr(model.config, \"use_cache\"):\n",
    "        model.config.use_cache = False  # reduce KV/activation memory during your search\n",
    "\n",
    "    # Optional: quick sanity check of sharding\n",
    "    try:\n",
    "        print(getattr(model, \"hf_device_map\", \"no device map\"))\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "model, tokenizer = load_model_and_tokenizer(\n",
    "    model_dir=\"/kaggle/input/trojai-rev2-00000001/id-00000001\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fde7343",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T14:04:15.055831Z",
     "iopub.status.busy": "2025-11-20T14:04:15.055174Z",
     "iopub.status.idle": "2025-11-20T14:04:15.061480Z",
     "shell.execute_reply": "2025-11-20T14:04:15.060691Z",
     "shell.execute_reply.started": "2025-11-20T14:04:15.055810Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_emb_layer(model):\n",
    "    model.eval()\n",
    "    if hasattr(model.config, \"use_cache\"):\n",
    "        model.config.use_cache = False\n",
    "    return model.get_input_embeddings()\n",
    "\n",
    "emb_layer = get_emb_layer(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d7c5689",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T14:04:15.062965Z",
     "iopub.status.busy": "2025-11-20T14:04:15.062265Z",
     "iopub.status.idle": "2025-11-20T14:04:15.106685Z",
     "shell.execute_reply": "2025-11-20T14:04:15.105854Z",
     "shell.execute_reply.started": "2025-11-20T14:04:15.062938Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def project_suffix_to_tokens_and_diagnostics(\n",
    "    suffix_z,\n",
    "    emb_layer,\n",
    "    tokenizer,\n",
    "):\n",
    "    \"\"\"\n",
    "    suffix_z: (Ls, E) - optimized continuous suffix embeddings\n",
    "    emb_layer: model.get_input_embeddings()\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        dev = emb_layer.weight.device\n",
    "        E = emb_layer.weight        # (V, E)\n",
    "        V, d = E.shape\n",
    "\n",
    "        # Move suffix to same device\n",
    "        z = suffix_z.to(dev)        # (Ls, E)\n",
    "\n",
    "        # ---- Fix dtype mismatch: work in float32 for stability ----\n",
    "        E_f = E.float()             # (V, E) fp32\n",
    "        z_f = z.float()             # (Ls, E) fp32\n",
    "\n",
    "        # Normalize for cosine similarity\n",
    "        E_norm = F.normalize(E_f, dim=-1)        # (V, E)\n",
    "        z_norm = F.normalize(z_f, dim=-1)        # (Ls, E)\n",
    "\n",
    "        # Cosine similarity: (V, E) @ (E, Ls) -> (V, Ls)\n",
    "        cos_sim = torch.matmul(E_norm, z_norm.T)  # (V, Ls)\n",
    "\n",
    "        # For each suffix position, get best matching token\n",
    "        best_token_ids = cos_sim.argmax(dim=0)    # (Ls,)\n",
    "\n",
    "        # Diagnostics: L2 distances between z[i] and E[best_token_ids[i]]\n",
    "        nearest_embs = E_f[best_token_ids]        # (Ls, E) fp32\n",
    "        l2_dists = (z_f - nearest_embs).norm(dim=-1)  # (Ls,)\n",
    "\n",
    "        print(\"L2 distance between optimized embeddings and nearest token embeddings:\")\n",
    "        print(f\"  min:  {l2_dists.min().item():.6f}\")\n",
    "        print(f\"  max:  {l2_dists.max().item():.6f}\")\n",
    "        print(f\"  mean: {l2_dists.mean().item():.6f}\")\n",
    "\n",
    "        best_cos = cos_sim.max(dim=0).values     # (Ls,)\n",
    "        print(\"Cosine similarity of optimized embeddings to nearest tokens:\")\n",
    "        print(f\"  min:  {best_cos.min().item():.6f}\")\n",
    "        print(f\"  max:  {best_cos.max().item():.6f}\")\n",
    "        print(f\"  mean: {best_cos.mean().item():.6f}\")\n",
    "\n",
    "        suffix_token_ids = best_token_ids.cpu()\n",
    "        suffix_tokens = tokenizer.convert_ids_to_tokens(suffix_token_ids.tolist())\n",
    "        suffix_text = tokenizer.decode(\n",
    "            suffix_token_ids.tolist(),\n",
    "            skip_special_tokens=False\n",
    "        )\n",
    "\n",
    "        print(\"\\nProjected discrete suffix token IDs:\", suffix_token_ids.tolist())\n",
    "        print(\"Projected discrete suffix tokens:\", suffix_tokens)\n",
    "        print(\"Projected suffix as text:\", repr(suffix_text))\n",
    "\n",
    "        return suffix_token_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0f69993",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T14:04:15.124058Z",
     "iopub.status.busy": "2025-11-20T14:04:15.123891Z",
     "iopub.status.idle": "2025-11-20T14:04:15.135689Z",
     "shell.execute_reply": "2025-11-20T14:04:15.134941Z",
     "shell.execute_reply.started": "2025-11-20T14:04:15.124044Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def read_suffix_pt(filepath: str) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Read suffix embeddings from a .pt file.\n",
    "    \"\"\"\n",
    "    suffix_z = torch.load(filepath)\n",
    "    return suffix_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be447ee5-5706-40bd-9f93-1c9fade94c57",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T14:08:14.086171Z",
     "iopub.status.busy": "2025-11-20T14:08:14.085302Z",
     "iopub.status.idle": "2025-11-20T14:08:14.098286Z",
     "shell.execute_reply": "2025-11-20T14:08:14.097374Z",
     "shell.execute_reply.started": "2025-11-20T14:08:14.086143Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch import amp\n",
    "\n",
    "def entropy_loss(batch_logits):\n",
    "    \"\"\"\n",
    "    batch_logits: (B, V) logits for the token of interest.\n",
    "    Returns scalar mean entropy.\n",
    "    \"\"\"\n",
    "    log_probs = F.log_softmax(batch_logits, dim=-1)\n",
    "    probs = log_probs.exp()\n",
    "    entropy = -(probs * log_probs).sum(dim=-1)  # (B,)\n",
    "    return entropy.mean()\n",
    "\n",
    "def compute_rollout_entropy_loss_for_suffix(\n",
    "    model,\n",
    "    emb_layer,\n",
    "    batch,\n",
    "    suffix_z,           # (Ls, E) nn.Parameter\n",
    "    n_tokens=10,\n",
    "    amp_dtype=torch.float16,\n",
    "):\n",
    "    \"\"\"\n",
    "    - For each example, build [prompt][suffix_z] in embedding space.\n",
    "    - Pad all to same length -> [prompt][suffix][PAD].\n",
    "    - Roll out n_tokens-1 tokens under inference_mode.\n",
    "    - Final forward WITH grad gives entropy loss on last generated token.\n",
    "    - Gradients flow into suffix_z only (prompts are detached).\n",
    "    \"\"\"\n",
    "    prompts = batch[\"input_ids\"]   # list of 1D LongTensors (Li,)\n",
    "    dev = emb_layer.weight.device\n",
    "    suffix_z = suffix_z.to(dev)    # (Ls, E)\n",
    "\n",
    "    B = len(prompts)\n",
    "    Ls, E = suffix_z.shape\n",
    "\n",
    "    base_embs = []   # each: (Li+Ls, E)\n",
    "    base_lens = []   # each: scalar length Li+Ls\n",
    "\n",
    "    # --- Build per-example [prompt][suffix] in embedding space ---\n",
    "    for p_ids in prompts:\n",
    "        p_ids_dev = p_ids.to(dev)\n",
    "        p_emb = emb_layer(p_ids_dev).detach()   # (Li, E), prompts are constants\n",
    "        base = torch.cat([p_emb, suffix_z], dim=0)  # (Li+Ls, E)\n",
    "        base_embs.append(base)\n",
    "        base_lens.append(base.size(0))\n",
    "\n",
    "    # Pad to [prompt][suffix][PAD...] across the batch\n",
    "    base = pad_sequence(base_embs, batch_first=True)   # (B, max_len, E)\n",
    "    base_lens = torch.tensor(base_lens, device=dev)    # (B,)\n",
    "    max_len = base.size(1)\n",
    "\n",
    "    # Attention mask: 1 for real tokens, 0 for pad\n",
    "    arange = torch.arange(max_len, device=dev).unsqueeze(0)  # (1, max_len)\n",
    "    base_mask = (arange < base_lens.unsqueeze(1)).long()     # (B, max_len)\n",
    "\n",
    "    # Now base has structure [prompt][suffix][PAD] per row (masked pads)\n",
    "\n",
    "    def _one_step_logits(e, m):\n",
    "        with amp.autocast(\"cuda\", dtype=amp_dtype):\n",
    "            out = model(\n",
    "                inputs_embeds=e,\n",
    "                attention_mask=m,\n",
    "                use_cache=False,\n",
    "                output_attentions=False,\n",
    "                output_hidden_states=False,\n",
    "                return_dict=True,\n",
    "            )\n",
    "        return out.logits[:, -1, :]  # (B, V)\n",
    "\n",
    "    # ---------- Rollout under no grad (from detached base) ----------\n",
    "    work_e = base.detach()  # rollout uses constants\n",
    "    work_m = base_mask\n",
    "    added_embs = []         # list of (B, E) constants\n",
    "\n",
    "    T = max(0, n_tokens - 1)\n",
    "    with torch.inference_mode():\n",
    "        for _ in range(T):\n",
    "            logits_t = _one_step_logits(work_e, work_m)\n",
    "            probs_t = torch.softmax(logits_t, dim=-1)\n",
    "            next_ids = torch.argmax(probs_t, dim=-1)        # (B,)\n",
    "\n",
    "            next_emb = emb_layer(next_ids.to(dev)).detach() # (B, E)\n",
    "            added_embs.append(next_emb)\n",
    "\n",
    "            work_e = torch.cat([work_e, next_emb.unsqueeze(1)], dim=1)\n",
    "            work_m = torch.cat(\n",
    "                [work_m, torch.ones((B, 1), dtype=work_m.dtype, device=dev)],\n",
    "                dim=1,\n",
    "            )\n",
    "\n",
    "    # ---------- Final inputs: [prompt][suffix][PAD] + generated tokens ----------\n",
    "    if len(added_embs) > 0:\n",
    "        added = torch.stack(added_embs, dim=1)              # (B, T, E)\n",
    "        final_emb = torch.cat([base, added], dim=1)         # (B, max_len+T, E)\n",
    "        # mask: base_mask for original, ones for generated\n",
    "        gen_mask = torch.ones((B, T), dtype=base_mask.dtype, device=dev)\n",
    "        final_mask = torch.cat([base_mask, gen_mask], dim=1)\n",
    "    else:\n",
    "        final_emb = base\n",
    "        final_mask = base_mask\n",
    "\n",
    "    # ---------- Final step WITH grad (depends on suffix_z) ----------\n",
    "    logits_last = _one_step_logits(final_emb, final_mask)\n",
    "    loss = entropy_loss(logits_last)\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc50bc60",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T14:16:59.120215Z",
     "iopub.status.busy": "2025-11-20T14:16:59.119578Z",
     "iopub.status.idle": "2025-11-20T14:17:06.411777Z",
     "shell.execute_reply": "2025-11-20T14:17:06.410871Z",
     "shell.execute_reply.started": "2025-11-20T14:16:59.120186Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read suffix_r0_e0 successfully.\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  4.062184\n",
      "  max:  6.090151\n",
      "  mean: 5.216796\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.113557\n",
      "  max:  0.263459\n",
      "  mean: 0.181232\n",
      "\n",
      "Projected discrete suffix token IDs: [3798, 467, 1846, 857, 2, 8011, 966, 18760, 29892, 29871]\n",
      "Projected discrete suffix tokens: ['oreferrer', ').', '.)', '▁С', '</s>', '▁Its', '▁les', '▁pushed', ',', '▁']\n",
      "Projected suffix as text: 'oreferrer)..) С</s> Its les pushed, '\n",
      "suffix loss (before projection): 0.0007734298706054688\n",
      "suffix loss (after projection): 2.861328125\n",
      "\n",
      "\n",
      "Read suffix_r0_e1 successfully.\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  5.848429\n",
      "  max:  8.728996\n",
      "  mean: 7.386656\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.298667\n",
      "  max:  0.651578\n",
      "  mean: 0.530579\n",
      "\n",
      "Projected discrete suffix token IDs: [3798, 467, 1846, 857, 2, 8011, 966, 18760, 29892, 29871]\n",
      "Projected discrete suffix tokens: ['oreferrer', ').', '.)', '▁С', '</s>', '▁Its', '▁les', '▁pushed', ',', '▁']\n",
      "Projected suffix as text: 'oreferrer)..) С</s> Its les pushed, '\n",
      "suffix loss (before projection): 2.1755695343017578e-05\n",
      "suffix loss (after projection): 2.861328125\n",
      "\n",
      "\n",
      "Read suffix_r0_e2 successfully.\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  nan\n",
      "  max:  nan\n",
      "  mean: nan\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  nan\n",
      "  max:  nan\n",
      "  mean: nan\n",
      "\n",
      "Projected discrete suffix token IDs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Projected discrete suffix tokens: ['<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>']\n",
      "Projected suffix as text: '<unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>'\n",
      "suffix loss (before projection): nan\n",
      "suffix loss (after projection): 2.8359375\n",
      "\n",
      "\n",
      "Read suffix_r1_e0 successfully.\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  1.090888\n",
      "  max:  1.518787\n",
      "  mean: 1.262459\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.148704\n",
      "  max:  0.488291\n",
      "  mean: 0.290124\n",
      "\n",
      "Projected discrete suffix token IDs: [3798, 21672, 18960, 30035, 28356, 500, 21145, 4806, 23951, 10667]\n",
      "Projected discrete suffix tokens: ['oreferrer', '▁sap', '▁whites', 'ă', '▁divent', '▁}', 'umbnail', 'We', 'illiant', '▁Sym']\n",
      "Projected suffix as text: 'oreferrer sap whitesă divent }umbnailWeilliant Sym'\n",
      "suffix loss (before projection): 0.0601806640625\n",
      "suffix loss (after projection): 4.34765625\n",
      "\n",
      "\n",
      "Read suffix_r1_e1 successfully.\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  1.013680\n",
      "  max:  1.535294\n",
      "  mean: 1.205463\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.177795\n",
      "  max:  0.606572\n",
      "  mean: 0.403620\n",
      "\n",
      "Projected discrete suffix token IDs: [3798, 21672, 18960, 30035, 28356, 500, 21145, 4806, 23951, 10667]\n",
      "Projected discrete suffix tokens: ['oreferrer', '▁sap', '▁whites', 'ă', '▁divent', '▁}', 'umbnail', 'We', 'illiant', '▁Sym']\n",
      "Projected suffix as text: 'oreferrer sap whitesă divent }umbnailWeilliant Sym'\n",
      "suffix loss (before projection): 0.08038330078125\n",
      "suffix loss (after projection): 4.34765625\n",
      "\n",
      "\n",
      "Read suffix_r1_e2 successfully.\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  0.538047\n",
      "  max:  1.280141\n",
      "  mean: 0.872813\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.476642\n",
      "  max:  0.894003\n",
      "  mean: 0.734498\n",
      "\n",
      "Projected discrete suffix token IDs: [3798, 21672, 18960, 30035, 28356, 500, 21145, 4806, 23951, 10667]\n",
      "Projected discrete suffix tokens: ['oreferrer', '▁sap', '▁whites', 'ă', '▁divent', '▁}', 'umbnail', 'We', 'illiant', '▁Sym']\n",
      "Projected suffix as text: 'oreferrer sap whitesă divent }umbnailWeilliant Sym'\n",
      "suffix loss (before projection): 0.5908203125\n",
      "suffix loss (after projection): 4.34765625\n",
      "\n",
      "\n",
      "Read suffix_r2_e0 successfully.\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  0.763118\n",
      "  max:  1.308334\n",
      "  mean: 1.125865\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.076365\n",
      "  max:  0.151187\n",
      "  mean: 0.115952\n",
      "\n",
      "Projected discrete suffix token IDs: [3798, 7636, 12648, 6951, 24239, 31664, 11229, 8639, 6416, 19552]\n",
      "Projected discrete suffix tokens: ['oreferrer', '▁edge', 'MAX', 'zu', '▁Parker', '߬', '▁становника', 'раль', '▁fall', '▁demselben']\n",
      "Projected suffix as text: 'oreferrer edgeMAXzu Parker߬ становникараль fall demselben'\n",
      "suffix loss (before projection): 0.344482421875\n",
      "suffix loss (after projection): 0.14501953125\n",
      "\n",
      "\n",
      "Read suffix_r2_e1 successfully.\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  0.807260\n",
      "  max:  1.300004\n",
      "  mean: 1.109996\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.123661\n",
      "  max:  0.257945\n",
      "  mean: 0.182921\n",
      "\n",
      "Projected discrete suffix token IDs: [3798, 7636, 12648, 6951, 24239, 31664, 11229, 8639, 6416, 19552]\n",
      "Projected discrete suffix tokens: ['oreferrer', '▁edge', 'MAX', 'zu', '▁Parker', '߬', '▁становника', 'раль', '▁fall', '▁demselben']\n",
      "Projected suffix as text: 'oreferrer edgeMAXzu Parker߬ становникараль fall demselben'\n",
      "suffix loss (before projection): 0.0699462890625\n",
      "suffix loss (after projection): 0.14501953125\n",
      "\n",
      "\n",
      "Read suffix_r2_e2 successfully.\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  0.807463\n",
      "  max:  1.299490\n",
      "  mean: 1.108758\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.124316\n",
      "  max:  0.262788\n",
      "  mean: 0.187447\n",
      "\n",
      "Projected discrete suffix token IDs: [3798, 7636, 12648, 6951, 24239, 31664, 11229, 8639, 6416, 19552]\n",
      "Projected discrete suffix tokens: ['oreferrer', '▁edge', 'MAX', 'zu', '▁Parker', '߬', '▁становника', 'раль', '▁fall', '▁demselben']\n",
      "Projected suffix as text: 'oreferrer edgeMAXzu Parker߬ становникараль fall demselben'\n",
      "suffix loss (before projection): 0.8681640625\n",
      "suffix loss (after projection): 0.14501953125\n",
      "\n",
      "\n",
      "Read suffix_r3_e0 successfully.\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  0.855636\n",
      "  max:  1.305697\n",
      "  mean: 1.155816\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.068621\n",
      "  max:  0.090556\n",
      "  mean: 0.080401\n",
      "\n",
      "Projected discrete suffix token IDs: [15188, 19942, 22842, 820, 21585, 31889, 20967, 31771, 21800, 26095]\n",
      "Projected discrete suffix tokens: ['▁musique', ']];', 'posa', 'ough', '▁Stanis', 'ദ', 'Boot', '⊤', '▁mex', 'currency']\n",
      "Projected suffix as text: 'musique]];posaough StanisദBoot⊤ mexcurrency'\n",
      "suffix loss (before projection): 0.63818359375\n",
      "suffix loss (after projection): 1.0419921875\n",
      "\n",
      "\n",
      "Read suffix_r3_e1 successfully.\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  0.856451\n",
      "  max:  1.303246\n",
      "  mean: 1.155134\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.071646\n",
      "  max:  0.101251\n",
      "  mean: 0.087357\n",
      "\n",
      "Projected discrete suffix token IDs: [15188, 19942, 22842, 820, 21585, 31889, 20967, 31771, 21800, 26095]\n",
      "Projected discrete suffix tokens: ['▁musique', ']];', 'posa', 'ough', '▁Stanis', 'ദ', 'Boot', '⊤', '▁mex', 'currency']\n",
      "Projected suffix as text: 'musique]];posaough StanisദBoot⊤ mexcurrency'\n",
      "suffix loss (before projection): 4.65234375\n",
      "suffix loss (after projection): 1.0419921875\n",
      "\n",
      "\n",
      "Read suffix_r3_e2 successfully.\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  0.856464\n",
      "  max:  1.302104\n",
      "  mean: 1.153428\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.075696\n",
      "  max:  0.114784\n",
      "  mean: 0.094299\n",
      "\n",
      "Projected discrete suffix token IDs: [15188, 19942, 22842, 820, 21585, 31889, 20967, 31771, 21800, 26095]\n",
      "Projected discrete suffix tokens: ['▁musique', ']];', 'posa', 'ough', '▁Stanis', 'ദ', 'Boot', '⊤', '▁mex', 'currency']\n",
      "Projected suffix as text: 'musique]];posaough StanisദBoot⊤ mexcurrency'\n",
      "suffix loss (before projection): 2.00390625\n",
      "suffix loss (after projection): 1.0419921875\n",
      "\n",
      "\n",
      "Read suffix_r4_e0 successfully.\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  0.782014\n",
      "  max:  1.288778\n",
      "  mean: 1.187878\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.059087\n",
      "  max:  0.070745\n",
      "  mean: 0.064839\n",
      "\n",
      "Projected discrete suffix token IDs: [8166, 6457, 16120, 21055, 8918, 22242, 1, 11263, 26661, 4813]\n",
      "Projected discrete suffix tokens: ['ół', '▁рабо', '▁expecting', 'MR', '▁Ur', '▁dzie', '<s>', '▁Cru', '▁деся', '▁Ke']\n",
      "Projected suffix as text: 'ół рабо expectingMR Ur dzie<s> Cru деся Ke'\n",
      "suffix loss (before projection): 4.3671875\n",
      "suffix loss (after projection): 2.6875\n",
      "\n",
      "\n",
      "Read suffix_r4_e1 successfully.\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  0.780872\n",
      "  max:  1.288242\n",
      "  mean: 1.187126\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.060775\n",
      "  max:  0.071865\n",
      "  mean: 0.066148\n",
      "\n",
      "Projected discrete suffix token IDs: [8166, 6457, 16120, 21055, 8918, 22242, 1, 11263, 26661, 4813]\n",
      "Projected discrete suffix tokens: ['ół', '▁рабо', '▁expecting', 'MR', '▁Ur', '▁dzie', '<s>', '▁Cru', '▁деся', '▁Ke']\n",
      "Projected suffix as text: 'ół рабо expectingMR Ur dzie<s> Cru деся Ke'\n",
      "suffix loss (before projection): 1.9150390625\n",
      "suffix loss (after projection): 2.6875\n",
      "\n",
      "\n",
      "Read suffix_r4_e2 successfully.\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  0.780382\n",
      "  max:  1.287997\n",
      "  mean: 1.186750\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.061402\n",
      "  max:  0.072423\n",
      "  mean: 0.066800\n",
      "\n",
      "Projected discrete suffix token IDs: [8166, 6457, 16120, 21055, 8918, 22242, 1, 11263, 26661, 4813]\n",
      "Projected discrete suffix tokens: ['ół', '▁рабо', '▁expecting', 'MR', '▁Ur', '▁dzie', '<s>', '▁Cru', '▁деся', '▁Ke']\n",
      "Projected suffix as text: 'ół рабо expectingMR Ur dzie<s> Cru деся Ke'\n",
      "suffix loss (before projection): 1.931640625\n",
      "suffix loss (after projection): 2.6875\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    for j in range(3):\n",
    "        suffix_z = read_suffix_pt(f\"/kaggle/working/hotflip/rounds/suffix_r{i}_e{j}.pt\")\n",
    "        print(f\"Read suffix_r{i}_e{j} successfully.\")\n",
    "        suffix_token_ids = project_suffix_to_tokens_and_diagnostics(suffix_z, emb_layer, tokenizer)\n",
    "        print(f\"suffix loss (before projection): {compute_rollout_entropy_loss_for_suffix(model, emb_layer, {'input_ids': [torch.tensor([tokenizer.eos_token_id])]}, suffix_z, n_tokens=10)}\")\n",
    "        suffix_z = emb_layer(suffix_token_ids)\n",
    "        print(f\"suffix loss (after projection): {compute_rollout_entropy_loss_for_suffix(model, emb_layer, {'input_ids': [torch.tensor([tokenizer.eos_token_id])]}, suffix_z, n_tokens=10)}\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8363949,
     "sourceId": 13197689,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
