{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13197689,"sourceType":"datasetVersion","datasetId":8363949}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"c5dde332","cell_type":"code","source":"def refresh_repo():\n    %cd /kaggle/working\n    %rm -rf hotflip\n    !git clone https://github.com/jefri021/hotflip.git\n    %cd /kaggle/working/hotflip/\n    !git pull origin main\n\nrefresh_repo()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T14:02:21.750651Z","iopub.execute_input":"2025-11-20T14:02:21.750882Z","iopub.status.idle":"2025-11-20T14:02:28.092614Z","shell.execute_reply.started":"2025-11-20T14:02:21.750855Z","shell.execute_reply":"2025-11-20T14:02:28.091596Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working\nCloning into 'hotflip'...\nremote: Enumerating objects: 198, done.\u001b[K\nremote: Counting objects: 100% (198/198), done.\u001b[K\nremote: Compressing objects: 100% (125/125), done.\u001b[K\nremote: Total 198 (delta 90), reused 162 (delta 54), pack-reused 0 (from 0)\u001b[K\nReceiving objects: 100% (198/198), 9.00 MiB | 4.02 MiB/s, done.\nResolving deltas: 100% (90/90), done.\n/kaggle/working/hotflip\nFrom https://github.com/jefri021/hotflip\n * branch            main       -> FETCH_HEAD\nAlready up to date.\n","output_type":"stream"}],"execution_count":1},{"id":"4581d65d","cell_type":"code","source":"import torch\nimport json\nimport os\nimport logging\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch.nn.functional as F\n\ndef load_model(model_filepath: str, torch_dtype:torch.dtype=torch.float16):\n    \"\"\"Load a model given a specific model_path.\n\n    Args:\n        model_filepath: str - Path to where the model is stored\n\n    Returns:\n        model, dict, str - Torch model + dictionary representation of the model + model class name\n    \"\"\"\n\n    conf_filepath = os.path.join(model_filepath, 'reduced-config.json')\n    logging.info(\"Loading config file from: {}\".format(conf_filepath))\n    with open(conf_filepath, 'r') as fh:\n        round_config = json.load(fh)\n\n    logging.info(\"Loading model from filepath: {}\".format(model_filepath))\n    # https://huggingface.co/docs/transformers/installation#offline-mode\n    if round_config['use_lora']:\n        base_model_filepath = os.path.join(model_filepath, 'base-model')\n        logging.info(\"loading the base model (before LORA) from {}\".format(base_model_filepath))\n        model = AutoModelForCausalLM.from_pretrained(base_model_filepath, device_map = \"auto\", trust_remote_code=True, torch_dtype=torch_dtype, local_files_only=True)\n        # model = AutoModelForCausalLM.from_pretrained(round_config['model_architecture'], trust_remote_code=True, attn_implementation=\"flash_attention_2\", torch_dtype=torch_dtype)\n\n        fine_tuned_model_filepath = os.path.join(model_filepath, 'fine-tuned-model')\n        logging.info(\"loading the LORA adapter onto the base model from {}\".format(fine_tuned_model_filepath))\n        model.load_adapter(fine_tuned_model_filepath)\n    else:\n        fine_tuned_model_filepath = os.path.join(model_filepath, 'fine-tuned-model')\n        logging.info(\"Loading full fine tune checkpoint into cpu from {}\".format(fine_tuned_model_filepath))\n        model = AutoModelForCausalLM.from_pretrained(fine_tuned_model_filepath, device_map = \"auto\", trust_remote_code=True, torch_dtype=torch_dtype, local_files_only=True)\n        # model = AutoModelForCausalLM.from_pretrained(fine_tuned_model_filepath, trust_remote_code=True, attn_implementation=\"flash_attention_2\", torch_dtype=torch_dtype)\n\n    model.eval()\n\n    tokenizer_filepath = os.path.join(model_filepath, 'tokenizer')\n    tokenizer = AutoTokenizer.from_pretrained(tokenizer_filepath)\n\n    return model, tokenizer\n\n\ndef _two_gpu_max_memory(headroom_gb=2):\n    \"\"\"\n    Reserve headroom so HF sharding MUST split across both 16GB T4s.\n    \"\"\"\n    if not torch.cuda.is_available():\n        return None\n    n = torch.cuda.device_count()\n    cap = f\"{16 - headroom_gb}GiB\"  # e.g., \"14GiB\"\n    return {i: cap for i in range(n)}\n\ndef _common_from_pretrained_kwargs():\n    \"\"\"\n    Settings that reduce both CPU and GPU peak memory and use a lean attention impl.\n    \"\"\"\n    kw = dict(\n        trust_remote_code=True,\n        local_files_only=True,\n        torch_dtype=torch.float16,     # T4 → FP16\n        low_cpu_mem_usage=True,        # streaming load\n        offload_state_dict=True,       # avoid CPU spikes\n        attn_implementation=\"sdpa\",    # available by default on Kaggle\n    )\n    mm = _two_gpu_max_memory(headroom_gb=2)\n    if mm and torch.cuda.device_count() > 1:\n        kw[\"device_map\"] = \"auto\"\n        kw[\"max_memory\"] = mm\n        # Optional if host RAM is tight:\n        # kw[\"offload_folder\"] = \"/kaggle/working/offload\"\n    else:\n        kw[\"device_map\"] = {\"\": 0}\n    return kw\n\ndef load_model_and_tokenizer(model_dir: str, merge_lora: bool = True):\n    \"\"\"\n    Robust loader for full fine-tunes or LoRA adapters stored under `model_dir`.\n    Expects:\n      - reduced-config.json with {\"use_lora\": <bool>, ...}\n      - For LoRA: base-model/, fine-tuned-model/\n      - For full FT: fine-tuned-model/\n      - tokenizer/ with tokenizer files\n    Returns: (model, tokenizer)\n    \"\"\"\n    conf_path = os.path.join(model_dir, \"reduced-config.json\")\n    logging.info(f\"Loading config: {conf_path}\")\n    with open(conf_path, \"r\") as fh:\n        cfg = json.load(fh)\n\n    kw = _common_from_pretrained_kwargs()\n\n    if cfg.get(\"use_lora\", False):\n        base_dir = os.path.join(model_dir, \"base-model\")\n        lora_dir = os.path.join(model_dir, \"fine-tuned-model\")\n\n        logging.info(f\"Loading base model: {base_dir}\")\n        model = AutoModelForCausalLM.from_pretrained(base_dir, **kw)\n        logging.info(f\"Attaching LoRA adapter: {lora_dir}\")\n        # If PeftModel is missing, use .load_adapter if available\n        try:\n            model = PeftModel.from_pretrained(model, lora_dir, is_trainable=False)  # type: ignore\n        except Exception:\n            model.load_adapter(lora_dir)\n\n    else:\n        ft_dir = os.path.join(model_dir, \"fine-tuned-model\")\n        logging.info(f\"Loading full fine-tuned model: {ft_dir}\")\n        model = AutoModelForCausalLM.from_pretrained(ft_dir, **kw)\n\n    # Tokenizer hygiene\n    tok_dir = os.path.join(model_dir, \"tokenizer\")\n    tokenizer = AutoTokenizer.from_pretrained(tok_dir, use_fast=True, local_files_only=True)\n    if tokenizer.pad_token_id is None and tokenizer.eos_token_id is not None:\n        tokenizer.pad_token = tokenizer.eos_token\n    tokenizer.padding_side = \"right\"  # better for causal LMs with dynamic padding\n\n    # Runtime memory knobs for your gradient-based rollout\n    model.eval()\n    if hasattr(model.config, \"use_cache\"):\n        model.config.use_cache = False  # reduce KV/activation memory during your search\n\n    # Optional: quick sanity check of sharding\n    try:\n        print(getattr(model, \"hf_device_map\", \"no device map\"))\n    except Exception:\n        pass\n\n    return model, tokenizer\n\nmodel, tokenizer = load_model_and_tokenizer(\n    model_dir=\"/kaggle/input/trojai-rev2-00000001/id-00000001\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T14:02:28.094727Z","iopub.execute_input":"2025-11-20T14:02:28.095092Z","iopub.status.idle":"2025-11-20T14:04:15.054278Z","shell.execute_reply.started":"2025-11-20T14:02:28.095054Z","shell.execute_reply":"2025-11-20T14:04:15.053458Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n2025-11-20 14:02:45.194325: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1763647365.392497      48 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1763647365.444028      48 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"684e416dcc8246a39560826607e2e34c"}},"metadata":{}},{"name":"stdout","text":"{'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 0, 'model.layers.9': 0, 'model.layers.10': 0, 'model.layers.11': 0, 'model.layers.12': 0, 'model.layers.13': 0, 'model.layers.14': 0, 'model.layers.15': 0, 'model.layers.16': 1, 'model.layers.17': 1, 'model.layers.18': 1, 'model.layers.19': 1, 'model.layers.20': 1, 'model.layers.21': 1, 'model.layers.22': 1, 'model.layers.23': 1, 'model.layers.24': 1, 'model.layers.25': 1, 'model.layers.26': 1, 'model.layers.27': 1, 'model.layers.28': 1, 'model.layers.29': 1, 'model.layers.30': 1, 'model.layers.31': 1, 'model.norm': 1, 'model.rotary_emb': 1, 'lm_head': 1}\n","output_type":"stream"}],"execution_count":2},{"id":"1fde7343","cell_type":"code","source":"def get_emb_layer(model):\n    model.eval()\n    if hasattr(model.config, \"use_cache\"):\n        model.config.use_cache = False\n    return model.get_input_embeddings()\n\nemb_layer = get_emb_layer(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T14:04:15.055174Z","iopub.execute_input":"2025-11-20T14:04:15.055831Z","iopub.status.idle":"2025-11-20T14:04:15.061480Z","shell.execute_reply.started":"2025-11-20T14:04:15.055810Z","shell.execute_reply":"2025-11-20T14:04:15.060691Z"}},"outputs":[],"execution_count":3},{"id":"9d7c5689","cell_type":"code","source":"def project_suffix_to_tokens_and_diagnostics(\n    suffix_z,\n    emb_layer,\n    tokenizer,\n):\n    \"\"\"\n    suffix_z: (Ls, E) - optimized continuous suffix embeddings\n    emb_layer: model.get_input_embeddings()\n    \"\"\"\n    with torch.no_grad():\n        dev = emb_layer.weight.device\n        E = emb_layer.weight        # (V, E)\n        V, d = E.shape\n\n        # Move suffix to same device\n        z = suffix_z.to(dev)        # (Ls, E)\n\n        # ---- Fix dtype mismatch: work in float32 for stability ----\n        E_f = E.float()             # (V, E) fp32\n        z_f = z.float()             # (Ls, E) fp32\n\n        # Normalize for cosine similarity\n        E_norm = F.normalize(E_f, dim=-1)        # (V, E)\n        z_norm = F.normalize(z_f, dim=-1)        # (Ls, E)\n\n        # Cosine similarity: (V, E) @ (E, Ls) -> (V, Ls)\n        cos_sim = torch.matmul(E_norm, z_norm.T)  # (V, Ls)\n\n        # For each suffix position, get best matching token\n        best_token_ids = cos_sim.argmax(dim=0)    # (Ls,)\n\n        # Diagnostics: L2 distances between z[i] and E[best_token_ids[i]]\n        nearest_embs = E_f[best_token_ids]        # (Ls, E) fp32\n        l2_dists = (z_f - nearest_embs).norm(dim=-1)  # (Ls,)\n\n        print(\"L2 distance between optimized embeddings and nearest token embeddings:\")\n        print(f\"  min:  {l2_dists.min().item():.6f}\")\n        print(f\"  max:  {l2_dists.max().item():.6f}\")\n        print(f\"  mean: {l2_dists.mean().item():.6f}\")\n\n        best_cos = cos_sim.max(dim=0).values     # (Ls,)\n        print(\"Cosine similarity of optimized embeddings to nearest tokens:\")\n        print(f\"  min:  {best_cos.min().item():.6f}\")\n        print(f\"  max:  {best_cos.max().item():.6f}\")\n        print(f\"  mean: {best_cos.mean().item():.6f}\")\n\n        suffix_token_ids = best_token_ids.cpu()\n        suffix_tokens = tokenizer.convert_ids_to_tokens(suffix_token_ids.tolist())\n        suffix_text = tokenizer.decode(\n            suffix_token_ids.tolist(),\n            skip_special_tokens=False\n        )\n\n        print(\"\\nProjected discrete suffix token IDs:\", suffix_token_ids.tolist())\n        print(\"Projected discrete suffix tokens:\", suffix_tokens)\n        print(\"Projected suffix as text:\", repr(suffix_text))\n\n        return suffix_token_ids\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T14:04:15.062265Z","iopub.execute_input":"2025-11-20T14:04:15.062965Z","iopub.status.idle":"2025-11-20T14:04:15.106685Z","shell.execute_reply.started":"2025-11-20T14:04:15.062938Z","shell.execute_reply":"2025-11-20T14:04:15.105854Z"}},"outputs":[],"execution_count":4},{"id":"e0f69993","cell_type":"code","source":"def read_suffix_pt(filepath: str) -> torch.Tensor:\n    \"\"\"\n    Read suffix embeddings from a .pt file.\n    \"\"\"\n    suffix_z = torch.load(filepath)\n    return suffix_z","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T14:04:15.123891Z","iopub.execute_input":"2025-11-20T14:04:15.124058Z","iopub.status.idle":"2025-11-20T14:04:15.135689Z","shell.execute_reply.started":"2025-11-20T14:04:15.124044Z","shell.execute_reply":"2025-11-20T14:04:15.134941Z"}},"outputs":[],"execution_count":6},{"id":"be447ee5-5706-40bd-9f93-1c9fade94c57","cell_type":"code","source":"from torch.nn.utils.rnn import pad_sequence\nfrom torch import amp\n\ndef entropy_loss(batch_logits):\n    \"\"\"\n    batch_logits: (B, V) logits for the token of interest.\n    Returns scalar mean entropy.\n    \"\"\"\n    log_probs = F.log_softmax(batch_logits, dim=-1)\n    probs = log_probs.exp()\n    entropy = -(probs * log_probs).sum(dim=-1)  # (B,)\n    return entropy.mean()\n\ndef compute_rollout_entropy_loss_for_suffix(\n    model,\n    emb_layer,\n    batch,\n    suffix_z,           # (Ls, E) nn.Parameter\n    n_tokens=10,\n    amp_dtype=torch.float16,\n):\n    \"\"\"\n    - For each example, build [prompt][suffix_z] in embedding space.\n    - Pad all to same length -> [prompt][suffix][PAD].\n    - Roll out n_tokens-1 tokens under inference_mode.\n    - Final forward WITH grad gives entropy loss on last generated token.\n    - Gradients flow into suffix_z only (prompts are detached).\n    \"\"\"\n    prompts = batch[\"input_ids\"]   # list of 1D LongTensors (Li,)\n    dev = emb_layer.weight.device\n    suffix_z = suffix_z.to(dev)    # (Ls, E)\n\n    B = len(prompts)\n    Ls, E = suffix_z.shape\n\n    base_embs = []   # each: (Li+Ls, E)\n    base_lens = []   # each: scalar length Li+Ls\n\n    # --- Build per-example [prompt][suffix] in embedding space ---\n    for p_ids in prompts:\n        p_ids_dev = p_ids.to(dev)\n        p_emb = emb_layer(p_ids_dev).detach()   # (Li, E), prompts are constants\n        base = torch.cat([p_emb, suffix_z], dim=0)  # (Li+Ls, E)\n        base_embs.append(base)\n        base_lens.append(base.size(0))\n\n    # Pad to [prompt][suffix][PAD...] across the batch\n    base = pad_sequence(base_embs, batch_first=True)   # (B, max_len, E)\n    base_lens = torch.tensor(base_lens, device=dev)    # (B,)\n    max_len = base.size(1)\n\n    # Attention mask: 1 for real tokens, 0 for pad\n    arange = torch.arange(max_len, device=dev).unsqueeze(0)  # (1, max_len)\n    base_mask = (arange < base_lens.unsqueeze(1)).long()     # (B, max_len)\n\n    # Now base has structure [prompt][suffix][PAD] per row (masked pads)\n\n    def _one_step_logits(e, m):\n        with amp.autocast(\"cuda\", dtype=amp_dtype):\n            out = model(\n                inputs_embeds=e,\n                attention_mask=m,\n                use_cache=False,\n                output_attentions=False,\n                output_hidden_states=False,\n                return_dict=True,\n            )\n        return out.logits[:, -1, :]  # (B, V)\n\n    # ---------- Rollout under no grad (from detached base) ----------\n    work_e = base.detach()  # rollout uses constants\n    work_m = base_mask\n    added_embs = []         # list of (B, E) constants\n\n    T = max(0, n_tokens - 1)\n    with torch.inference_mode():\n        for _ in range(T):\n            logits_t = _one_step_logits(work_e, work_m)\n            probs_t = torch.softmax(logits_t, dim=-1)\n            next_ids = torch.argmax(probs_t, dim=-1)        # (B,)\n\n            next_emb = emb_layer(next_ids.to(dev)).detach() # (B, E)\n            added_embs.append(next_emb)\n\n            work_e = torch.cat([work_e, next_emb.unsqueeze(1)], dim=1)\n            work_m = torch.cat(\n                [work_m, torch.ones((B, 1), dtype=work_m.dtype, device=dev)],\n                dim=1,\n            )\n\n    # ---------- Final inputs: [prompt][suffix][PAD] + generated tokens ----------\n    if len(added_embs) > 0:\n        added = torch.stack(added_embs, dim=1)              # (B, T, E)\n        final_emb = torch.cat([base, added], dim=1)         # (B, max_len+T, E)\n        # mask: base_mask for original, ones for generated\n        gen_mask = torch.ones((B, T), dtype=base_mask.dtype, device=dev)\n        final_mask = torch.cat([base_mask, gen_mask], dim=1)\n    else:\n        final_emb = base\n        final_mask = base_mask\n\n    # ---------- Final step WITH grad (depends on suffix_z) ----------\n    logits_last = _one_step_logits(final_emb, final_mask)\n    loss = entropy_loss(logits_last)\n\n    return loss\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T14:08:14.085302Z","iopub.execute_input":"2025-11-20T14:08:14.086171Z","iopub.status.idle":"2025-11-20T14:08:14.098286Z","shell.execute_reply.started":"2025-11-20T14:08:14.086143Z","shell.execute_reply":"2025-11-20T14:08:14.097374Z"}},"outputs":[],"execution_count":8},{"id":"cc50bc60","cell_type":"code","source":"for i in range(5):\n    for j in range(3):\n        suffix_z = read_suffix_pt(f\"/kaggle/working/hotflip/rounds/suffix_r{i}_e{j}.pt\")\n        print(f\"Read suffix_r{i}_e{j} successfully.\")\n        suffix_token_ids = project_suffix_to_tokens_and_diagnostics(suffix_z, emb_layer, tokenizer)\n        print(f\"suffix loss (before projection): {compute_rollout_entropy_loss_for_suffix(model, emb_layer, {'input_ids': [torch.tensor([tokenizer.eos_token_id])]}, suffix_z, n_tokens=10)}\")\n        suffix_z = emb_layer(suffix_token_ids)\n        print(f\"suffix loss (after projection): {compute_rollout_entropy_loss_for_suffix(model, emb_layer, {'input_ids': [torch.tensor([tokenizer.eos_token_id])]}, suffix_z, n_tokens=10)}\\n\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T14:16:59.119578Z","iopub.execute_input":"2025-11-20T14:16:59.120215Z","iopub.status.idle":"2025-11-20T14:17:06.411777Z","shell.execute_reply.started":"2025-11-20T14:16:59.120186Z","shell.execute_reply":"2025-11-20T14:17:06.410871Z"}},"outputs":[{"name":"stdout","text":"Read suffix0 successfully.\nL2 distance between optimized embeddings and nearest token embeddings:\n  min:  5.095870\n  max:  8.697512\n  mean: 6.556546\nCosine similarity of optimized embeddings to nearest tokens:\n  min:  0.062417\n  max:  0.118511\n  mean: 0.085406\n\nProjected discrete suffix token IDs: [4949, 7128, 19466, 20462, 1252, 431, 29914, 20542, 7032, 12583]\nProjected discrete suffix tokens: ['▁/*', '▁appropri', 'Collections', '▁Operation', 'Ex', 'ub', '/', '▁zones', 'Process', '▁Luis']\nProjected suffix as text: '/* appropriCollections OperationExub/ zonesProcess Luis'\nsuffix loss (before projection): 0.88427734375\nsuffix loss (after projection): 2.70703125\n\n\nRead suffix1 successfully.\nL2 distance between optimized embeddings and nearest token embeddings:\n  min:  4.421392\n  max:  10.633087\n  mean: 8.085752\nCosine similarity of optimized embeddings to nearest tokens:\n  min:  0.063286\n  max:  0.176885\n  mean: 0.106763\n\nProjected discrete suffix token IDs: [28574, 29899, 1576, 15851, 28574, 2, 31936, 2, 5215, 346]\nProjected discrete suffix tokens: ['▁Mediabestanden', '-', 'The', 'rias', '▁Mediabestanden', '</s>', '１', '</s>', 'import', 'ce']\nProjected suffix as text: 'Mediabestanden-Therias Mediabestanden</s>１</s>importce'\nsuffix loss (before projection): 4.8359375\nsuffix loss (after projection): 4.1484375\n\n\nRead suffix2 successfully.\nL2 distance between optimized embeddings and nearest token embeddings:\n  min:  2.281430\n  max:  2.895950\n  mean: 2.506272\nCosine similarity of optimized embeddings to nearest tokens:\n  min:  0.062345\n  max:  0.144160\n  mean: 0.084431\n\nProjected discrete suffix token IDs: [29892, 29889, 382, 30772, 28892, 30499, 28892, 6044, 13, 20971]\nProjected discrete suffix tokens: [',', '.', '▁E', '╌', '▁Пів', 'で', '▁Пів', '▁Jac', '<0x0A>', 'Conne']\nProjected suffix as text: ',. E╌ Півで Пів Jac\\nConne'\nsuffix loss (before projection): 5.984375\nsuffix loss (after projection): 4.9140625\n\n\nRead suffix3 successfully.\nL2 distance between optimized embeddings and nearest token embeddings:\n  min:  2.203203\n  max:  4.490810\n  mean: 3.348891\nCosine similarity of optimized embeddings to nearest tokens:\n  min:  0.077428\n  max:  0.246807\n  mean: 0.132137\n\nProjected discrete suffix token IDs: [1724, 1, 13, 20609, 29889, 9143, 2, 29949, 773, 6]\nProjected discrete suffix tokens: ['▁What', '<s>', '<0x0A>', '▁Portály', '.', '▁Hill', '</s>', 'O', '▁using', '<0x03>']\nProjected suffix as text: 'What<s>\\n Portály. Hill</s>O using\\x03'\nsuffix loss (before projection): 0.498779296875\nsuffix loss (after projection): 1.8115234375\n\n\nRead suffix4 successfully.\nL2 distance between optimized embeddings and nearest token embeddings:\n  min:  6.680331\n  max:  10.798526\n  mean: 8.261905\nCosine similarity of optimized embeddings to nearest tokens:\n  min:  0.066422\n  max:  0.124670\n  mean: 0.084292\n\nProjected discrete suffix token IDs: [1, 1317, 11451, 584, 4733, 8073, 29920, 28608, 19103, 166]\nProjected discrete suffix tokens: ['<s>', '▁Is', '▁py', '▁:', '▁models', '▁SD', 'z', '▁Pero', 'cció', '<0xA3>']\nProjected suffix as text: '<s> Is py : models SDz Perocció�'\nsuffix loss (before projection): 0.9306640625\nsuffix loss (after projection): 4.3828125\n\n\n","output_type":"stream"}],"execution_count":16}]}