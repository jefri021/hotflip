{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03da4878",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "def check_memory(gpu_index: int = 0):\n",
    "    if torch.cuda.is_available():\n",
    "        free_memory = torch.cuda.get_device_properties(gpu_index).total_memory - torch.cuda.memory_allocated(gpu_index)\n",
    "        total_memory = torch.cuda.get_device_properties(gpu_index).total_memory\n",
    "        print(f\"Free GPU Memory: {free_memory / 1024**3:.2f} GB\")\n",
    "        print(f\"Total GPU Memory: {total_memory / 1024**3:.2f} GB\")\n",
    "    else:\n",
    "        print(\"CUDA is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37623a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "from typing import List, Union\n",
    "\n",
    "def clear_memory(keep_vars: Union[List[str], None] = None, verbose: bool = True):\n",
    "    \"\"\"\n",
    "    Clears memory while preserving specified variables.\n",
    "    Still clears GPU memory for all CUDA objects, including kept variables.\n",
    "    \n",
    "    Args:\n",
    "        keep_vars: List of variable names to preserve in memory (will still be cleared from GPU)\n",
    "        verbose: Whether to print memory clearing information\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(\"Starting memory clearing process...\")\n",
    "    \n",
    "    # Convert keep_vars to set for faster lookups\n",
    "    keep_set = set(keep_vars) if keep_vars else set()\n",
    "    \n",
    "    # First pass: Move kept CUDA variables to CPU\n",
    "    if torch.cuda.is_available():\n",
    "        for name, var in list(globals().items()):\n",
    "            if name in keep_set and isinstance(var, torch.Tensor) and var.is_cuda:\n",
    "                if verbose:\n",
    "                    print(f\"Moving kept tensor '{name}' to CPU\")\n",
    "                globals()[name] = var.cpu()\n",
    "    \n",
    "    # Clear Python garbage collector\n",
    "    gc.collect()\n",
    "    if verbose:\n",
    "        print(\"Ran Python garbage collection\")\n",
    "    \n",
    "    # Clear CUDA memory if available\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        if verbose:\n",
    "            print(\"Cleared CUDA cache\")\n",
    "            print(f\"Current CUDA memory allocated: {torch.cuda.memory_allocated()/1024**2:.2f} MB\")\n",
    "            print(f\"Current CUDA memory cached: {torch.cuda.memory_reserved()/1024**2:.2f} MB\")\n",
    "    \n",
    "    # Try to clear TensorFlow/Keras if available\n",
    "    try:\n",
    "        import tensorflow as tf\n",
    "        tf.keras.backend.clear_session()\n",
    "        if verbose:\n",
    "            print(\"Cleared TensorFlow/Keras session\")\n",
    "    except ImportError:\n",
    "        pass\n",
    "    \n",
    "    # Delete objects not in keep_vars\n",
    "    for name, var in list(globals().items()):\n",
    "        if not name.startswith('__') and name not in keep_set:\n",
    "            if isinstance(var, (torch.Tensor, torch.nn.Module)):\n",
    "                del globals()[name]\n",
    "                if verbose:\n",
    "                    print(f\"Deleted torch object: {name}\")\n",
    "            elif isinstance(var, list) and var and isinstance(var[0], torch.Tensor):\n",
    "                del globals()[name]\n",
    "                if verbose:\n",
    "                    print(f\"Deleted list of torch tensors: {name}\")\n",
    "    \n",
    "    # Final garbage collection\n",
    "    gc.collect()\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Memory clearing complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe7f4183",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gdown\n",
    "\n",
    "def download_file_from_google_drive(file_id, output_dir, output_filename, quiet=False):\n",
    "    \"\"\"\n",
    "    Downloads a file from Google Drive given its file ID and saves it to the specified directory.\n",
    "    \n",
    "    Args:\n",
    "        file_id (str): The Google Drive file ID (found in the file URL)\n",
    "        output_dir (str): Directory where the file should be saved\n",
    "        output_filename (str): Name of the output file\n",
    "        quiet (bool): Whether to suppress gdown output (default: False)\n",
    "    \n",
    "    Returns:\n",
    "        str: Path to the downloaded file if successful, None otherwise\n",
    "    \"\"\"\n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Full output path\n",
    "    output_file = os.path.join(output_dir, output_filename)\n",
    "    \n",
    "    print(\"Downloading the file...\")\n",
    "    try:\n",
    "        gdown.download(id=file_id, output=output_file, quiet=quiet, fuzzy=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Download failed: {str(e)}\")\n",
    "        return None\n",
    "    \n",
    "    # Verify download\n",
    "    if os.path.exists(output_file):\n",
    "        file_size = os.path.getsize(output_file) / (1024 * 1024)  # in MB\n",
    "        print(f\"Download successful! File saved to: {output_file}\")\n",
    "        print(f\"File size: {file_size:.2f} MB\")\n",
    "        return output_file\n",
    "    else:\n",
    "        print(\"Download failed - file not found\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d55255a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "from typing import List, Union\n",
    "\n",
    "def extract_and_delete_tar_gz(file_path: str, delete_compressed: bool = True) -> bool:\n",
    "    \"\"\"\n",
    "    Extracts a .tar.gz file and optionally deletes the compressed file.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the .tar.gz file\n",
    "        delete_compressed (bool): Whether to delete the compressed file after extraction (default: True)\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if extraction was successful, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Extracting: {file_path}\")\n",
    "        with tarfile.open(file_path, 'r:gz') as tar:\n",
    "            tar.extractall(path=os.path.dirname(file_path))\n",
    "        \n",
    "        if delete_compressed:\n",
    "            os.remove(file_path)\n",
    "            print(f\"Deleted compressed file: {file_path}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def process_directory(directory: str, recursive: bool = True, max_depth: Union[int, None] = None) -> int:\n",
    "    \"\"\"\n",
    "    Processes a directory to find and extract .tar.gz files.\n",
    "    \n",
    "    Args:\n",
    "        directory (str): Directory path to process\n",
    "        recursive (bool): Whether to process subdirectories (default: True)\n",
    "        max_depth (int|None): Maximum recursion depth (None for unlimited)\n",
    "    \n",
    "    Returns:\n",
    "        int: Number of .tar.gz files processed\n",
    "    \"\"\"\n",
    "    processed_count = 0\n",
    "    current_depth = 0\n",
    "    \n",
    "    while True:\n",
    "        found_tar_gz = False\n",
    "        for root, dirs, files in os.walk(directory):\n",
    "            # Calculate current depth\n",
    "            rel_path = os.path.relpath(root, directory)\n",
    "            current_depth = rel_path.count(os.sep) + 1 if rel_path != '.' else 0\n",
    "            \n",
    "            # Skip if beyond max depth\n",
    "            if max_depth is not None and current_depth > max_depth:\n",
    "                continue\n",
    "                \n",
    "            for file in files:\n",
    "                if file.endswith('.tar.gz'):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    if extract_and_delete_tar_gz(file_path):\n",
    "                        processed_count += 1\n",
    "                        found_tar_gz = True\n",
    "        \n",
    "        # If not recursive or no more .tar.gz files found, exit\n",
    "        if not recursive or not found_tar_gz:\n",
    "            break\n",
    "    \n",
    "    return processed_count\n",
    "\n",
    "def process_paths(paths: List[str], recursive: bool = True, max_depth: Union[int, None] = None) -> int:\n",
    "    \"\"\"\n",
    "    Processes a list of paths (files or directories) to extract .tar.gz files.\n",
    "    \n",
    "    Args:\n",
    "        paths (List[str]): List of file/directory paths to process\n",
    "        recursive (bool): Whether to process directories recursively (default: True)\n",
    "        max_depth (int|None): Maximum recursion depth for directories (None for unlimited)\n",
    "    \n",
    "    Returns:\n",
    "        int: Total number of .tar.gz files processed\n",
    "    \"\"\"\n",
    "    total_processed = 0\n",
    "    \n",
    "    for path in paths:\n",
    "        if not os.path.exists(path):\n",
    "            print(f\"Warning: Path does not exist - {path}\")\n",
    "            continue\n",
    "            \n",
    "        if path.endswith('.tar.gz'):\n",
    "            if extract_and_delete_tar_gz(path):\n",
    "                total_processed += 1\n",
    "        elif os.path.isdir(path):\n",
    "            print(f\"Processing directory: {path}\")\n",
    "            total_processed += process_directory(\n",
    "                directory=path,\n",
    "                recursive=recursive,\n",
    "                max_depth=max_depth\n",
    "            )\n",
    "    \n",
    "    print(f\"Total .tar.gz files processed: {total_processed}\")\n",
    "    return total_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0a581a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "import logging\n",
    "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "def load_model(model_filepath: str, torch_dtype:torch.dtype=torch.float16):\n",
    "    \"\"\"Load a model given a specific model_path.\n",
    "\n",
    "    Args:\n",
    "        model_filepath: str - Path to where the model is stored\n",
    "\n",
    "    Returns:\n",
    "        model, dict, str - Torch model + dictionary representation of the model + model class name\n",
    "    \"\"\"\n",
    "\n",
    "    conf_filepath = os.path.join(model_filepath, 'reduced-config.json')\n",
    "    logging.info(\"Loading config file from: {}\".format(conf_filepath))\n",
    "    with open(conf_filepath, 'r') as fh:\n",
    "        round_config = json.load(fh)\n",
    "\n",
    "    logging.info(\"Loading model from filepath: {}\".format(model_filepath))\n",
    "    # https://huggingface.co/docs/transformers/installation#offline-mode\n",
    "    if round_config['use_lora']:\n",
    "        base_model_filepath = os.path.join(model_filepath, 'base-model')\n",
    "        logging.info(\"loading the base model (before LORA) from {}\".format(base_model_filepath))\n",
    "        model = AutoModelForCausalLM.from_pretrained(base_model_filepath, device_map = \"auto\", trust_remote_code=True, torch_dtype=torch_dtype, local_files_only=True)\n",
    "        # model = AutoModelForCausalLM.from_pretrained(round_config['model_architecture'], trust_remote_code=True, attn_implementation=\"flash_attention_2\", torch_dtype=torch_dtype)\n",
    "\n",
    "        fine_tuned_model_filepath = os.path.join(model_filepath, 'fine-tuned-model')\n",
    "        logging.info(\"loading the LORA adapter onto the base model from {}\".format(fine_tuned_model_filepath))\n",
    "        model.load_adapter(fine_tuned_model_filepath)\n",
    "    else:\n",
    "        fine_tuned_model_filepath = os.path.join(model_filepath, 'fine-tuned-model')\n",
    "        logging.info(\"Loading full fine tune checkpoint into cpu from {}\".format(fine_tuned_model_filepath))\n",
    "        model = AutoModelForCausalLM.from_pretrained(fine_tuned_model_filepath, device_map = \"auto\", trust_remote_code=True, torch_dtype=torch_dtype, local_files_only=True)\n",
    "        # model = AutoModelForCausalLM.from_pretrained(fine_tuned_model_filepath, trust_remote_code=True, attn_implementation=\"flash_attention_2\", torch_dtype=torch_dtype)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    tokenizer_filepath = os.path.join(model_filepath, 'tokenizer')\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_filepath)\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "949730f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, logging, torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "def _two_gpu_max_memory(headroom_gb=2):\n",
    "    \"\"\"\n",
    "    Reserve headroom so HF sharding MUST split across both 16GB T4s.\n",
    "    \"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        return None\n",
    "    n = torch.cuda.device_count()\n",
    "    cap = f\"{16 - headroom_gb}GiB\"  # e.g., \"14GiB\"\n",
    "    return {i: cap for i in range(n)}\n",
    "\n",
    "def _common_from_pretrained_kwargs():\n",
    "    \"\"\"\n",
    "    Settings that reduce both CPU and GPU peak memory and use a lean attention impl.\n",
    "    \"\"\"\n",
    "    kw = dict(\n",
    "        trust_remote_code=True,\n",
    "        local_files_only=True,\n",
    "        torch_dtype=torch.float16,     # T4 → FP16\n",
    "        low_cpu_mem_usage=True,        # streaming load\n",
    "        offload_state_dict=True,       # avoid CPU spikes\n",
    "        attn_implementation=\"sdpa\",    # available by default on Kaggle\n",
    "    )\n",
    "    mm = _two_gpu_max_memory(headroom_gb=2)\n",
    "    if mm and torch.cuda.device_count() > 1:\n",
    "        kw[\"device_map\"] = \"auto\"\n",
    "        kw[\"max_memory\"] = mm\n",
    "        # Optional if host RAM is tight:\n",
    "        # kw[\"offload_folder\"] = \"/kaggle/working/offload\"\n",
    "    else:\n",
    "        kw[\"device_map\"] = {\"\": 0}\n",
    "    return kw\n",
    "\n",
    "def load_model_and_tokenizer(model_dir: str, merge_lora: bool = True):\n",
    "    \"\"\"\n",
    "    Robust loader for full fine-tunes or LoRA adapters stored under `model_dir`.\n",
    "    Expects:\n",
    "      - reduced-config.json with {\"use_lora\": <bool>, ...}\n",
    "      - For LoRA: base-model/, fine-tuned-model/\n",
    "      - For full FT: fine-tuned-model/\n",
    "      - tokenizer/ with tokenizer files\n",
    "    Returns: (model, tokenizer)\n",
    "    \"\"\"\n",
    "    conf_path = os.path.join(model_dir, \"reduced-config.json\")\n",
    "    logging.info(f\"Loading config: {conf_path}\")\n",
    "    with open(conf_path, \"r\") as fh:\n",
    "        cfg = json.load(fh)\n",
    "\n",
    "    kw = _common_from_pretrained_kwargs()\n",
    "\n",
    "    if cfg.get(\"use_lora\", False):\n",
    "        base_dir = os.path.join(model_dir, \"base-model\")\n",
    "        lora_dir = os.path.join(model_dir, \"fine-tuned-model\")\n",
    "\n",
    "        logging.info(f\"Loading base model: {base_dir}\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(base_dir, **kw)\n",
    "        logging.info(f\"Attaching LoRA adapter: {lora_dir}\")\n",
    "        # If PeftModel is missing, use .load_adapter if available\n",
    "        try:\n",
    "            model = PeftModel.from_pretrained(model, lora_dir, is_trainable=False)  # type: ignore\n",
    "        except Exception:\n",
    "            model.load_adapter(lora_dir)\n",
    "\n",
    "    else:\n",
    "        ft_dir = os.path.join(model_dir, \"fine-tuned-model\")\n",
    "        logging.info(f\"Loading full fine-tuned model: {ft_dir}\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(ft_dir, **kw)\n",
    "\n",
    "    # Tokenizer hygiene\n",
    "    tok_dir = os.path.join(model_dir, \"tokenizer\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tok_dir, use_fast=True, local_files_only=True)\n",
    "    if tokenizer.pad_token_id is None and tokenizer.eos_token_id is not None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"left\"  # better for causal LMs with dynamic padding\n",
    "\n",
    "    # Runtime memory knobs for your gradient-based rollout\n",
    "    model.eval()\n",
    "    if hasattr(model.config, \"use_cache\"):\n",
    "        model.config.use_cache = False  # reduce KV/activation memory during your search\n",
    "\n",
    "    # Optional: quick sanity check of sharding\n",
    "    try:\n",
    "        print(getattr(model, \"hf_device_map\", \"no device map\"))\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "def download_and_load(file_id, output_filename, load_model_path):\n",
    "    \"\"\"\n",
    "    Wrapper that uses your existing helpers:\n",
    "      - clear_memory(), download_file_from_google_drive(), process_paths()\n",
    "    \"\"\"\n",
    "    clear_memory(verbose=False)\n",
    "\n",
    "    _ = download_file_from_google_drive(\n",
    "        file_id=file_id,\n",
    "        output_dir=\"/kaggle/tmp\",\n",
    "        output_filename=output_filename,\n",
    "        quiet=False\n",
    "    )\n",
    "\n",
    "    process_paths(paths=[\"/kaggle/tmp\"], recursive=True, max_depth=None)\n",
    "\n",
    "    model, tokenizer = load_model_and_tokenizer(load_model_path, merge_lora=True)\n",
    "    return model, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bcb29746",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-26 19:12:03.442808: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1761505923.648915      77 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1761505923.710066      77 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading the file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1lwC9JLRu4Z4SSQwjNtetAymStPqQeaDc\n",
      "From (redirected): https://drive.google.com/uc?id=1lwC9JLRu4Z4SSQwjNtetAymStPqQeaDc&confirm=t&uuid=2e1baab3-18bd-4880-adc2-7bd8a1125ae1\n",
      "To: /kaggle/tmp/model0.tar.gz\n",
      "100%|██████████| 10.6G/10.6G [01:15<00:00, 140MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download successful! File saved to: /kaggle/tmp/model0.tar.gz\n",
      "File size: 10092.92 MB\n",
      "Processing directory: /kaggle/tmp\n",
      "Extracting: /kaggle/tmp/model0.tar.gz\n",
      "Deleted compressed file: /kaggle/tmp/model0.tar.gz\n",
      "Total .tar.gz files processed: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f377379832144f49b44c6519733d6079",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 0, 'model.layers.9': 0, 'model.layers.10': 0, 'model.layers.11': 0, 'model.layers.12': 0, 'model.layers.13': 0, 'model.layers.14': 0, 'model.layers.15': 0, 'model.layers.16': 1, 'model.layers.17': 1, 'model.layers.18': 1, 'model.layers.19': 1, 'model.layers.20': 1, 'model.layers.21': 1, 'model.layers.22': 1, 'model.layers.23': 1, 'model.layers.24': 1, 'model.layers.25': 1, 'model.layers.26': 1, 'model.layers.27': 1, 'model.layers.28': 1, 'model.layers.29': 1, 'model.layers.30': 1, 'model.layers.31': 1, 'model.norm': 1, 'model.rotary_emb': 1, 'lm_head': 1}\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = download_and_load(\n",
    "    file_id=\"1lwC9JLRu4Z4SSQwjNtetAymStPqQeaDc\",\n",
    "    output_filename=\"model0.tar.gz\",\n",
    "    load_model_path=\"/kaggle/tmp/id-00000000\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97cfa3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "def entropy_loss(batch_logits):\n",
    "    log_probs = F.log_softmax(batch_logits, dim=-1)\n",
    "    probs = log_probs.exp()\n",
    "    entropy = -(probs * log_probs).sum(dim=-1)\n",
    "    return entropy.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06b2a239",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import os, torch\n",
    "\n",
    "def load_prompts(tokenizer, args):\n",
    "    # Load once; keep raw text to avoid huge pre-tokenized tensors in RAM\n",
    "    ds = load_dataset(\"tatsu-lab/alpaca\", split=\"train\", cache_dir=args[\"data_dir\"])\n",
    "\n",
    "    def collate(batch):\n",
    "        texts = [ex[\"instruction\"] for ex in batch]\n",
    "        enc = tokenizer(\n",
    "            texts,\n",
    "            padding=True,                  # dynamic padding to batch max\n",
    "            truncation=True,\n",
    "            max_length=args[\"max_length\"],\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return enc[\"input_ids\"], enc[\"attention_mask\"]\n",
    "\n",
    "    num_workers = max(2, os.cpu_count() // 2)\n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size=args[\"batch_size\"],\n",
    "        shuffle=False,\n",
    "        pin_memory=True,\n",
    "        num_workers=num_workers,\n",
    "        persistent_workers=True,\n",
    "        collate_fn=collate\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aaa918cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"data_dir\": \"kaggle/working/data\",\n",
    "    \"max_length\": 512,\n",
    "    \"batch_size\": 16\n",
    "}\n",
    "dataloader = load_prompts(tokenizer, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3cc543d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "def find_embedding_layer(model):\n",
    "    \"\"\"\n",
    "    Attempts to find the embedding layer in an arbitrary PyTorch model.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: A PyTorch model (nn.Module) or Hugging Face model.\n",
    "    \n",
    "    Returns:\n",
    "    - embedding_layer: The nn.Embedding layer (or None if not found).\n",
    "    - path: The attribute path to the embedding layer (e.g., 'embeddings.word_embeddings').\n",
    "    \"\"\"\n",
    "    # Check if the model has a get_input_embeddings method (common in Hugging Face)\n",
    "    if hasattr(model, 'get_input_embeddings'):\n",
    "        emb = model.get_input_embeddings()\n",
    "        if isinstance(emb, nn.Embedding):\n",
    "            return emb, 'get_input_embeddings()'\n",
    "    \n",
    "    # Iterate through all named modules to find an embedding layer\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Embedding):\n",
    "            return module, name\n",
    "    \n",
    "    # If no embedding layer is found, return None\n",
    "    return None, None\n",
    "\n",
    "def freeze_except_embeddings(model, emb_layers):\n",
    "    \"\"\"\n",
    "    Freezes all model parameters except the weights of specified embedding layers.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: PyTorch model (nn.Module).\n",
    "    - emb_layers: Single nn.Embedding layer or list of nn.Embedding layers to keep unfrozen.\n",
    "    \n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # Convert single embedding layer to list for generality\n",
    "    if isinstance(emb_layers, nn.Embedding):\n",
    "        emb_layers = [emb_layers]\n",
    "    \n",
    "    # Validate that emb_layers are part of the model\n",
    "    model_params = set(model.parameters())\n",
    "    for emb_layer in emb_layers:\n",
    "        if not isinstance(emb_layer, nn.Embedding):\n",
    "            raise ValueError(f\"Expected nn.Embedding, got {type(emb_layer)}\")\n",
    "        if emb_layer.weight not in model_params:\n",
    "            raise ValueError(\"Embedding layer weight is not part of the model's parameters\")\n",
    "    \n",
    "    # Get set of embedding weights to keep unfrozen\n",
    "    emb_weights = set(emb_layer.weight for emb_layer in emb_layers)\n",
    "    \n",
    "    # Freeze parameters and clear gradients\n",
    "    for name, param in model.named_parameters():\n",
    "        if param in emb_weights:\n",
    "            param.requires_grad = True  # Ensure embedding weights are trainable\n",
    "        else:\n",
    "            param.requires_grad = False\n",
    "            param.grad = None  # Clear gradients to save memory\n",
    "    \n",
    "    # Verify embedding layers remain unfrozen\n",
    "    for emb_layer in emb_layers:\n",
    "        assert emb_layer.weight.requires_grad, f\"Embedding layer {emb_layer} was unexpectedly frozen\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed712f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_to_word(token_id, tokenizer):\n",
    "    \"\"\"Convert a token ID to its corresponding word using the tokenizer.\"\"\"\n",
    "    return tokenizer.decode([token_id]).strip()\n",
    "\n",
    "def word_to_token(word, tokenizer):\n",
    "    \"\"\"Convert a word to its corresponding token ID using the tokenizer.\"\"\"\n",
    "    return tokenizer.encode(word, add_special_tokens=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ea947bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda.amp import autocast\n",
    "\n",
    "@torch.no_grad()\n",
    "def _one_step(model, embeddings, attention_mask, amp_dtype=torch.float16):\n",
    "    with autocast(dtype=amp_dtype):\n",
    "        out = model(inputs_embeds=embeddings, attention_mask=attention_mask)\n",
    "        logits = out.logits[:, -1, :]\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "    return probs  # (B, V)\n",
    "\n",
    "def compute_loss(\n",
    "    model, emb_layer, embeddings, attention_mask, loss_fn,\n",
    "    n_tokens=10, amp_dtype=torch.float16, track_last_only=True\n",
    "):\n",
    "    \"\"\"\n",
    "    If track_last_only=True:\n",
    "      - Roll n-1 steps with no grad (cheap).\n",
    "      - Take 1 final tracked step and compute loss there only.\n",
    "    \"\"\"\n",
    "    B, L, E = embeddings.shape\n",
    "    dev = embeddings.device\n",
    "\n",
    "    # Roll n-1 steps without grad to grow the sequence cheaply\n",
    "    for _ in range(n_tokens):\n",
    "        probs = _one_step(model, embeddings, attention_mask, amp_dtype)\n",
    "        w = emb_layer.weight.to(dev)\n",
    "        probs = probs.to(w.dtype)           # align dtype: float16 on T4\n",
    "        next_embeds = probs @ w\n",
    "        embeddings = torch.cat([embeddings, next_embeds.unsqueeze(1)], dim=1)\n",
    "        attention_mask = torch.cat(\n",
    "            [attention_mask, torch.ones((B, 1), dtype=attention_mask.dtype, device=dev)], dim=1\n",
    "        )\n",
    "\n",
    "    # Final step with grad tracking\n",
    "    with autocast(dtype=amp_dtype):\n",
    "        out = model(inputs_embeds=embeddings, attention_mask=attention_mask)\n",
    "        logits = out.logits[:, -1, :]          # (B, V)\n",
    "        loss = loss_fn(logits) if track_last_only else 0.0\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8e3474a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def _first_device_of_embedding(model):\n",
    "    emb, _ = find_embedding_layer(model)\n",
    "    if emb is None:\n",
    "        raise RuntimeError(\"Could not find an nn.Embedding in the model.\")\n",
    "    return emb, emb.weight.device\n",
    "\n",
    "def find_best_flip(\n",
    "    model, batch, loss_fn, max_length_tokens, batch_id, topk, vocab_chunk\n",
    "):\n",
    "    \"\"\"\n",
    "    Memory-lean search:\n",
    "      - compute grad wrt embeddings only for the final step,\n",
    "      - process each sample independently,\n",
    "      - chunk vocab to avoid allocating (V×L) at once.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    emb_layer, first_dev = _first_device_of_embedding(model)\n",
    "\n",
    "    input_ids = batch[0].to(first_dev, non_blocking=True)      # (B, L)\n",
    "    attention_mask = batch[1].to(first_dev, non_blocking=True) # (B, L)\n",
    "\n",
    "    # Build initial embeddings (needs to be tracked)\n",
    "    freeze_except_embeddings(model, emb_layer)\n",
    "    embeddings = emb_layer(input_ids)                          # (B, L, E)\n",
    "    embeddings.retain_grad()\n",
    "\n",
    "    # Compute loss (only last step tracked)\n",
    "    loss = compute_loss(\n",
    "        model, emb_layer, embeddings, attention_mask,\n",
    "        loss_fn=loss_fn, n_tokens=max_length_tokens,\n",
    "        amp_dtype=torch.float16, track_last_only=True\n",
    "    )\n",
    "\n",
    "    # Backprop to get grads wrt embeddings\n",
    "    model.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "\n",
    "    grads = embeddings.grad.detach()   # (B, L, E)\n",
    "    embeds_det = embeddings.detach()   # (B, L, E)\n",
    "    del embeddings, loss\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # We'll need vocab embeddings on GPU, but in CHUNKS\n",
    "    results = []\n",
    "\n",
    "    B, L, E = grads.shape\n",
    "    V = emb_layer.weight.size(0)\n",
    "    dev = first_dev\n",
    "\n",
    "    # s_i = (g_i · v_i) per position, per sample -> (B, L)\n",
    "    s_i = (grads * embeds_det).sum(dim=2)  # (B, L)\n",
    "\n",
    "    # If you have attention_mask available in this scope, mask out padding positions now.\n",
    "    # Make sure it's on the same device as scores later.\n",
    "    # Uncomment if attention_mask exists:\n",
    "    # attn_mask = attention_mask.to(dev, non_blocking=True)  # (B, L)\n",
    "    # mask_b1l = (attn_mask == 0).unsqueeze(1)               # (B, 1, L)\n",
    "\n",
    "    # Running accumulators across vocab chunks\n",
    "    if topk == 1:\n",
    "        best_vals = torch.full((B,), float(\"inf\"), device=dev, dtype=grads.dtype)\n",
    "        best_flat_idx = torch.full((B,), -1, device=dev, dtype=torch.long)\n",
    "    else:\n",
    "        vals_keep = None  # (B, k_accum)\n",
    "        idx_keep  = None  # (B, k_accum)\n",
    "\n",
    "    offset = 0  # how many (vocab positions * L) we've traversed so far\n",
    "\n",
    "    for start in tqdm(range(0, V, vocab_chunk), desc=\"Processing vocab chunks\"):\n",
    "        end = min(start + vocab_chunk, V)\n",
    "        vocab_slice = emb_layer.weight[start:end].to(dev, non_blocking=True)  # (vchunk, E)\n",
    "        # scores = (B, vchunk, L) = (vchunk, E) @ (B, E, L)\n",
    "        # using einsum handles broadcasting cleanly\n",
    "        scores = torch.einsum(\"ve,ble->bvl\", vocab_slice, grads)  # (B, vchunk, L)\n",
    "\n",
    "        # subtract s_i across L\n",
    "        scores = scores - s_i.unsqueeze(1)  # (B, 1, L) broadcast\n",
    "\n",
    "        # Optional: mask padding positions so they are never picked\n",
    "        # if 'mask_b1l' defined above:\n",
    "        # scores = scores.masked_fill(mask_b1l, float(\"inf\"))\n",
    "\n",
    "        # Flatten chunk per sample to shape (B, vchunk*L)\n",
    "        flat = scores.reshape(B, -1)\n",
    "\n",
    "        if topk == 1:\n",
    "            # best of this chunk per sample\n",
    "            chunk_vals, chunk_idx = torch.min(flat, dim=1)  # (B,)\n",
    "            # where better than current best, update\n",
    "            update = chunk_vals < best_vals\n",
    "            best_vals = torch.where(update, chunk_vals, best_vals)\n",
    "            best_flat_idx = torch.where(update, chunk_idx + offset, best_flat_idx)\n",
    "        else:\n",
    "            k_here = min(topk, flat.size(1))\n",
    "            chunk_vals, chunk_idx = torch.topk(flat, k=k_here, largest=False, dim=1)  # (B, k_here)\n",
    "            chunk_idx = chunk_idx + offset  # globalize indices\n",
    "\n",
    "            if vals_keep is None:\n",
    "                vals_keep, idx_keep = chunk_vals, chunk_idx\n",
    "            else:\n",
    "                # Merge with accumulated bests and reselect top-k\n",
    "                vals_keep = torch.cat([vals_keep, chunk_vals], dim=1)  # (B, k_accum + k_here)\n",
    "                idx_keep  = torch.cat([idx_keep,  chunk_idx],  dim=1)  # (B, k_accum + k_here)\n",
    "                # Re-topk across the concatenated candidates\n",
    "                k_sel = min(topk, vals_keep.size(1))\n",
    "                sel_vals, sel_pos = torch.topk(vals_keep, k=k_sel, largest=False, dim=1)  # (B, k_sel)\n",
    "                # Gather the matching global indices\n",
    "                batch_ids = torch.arange(B, device=dev).unsqueeze(1).expand_as(sel_pos)\n",
    "                idx_keep = idx_keep[batch_ids, sel_pos]  # (B, k_sel)\n",
    "                vals_keep = sel_vals\n",
    "\n",
    "        # free chunk temporaries\n",
    "        del vocab_slice, scores, flat\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # advance flat offset by the chunk length (vchunk * L)\n",
    "        offset += (end - start) * L\n",
    "\n",
    "    # ----- Convert batched results to your result list -----\n",
    "    if topk == 1:\n",
    "        # best_flat_idx encodes (v, pos) as: idx = v * L + pos\n",
    "        best_v   = (best_flat_idx // L).tolist()\n",
    "        best_pos = (best_flat_idx %  L).tolist()\n",
    "        best_val = best_vals.tolist()\n",
    "\n",
    "        for b in range(B):\n",
    "            results.append({\n",
    "                \"best_position\": int(best_pos[b]),\n",
    "                \"best_vocab_index\": int(best_v[b]),\n",
    "                \"min_score\": float(best_val[b]),\n",
    "                \"sample_id\": int(b),\n",
    "                \"batch_id\": int(batch_id)\n",
    "            })\n",
    "    else:\n",
    "        # idx_keep/vals_keep are (B, topk)\n",
    "        v_idx = (idx_keep // L).tolist()\n",
    "        pos_i = (idx_keep %  L).tolist()\n",
    "        vals  =  vals_keep.tolist()\n",
    "\n",
    "        for b in range(B):\n",
    "            pairs = [{\"best_position\": int(pos_i[b][j]),\n",
    "                    \"best_vocab_index\": int(v_idx[b][j]),\n",
    "                    \"min_score\": float(vals[b][j]),\n",
    "                    \"sample_id\": int(b),\n",
    "                    \"batch_id\": int(batch_id)}\n",
    "                    for j in range(len(v_idx[b]))]\n",
    "            results.extend(pairs)\n",
    "\n",
    "    del grads, embeds_det, input_ids, attention_mask\n",
    "    torch.cuda.empty_cache(); gc.collect()\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c06bd502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free GPU Memory: 8.45 GB\n",
      "Total GPU Memory: 14.74 GB\n"
     ]
    }
   ],
   "source": [
    "check_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d473dd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def get_top_k_by_min_score(dict_list, k, per_sample=True):\n",
    "    items = [d for d in dict_list if (\"min_score\" in d and \"sample_id\" in d)]\n",
    "    if not items:\n",
    "        raise NotImplementedError\n",
    "    if per_sample:\n",
    "        best = {}\n",
    "        for d in items:\n",
    "            sid = int(d[\"sample_id\"])\n",
    "            if (sid not in best) or (d[\"min_score\"] < best[sid][\"min_score\"]):\n",
    "                best[sid] = d\n",
    "        items = list(best.values())\n",
    "    items.sort(key=lambda x: x[\"min_score\"])\n",
    "    return items[:k]\n",
    "\n",
    "\n",
    "def _collate_varlen(samples):\n",
    "    \"\"\"\n",
    "    samples: list of tuples (ids_1D, mask_1D) with possibly varying lengths.\n",
    "    Pads within the batch using each sample's last token id as pad value; mask pads with 0.\n",
    "    \"\"\"\n",
    "    if not samples:\n",
    "        return torch.empty(0, dtype=torch.long), torch.empty(0, dtype=torch.long)\n",
    "    max_len = max(s[0].numel() for s in samples)\n",
    "    ids_out, msk_out = [], []\n",
    "    for ids, msk in samples:\n",
    "        pad_val = int(ids[-1].item()) if ids.numel() > 0 else 0\n",
    "        if ids.numel() < max_len:\n",
    "            ids = F.pad(ids, (0, max_len - ids.numel()), value=pad_val)\n",
    "            msk = F.pad(msk, (0, max_len - msk.numel()), value=0)\n",
    "        ids_out.append(ids)\n",
    "        msk_out.append(msk)\n",
    "    return torch.stack(ids_out, dim=0), torch.stack(msk_out, dim=0)\n",
    "\n",
    "def apply_topk_results_to_inputs(\n",
    "    original_dataloader,\n",
    "    top_results,\n",
    "    batch_cache,                 # REQUIRED: dict {batch_id: (ids_cpu, mask_cpu)}\n",
    "    device=\"cpu\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Build and return a DataLoader containing ONE edited sample PER flip entry.\n",
    "    Uses `batch_cache` captured during the scoring pass, so it doesn't rescan the loader.\n",
    "\n",
    "    Each top_results item must include: batch_id, sample_id, best_position, best_vocab_index.\n",
    "    \"\"\"\n",
    "    # Mirror original loader knobs\n",
    "    bs   = getattr(original_dataloader, \"batch_size\", 1)\n",
    "    nw   = getattr(original_dataloader, \"num_workers\", 0)\n",
    "    pin  = getattr(original_dataloader, \"pin_memory\", device != \"cpu\")\n",
    "    pers = getattr(original_dataloader, \"persistent_workers\", False)\n",
    "\n",
    "    if not top_results:\n",
    "        return DataLoader([], batch_size=bs, shuffle=False, pin_memory=pin,\n",
    "                          num_workers=nw, persistent_workers=pers if nw > 0 else False,\n",
    "                          collate_fn=_collate_varlen)\n",
    "\n",
    "    # Group flips by batch for efficient lookup\n",
    "    flips_by_batch = defaultdict(list)  # batch_id -> list[(sample_id, pos, tok)]\n",
    "    for r in top_results:\n",
    "        flips_by_batch[int(r[\"batch_id\"])].append(\n",
    "            (int(r[\"sample_id\"]), int(r[\"best_position\"]), int(r[\"best_vocab_index\"]))\n",
    "        )\n",
    "\n",
    "    edited_samples = []\n",
    "    for b_id, flips in flips_by_batch.items():\n",
    "        if b_id not in batch_cache:\n",
    "            # No cache for this batch — skip its flips\n",
    "            continue\n",
    "        ids_cpu, msk_cpu = batch_cache[b_id]  # (B,L), (B,L)\n",
    "        B = ids_cpu.size(0)\n",
    "        for s, p, t in flips:\n",
    "            if 0 <= s < B:\n",
    "                ids_1d = ids_cpu[s].clone()\n",
    "                msk_1d = msk_cpu[s].clone()\n",
    "                if 0 <= p < ids_1d.numel():\n",
    "                    ids_1d[p] = t\n",
    "                    msk_1d[p] = 1\n",
    "                    edited_samples.append((ids_1d, msk_1d))\n",
    "                # else: out-of-range pos → skip\n",
    "\n",
    "    # Return loader over ONLY edited samples; safe even if empty\n",
    "    return DataLoader(\n",
    "        edited_samples,\n",
    "        batch_size=bs,\n",
    "        shuffle=True,\n",
    "        pin_memory=pin,\n",
    "        num_workers=nw,\n",
    "        persistent_workers=pers if nw > 0 else False,\n",
    "        collate_fn=_collate_varlen,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a32ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, gc\n",
    "\n",
    "results = []\n",
    "batch_id = 0\n",
    "rounds = 10\n",
    "topk_values = [26858, 14427, 7750, 4163, 2236, 1201, 645, 347, 186, 100]\n",
    "\n",
    "for r in tqdm(range(rounds)):\n",
    "    print(f\"=== Round {r+1}/{rounds} ===\")\n",
    "    batch_cache = {}\n",
    "    #############\n",
    "    if round == 0:\n",
    "        results = [{'best_position': 21, 'best_vocab_index': 25167, 'min_score': -32.375, 'sample_id': 6, 'batch_id': 2426}, {'best_position': 21, 'best_vocab_index': 30615, 'min_score': -32.25, 'sample_id': 9, 'batch_id': 2522}, {'best_position': 21, 'best_vocab_index': 30289, 'min_score': -31.40625, 'sample_id': 10, 'batch_id': 1427}, {'best_position': 24, 'best_vocab_index': 30466, 'min_score': -30.0, 'sample_id': 0, 'batch_id': 2087}, {'best_position': 23, 'best_vocab_index': 9588, 'min_score': -28.40625, 'sample_id': 13, 'batch_id': 84}, {'best_position': 28, 'best_vocab_index': 30143, 'min_score': -28.03125, 'sample_id': 7, 'batch_id': 3213}, {'best_position': 15, 'best_vocab_index': 30289, 'min_score': -27.484375, 'sample_id': 1, 'batch_id': 827}, {'best_position': 33, 'best_vocab_index': 9588, 'min_score': -26.265625, 'sample_id': 2, 'batch_id': 2389}, {'best_position': 21, 'best_vocab_index': 22506, 'min_score': -24.515625, 'sample_id': 4, 'batch_id': 3142}, {'best_position': 18, 'best_vocab_index': 30143, 'min_score': -22.921875, 'sample_id': 3, 'batch_id': 312}, {'best_position': 37, 'best_vocab_index': 4250, 'min_score': -22.65625, 'sample_id': 12, 'batch_id': 1920}, {'best_position': 19, 'best_vocab_index': 2, 'min_score': -21.671875, 'sample_id': 11, 'batch_id': 2232}, {'best_position': 21, 'best_vocab_index': 9588, 'min_score': -21.34375, 'sample_id': 5, 'batch_id': 583}, {'best_position': 14, 'best_vocab_index': 7784, 'min_score': -21.328125, 'sample_id': 8, 'batch_id': 1272}, {'best_position': 5, 'best_vocab_index': 21588, 'min_score': -16.875, 'sample_id': 14, 'batch_id': 2546}, {'best_position': 23, 'best_vocab_index': 9588, 'min_score': -16.203125, 'sample_id': 15, 'batch_id': 203}]\n",
    "        for batch in tqdm(dataloader, desc=\"Loading batches\"):\n",
    "            batch_cache[batch_id] = (batch[0].cpu(), batch[1].cpu())\n",
    "    else:\n",
    "        for batch in tqdm(dataloader, desc=\"Processing batches\"):\n",
    "            _, first_dev = _first_device_of_embedding(model)\n",
    "            batch = (batch[0].to(first_dev, non_blocking=True),\n",
    "                    batch[1].to(first_dev, non_blocking=True))\n",
    "            batch_cache[batch_id] = (batch[0].cpu(), batch[1].cpu())\n",
    "\n",
    "            results.extend(find_best_flip(\n",
    "                model=model,\n",
    "                batch=batch,\n",
    "                loss_fn=entropy_loss,\n",
    "                max_length_tokens=10,\n",
    "                batch_id=batch_id,\n",
    "                topk=5,\n",
    "                vocab_chunk=8192,         # tune: 4096–16384 depending on VRAM\n",
    "            ))\n",
    "            batch_id += 1\n",
    "\n",
    "            # free per-iteration junk\n",
    "            del batch\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    # Keep only the top-k and rebuild the loader\n",
    "    print(f\"results before: {results}\")\n",
    "    results = get_top_k_by_min_score(results, k=topk_values[int(r)])\n",
    "    print(f\"results after: {results}\")\n",
    "    dataloader = apply_topk_results_to_inputs(dataloader, results, batch_cache)\n",
    "\n",
    "    # housekeeping\n",
    "    torch.cuda.empty_cache(); gc.collect()\n",
    "\n",
    "print(f\"final results: {results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cccdbfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free GPU Memory: 1.95 GB\n",
      "Total GPU Memory: 14.74 GB\n"
     ]
    }
   ],
   "source": [
    "check_memory()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
