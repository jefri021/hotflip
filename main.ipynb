{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03da4878",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "def check_memory(gpu_index: int = 0):\n",
    "    if torch.cuda.is_available():\n",
    "        free_memory = torch.cuda.get_device_properties(gpu_index).total_memory - torch.cuda.memory_allocated(gpu_index)\n",
    "        total_memory = torch.cuda.get_device_properties(gpu_index).total_memory\n",
    "        print(f\"Free GPU Memory: {free_memory / 1024**3:.2f} GB\")\n",
    "        print(f\"Total GPU Memory: {total_memory / 1024**3:.2f} GB\")\n",
    "    else:\n",
    "        print(\"CUDA is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4875bdb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working\n",
      "Cloning into 'hotflip'...\n",
      "remote: Enumerating objects: 90, done.\u001b[K\n",
      "remote: Counting objects: 100% (90/90), done.\u001b[K\n",
      "remote: Compressing objects: 100% (48/48), done.\u001b[K\n",
      "remote: Total 90 (delta 32), reused 82 (delta 24), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (90/90), 3.47 MiB | 3.38 MiB/s, done.\n",
      "Resolving deltas: 100% (32/32), done.\n",
      "/kaggle/working/hotflip\n",
      "From https://github.com/jefri021/hotflip\n",
      " * branch            main       -> FETCH_HEAD\n",
      "Already up to date.\n"
     ]
    }
   ],
   "source": [
    "def refresh_repo():\n",
    "    %cd /kaggle/working\n",
    "    %rm -rf hotflip\n",
    "    !git clone https://github.com/jefri021/hotflip.git\n",
    "    %cd /kaggle/working/hotflip/\n",
    "    !git pull origin main\n",
    "\n",
    "refresh_repo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37623a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "from typing import List, Union\n",
    "\n",
    "def clear_memory(keep_vars: Union[List[str], None] = None, verbose: bool = True):\n",
    "    \"\"\"\n",
    "    Clears memory while preserving specified variables.\n",
    "    Still clears GPU memory for all CUDA objects, including kept variables.\n",
    "    \n",
    "    Args:\n",
    "        keep_vars: List of variable names to preserve in memory (will still be cleared from GPU)\n",
    "        verbose: Whether to print memory clearing information\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(\"Starting memory clearing process...\")\n",
    "    \n",
    "    # Convert keep_vars to set for faster lookups\n",
    "    keep_set = set(keep_vars) if keep_vars else set()\n",
    "    \n",
    "    # First pass: Move kept CUDA variables to CPU\n",
    "    if torch.cuda.is_available():\n",
    "        for name, var in list(globals().items()):\n",
    "            if name in keep_set and isinstance(var, torch.Tensor) and var.is_cuda:\n",
    "                if verbose:\n",
    "                    print(f\"Moving kept tensor '{name}' to CPU\")\n",
    "                globals()[name] = var.cpu()\n",
    "    \n",
    "    # Clear Python garbage collector\n",
    "    gc.collect()\n",
    "    if verbose:\n",
    "        print(\"Ran Python garbage collection\")\n",
    "    \n",
    "    # Clear CUDA memory if available\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        if verbose:\n",
    "            print(\"Cleared CUDA cache\")\n",
    "            print(f\"Current CUDA memory allocated: {torch.cuda.memory_allocated()/1024**2:.2f} MB\")\n",
    "            print(f\"Current CUDA memory cached: {torch.cuda.memory_reserved()/1024**2:.2f} MB\")\n",
    "    \n",
    "    # Try to clear TensorFlow/Keras if available\n",
    "    try:\n",
    "        import tensorflow as tf\n",
    "        tf.keras.backend.clear_session()\n",
    "        if verbose:\n",
    "            print(\"Cleared TensorFlow/Keras session\")\n",
    "    except ImportError:\n",
    "        pass\n",
    "    \n",
    "    # Delete objects not in keep_vars\n",
    "    for name, var in list(globals().items()):\n",
    "        if not name.startswith('__') and name not in keep_set:\n",
    "            if isinstance(var, (torch.Tensor, torch.nn.Module)):\n",
    "                del globals()[name]\n",
    "                if verbose:\n",
    "                    print(f\"Deleted torch object: {name}\")\n",
    "            elif isinstance(var, list) and var and isinstance(var[0], torch.Tensor):\n",
    "                del globals()[name]\n",
    "                if verbose:\n",
    "                    print(f\"Deleted list of torch tensors: {name}\")\n",
    "    \n",
    "    # Final garbage collection\n",
    "    gc.collect()\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Memory clearing complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe7f4183",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gdown\n",
    "\n",
    "def download_file_from_google_drive(file_id, output_dir, output_filename, quiet=False):\n",
    "    \"\"\"\n",
    "    Downloads a file from Google Drive given its file ID and saves it to the specified directory.\n",
    "    \n",
    "    Args:\n",
    "        file_id (str): The Google Drive file ID (found in the file URL)\n",
    "        output_dir (str): Directory where the file should be saved\n",
    "        output_filename (str): Name of the output file\n",
    "        quiet (bool): Whether to suppress gdown output (default: False)\n",
    "    \n",
    "    Returns:\n",
    "        str: Path to the downloaded file if successful, None otherwise\n",
    "    \"\"\"\n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Full output path\n",
    "    output_file = os.path.join(output_dir, output_filename)\n",
    "    \n",
    "    print(\"Downloading the file...\")\n",
    "    try:\n",
    "        gdown.download(id=file_id, output=output_file, quiet=quiet, fuzzy=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Download failed: {str(e)}\")\n",
    "        return None\n",
    "    \n",
    "    # Verify download\n",
    "    if os.path.exists(output_file):\n",
    "        file_size = os.path.getsize(output_file) / (1024 * 1024)  # in MB\n",
    "        print(f\"Download successful! File saved to: {output_file}\")\n",
    "        print(f\"File size: {file_size:.2f} MB\")\n",
    "        return output_file\n",
    "    else:\n",
    "        print(\"Download failed - file not found\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d55255a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "from typing import List, Union\n",
    "\n",
    "def extract_and_delete_tar_gz(file_path: str, delete_compressed: bool = True) -> bool:\n",
    "    \"\"\"\n",
    "    Extracts a .tar.gz file and optionally deletes the compressed file.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the .tar.gz file\n",
    "        delete_compressed (bool): Whether to delete the compressed file after extraction (default: True)\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if extraction was successful, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Extracting: {file_path}\")\n",
    "        with tarfile.open(file_path, 'r:gz') as tar:\n",
    "            tar.extractall(path=os.path.dirname(file_path))\n",
    "        \n",
    "        if delete_compressed:\n",
    "            os.remove(file_path)\n",
    "            print(f\"Deleted compressed file: {file_path}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def process_directory(directory: str, recursive: bool = True, max_depth: Union[int, None] = None) -> int:\n",
    "    \"\"\"\n",
    "    Processes a directory to find and extract .tar.gz files.\n",
    "    \n",
    "    Args:\n",
    "        directory (str): Directory path to process\n",
    "        recursive (bool): Whether to process subdirectories (default: True)\n",
    "        max_depth (int|None): Maximum recursion depth (None for unlimited)\n",
    "    \n",
    "    Returns:\n",
    "        int: Number of .tar.gz files processed\n",
    "    \"\"\"\n",
    "    processed_count = 0\n",
    "    current_depth = 0\n",
    "    \n",
    "    while True:\n",
    "        found_tar_gz = False\n",
    "        for root, dirs, files in os.walk(directory):\n",
    "            # Calculate current depth\n",
    "            rel_path = os.path.relpath(root, directory)\n",
    "            current_depth = rel_path.count(os.sep) + 1 if rel_path != '.' else 0\n",
    "            \n",
    "            # Skip if beyond max depth\n",
    "            if max_depth is not None and current_depth > max_depth:\n",
    "                continue\n",
    "                \n",
    "            for file in files:\n",
    "                if file.endswith('.tar.gz'):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    if extract_and_delete_tar_gz(file_path):\n",
    "                        processed_count += 1\n",
    "                        found_tar_gz = True\n",
    "        \n",
    "        # If not recursive or no more .tar.gz files found, exit\n",
    "        if not recursive or not found_tar_gz:\n",
    "            break\n",
    "    \n",
    "    return processed_count\n",
    "\n",
    "def process_paths(paths: List[str], recursive: bool = True, max_depth: Union[int, None] = None) -> int:\n",
    "    \"\"\"\n",
    "    Processes a list of paths (files or directories) to extract .tar.gz files.\n",
    "    \n",
    "    Args:\n",
    "        paths (List[str]): List of file/directory paths to process\n",
    "        recursive (bool): Whether to process directories recursively (default: True)\n",
    "        max_depth (int|None): Maximum recursion depth for directories (None for unlimited)\n",
    "    \n",
    "    Returns:\n",
    "        int: Total number of .tar.gz files processed\n",
    "    \"\"\"\n",
    "    total_processed = 0\n",
    "    \n",
    "    for path in paths:\n",
    "        if not os.path.exists(path):\n",
    "            print(f\"Warning: Path does not exist - {path}\")\n",
    "            continue\n",
    "            \n",
    "        if path.endswith('.tar.gz'):\n",
    "            if extract_and_delete_tar_gz(path):\n",
    "                total_processed += 1\n",
    "        elif os.path.isdir(path):\n",
    "            print(f\"Processing directory: {path}\")\n",
    "            total_processed += process_directory(\n",
    "                directory=path,\n",
    "                recursive=recursive,\n",
    "                max_depth=max_depth\n",
    "            )\n",
    "    \n",
    "    print(f\"Total .tar.gz files processed: {total_processed}\")\n",
    "    return total_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0a581a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "import logging\n",
    "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "def load_model(model_filepath: str, torch_dtype:torch.dtype=torch.float16):\n",
    "    \"\"\"Load a model given a specific model_path.\n",
    "\n",
    "    Args:\n",
    "        model_filepath: str - Path to where the model is stored\n",
    "\n",
    "    Returns:\n",
    "        model, dict, str - Torch model + dictionary representation of the model + model class name\n",
    "    \"\"\"\n",
    "\n",
    "    conf_filepath = os.path.join(model_filepath, 'reduced-config.json')\n",
    "    logging.info(\"Loading config file from: {}\".format(conf_filepath))\n",
    "    with open(conf_filepath, 'r') as fh:\n",
    "        round_config = json.load(fh)\n",
    "\n",
    "    logging.info(\"Loading model from filepath: {}\".format(model_filepath))\n",
    "    # https://huggingface.co/docs/transformers/installation#offline-mode\n",
    "    if round_config['use_lora']:\n",
    "        base_model_filepath = os.path.join(model_filepath, 'base-model')\n",
    "        logging.info(\"loading the base model (before LORA) from {}\".format(base_model_filepath))\n",
    "        model = AutoModelForCausalLM.from_pretrained(base_model_filepath, device_map = \"auto\", trust_remote_code=True, torch_dtype=torch_dtype, local_files_only=True)\n",
    "        # model = AutoModelForCausalLM.from_pretrained(round_config['model_architecture'], trust_remote_code=True, attn_implementation=\"flash_attention_2\", torch_dtype=torch_dtype)\n",
    "\n",
    "        fine_tuned_model_filepath = os.path.join(model_filepath, 'fine-tuned-model')\n",
    "        logging.info(\"loading the LORA adapter onto the base model from {}\".format(fine_tuned_model_filepath))\n",
    "        model.load_adapter(fine_tuned_model_filepath)\n",
    "    else:\n",
    "        fine_tuned_model_filepath = os.path.join(model_filepath, 'fine-tuned-model')\n",
    "        logging.info(\"Loading full fine tune checkpoint into cpu from {}\".format(fine_tuned_model_filepath))\n",
    "        model = AutoModelForCausalLM.from_pretrained(fine_tuned_model_filepath, device_map = \"auto\", trust_remote_code=True, torch_dtype=torch_dtype, local_files_only=True)\n",
    "        # model = AutoModelForCausalLM.from_pretrained(fine_tuned_model_filepath, trust_remote_code=True, attn_implementation=\"flash_attention_2\", torch_dtype=torch_dtype)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    tokenizer_filepath = os.path.join(model_filepath, 'tokenizer')\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_filepath)\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "949730f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, logging, torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "def _two_gpu_max_memory(headroom_gb=2):\n",
    "    \"\"\"\n",
    "    Reserve headroom so HF sharding MUST split across both 16GB T4s.\n",
    "    \"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        return None\n",
    "    n = torch.cuda.device_count()\n",
    "    cap = f\"{16 - headroom_gb}GiB\"  # e.g., \"14GiB\"\n",
    "    return {i: cap for i in range(n)}\n",
    "\n",
    "def _common_from_pretrained_kwargs():\n",
    "    \"\"\"\n",
    "    Settings that reduce both CPU and GPU peak memory and use a lean attention impl.\n",
    "    \"\"\"\n",
    "    kw = dict(\n",
    "        trust_remote_code=True,\n",
    "        local_files_only=True,\n",
    "        torch_dtype=torch.float16,     # T4 → FP16\n",
    "        low_cpu_mem_usage=True,        # streaming load\n",
    "        offload_state_dict=True,       # avoid CPU spikes\n",
    "        attn_implementation=\"sdpa\",    # available by default on Kaggle\n",
    "    )\n",
    "    mm = _two_gpu_max_memory(headroom_gb=2)\n",
    "    if mm and torch.cuda.device_count() > 1:\n",
    "        kw[\"device_map\"] = \"auto\"\n",
    "        kw[\"max_memory\"] = mm\n",
    "        # Optional if host RAM is tight:\n",
    "        # kw[\"offload_folder\"] = \"/kaggle/working/offload\"\n",
    "    else:\n",
    "        kw[\"device_map\"] = {\"\": 0}\n",
    "    return kw\n",
    "\n",
    "def load_model_and_tokenizer(model_dir: str, merge_lora: bool = True):\n",
    "    \"\"\"\n",
    "    Robust loader for full fine-tunes or LoRA adapters stored under `model_dir`.\n",
    "    Expects:\n",
    "      - reduced-config.json with {\"use_lora\": <bool>, ...}\n",
    "      - For LoRA: base-model/, fine-tuned-model/\n",
    "      - For full FT: fine-tuned-model/\n",
    "      - tokenizer/ with tokenizer files\n",
    "    Returns: (model, tokenizer)\n",
    "    \"\"\"\n",
    "    conf_path = os.path.join(model_dir, \"reduced-config.json\")\n",
    "    logging.info(f\"Loading config: {conf_path}\")\n",
    "    with open(conf_path, \"r\") as fh:\n",
    "        cfg = json.load(fh)\n",
    "\n",
    "    kw = _common_from_pretrained_kwargs()\n",
    "\n",
    "    if cfg.get(\"use_lora\", False):\n",
    "        base_dir = os.path.join(model_dir, \"base-model\")\n",
    "        lora_dir = os.path.join(model_dir, \"fine-tuned-model\")\n",
    "\n",
    "        logging.info(f\"Loading base model: {base_dir}\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(base_dir, **kw)\n",
    "        logging.info(f\"Attaching LoRA adapter: {lora_dir}\")\n",
    "        # If PeftModel is missing, use .load_adapter if available\n",
    "        try:\n",
    "            model = PeftModel.from_pretrained(model, lora_dir, is_trainable=False)  # type: ignore\n",
    "        except Exception:\n",
    "            model.load_adapter(lora_dir)\n",
    "\n",
    "    else:\n",
    "        ft_dir = os.path.join(model_dir, \"fine-tuned-model\")\n",
    "        logging.info(f\"Loading full fine-tuned model: {ft_dir}\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(ft_dir, **kw)\n",
    "\n",
    "    # Tokenizer hygiene\n",
    "    tok_dir = os.path.join(model_dir, \"tokenizer\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tok_dir, use_fast=True, local_files_only=True)\n",
    "    if tokenizer.pad_token_id is None and tokenizer.eos_token_id is not None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"left\"  # better for causal LMs with dynamic padding\n",
    "\n",
    "    # Runtime memory knobs for your gradient-based rollout\n",
    "    model.eval()\n",
    "    if hasattr(model.config, \"use_cache\"):\n",
    "        model.config.use_cache = False  # reduce KV/activation memory during your search\n",
    "\n",
    "    # Optional: quick sanity check of sharding\n",
    "    try:\n",
    "        print(getattr(model, \"hf_device_map\", \"no device map\"))\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "def download_and_load(file_id, output_filename, load_model_path):\n",
    "    \"\"\"\n",
    "    Wrapper that uses your existing helpers:\n",
    "      - clear_memory(), download_file_from_google_drive(), process_paths()\n",
    "    \"\"\"\n",
    "    clear_memory(verbose=False)\n",
    "\n",
    "    _ = download_file_from_google_drive(\n",
    "        file_id=file_id,\n",
    "        output_dir=\"/kaggle/tmp\",\n",
    "        output_filename=output_filename,\n",
    "        quiet=False\n",
    "    )\n",
    "\n",
    "    process_paths(paths=[\"/kaggle/tmp\"], recursive=True, max_depth=None)\n",
    "\n",
    "    model, tokenizer = load_model_and_tokenizer(load_model_path, merge_lora=True)\n",
    "    return model, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bcb29746",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-31 14:54:52.706824: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1761922493.133658      75 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1761922493.266238      75 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading the file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1lwC9JLRu4Z4SSQwjNtetAymStPqQeaDc\n",
      "From (redirected): https://drive.google.com/uc?id=1lwC9JLRu4Z4SSQwjNtetAymStPqQeaDc&confirm=t&uuid=da4c5611-7b11-493d-a2cf-f47a03900023\n",
      "To: /kaggle/tmp/model0.tar.gz\n",
      "100%|██████████| 10.6G/10.6G [01:24<00:00, 125MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download successful! File saved to: /kaggle/tmp/model0.tar.gz\n",
      "File size: 10092.92 MB\n",
      "Processing directory: /kaggle/tmp\n",
      "Extracting: /kaggle/tmp/model0.tar.gz\n",
      "Deleted compressed file: /kaggle/tmp/model0.tar.gz\n",
      "Total .tar.gz files processed: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "636a1583231940aa868d8c79087d8f9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 0, 'model.layers.9': 0, 'model.layers.10': 0, 'model.layers.11': 0, 'model.layers.12': 0, 'model.layers.13': 0, 'model.layers.14': 0, 'model.layers.15': 0, 'model.layers.16': 1, 'model.layers.17': 1, 'model.layers.18': 1, 'model.layers.19': 1, 'model.layers.20': 1, 'model.layers.21': 1, 'model.layers.22': 1, 'model.layers.23': 1, 'model.layers.24': 1, 'model.layers.25': 1, 'model.layers.26': 1, 'model.layers.27': 1, 'model.layers.28': 1, 'model.layers.29': 1, 'model.layers.30': 1, 'model.layers.31': 1, 'model.norm': 1, 'model.rotary_emb': 1, 'lm_head': 1}\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = download_and_load(\n",
    "    file_id=\"1lwC9JLRu4Z4SSQwjNtetAymStPqQeaDc\",\n",
    "    output_filename=\"model0.tar.gz\",\n",
    "    load_model_path=\"/kaggle/tmp/id-00000000\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97cfa3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "def entropy_loss(batch_logits):\n",
    "    log_probs = F.log_softmax(batch_logits, dim=-1)\n",
    "    probs = log_probs.exp()\n",
    "    entropy = -(probs * log_probs).sum(dim=-1)\n",
    "    return entropy.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06b2a239",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import os, torch\n",
    "\n",
    "def load_prompts(tokenizer, args):\n",
    "    # Load once; keep raw text to avoid huge pre-tokenized tensors in RAM\n",
    "    ds = load_dataset(\"tatsu-lab/alpaca\", split=\"train\", cache_dir=args[\"data_dir\"])\n",
    "\n",
    "    def collate(batch):\n",
    "        texts = [ex[\"instruction\"] for ex in batch]\n",
    "        enc = tokenizer(\n",
    "            texts,\n",
    "            padding=True,                  # dynamic padding to batch max\n",
    "            truncation=True,\n",
    "            max_length=args[\"max_length\"],\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return enc[\"input_ids\"], enc[\"attention_mask\"]\n",
    "\n",
    "    num_workers = max(2, os.cpu_count() // 2)\n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size=args[\"batch_size\"],\n",
    "        shuffle=False,\n",
    "        pin_memory=True,\n",
    "        num_workers=num_workers,\n",
    "        persistent_workers=True,\n",
    "        collate_fn=collate\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa918cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"data_dir\": \"kaggle/working/data\",\n",
    "    \"max_length\": 512,\n",
    "    \"batch_size\": 1\n",
    "}\n",
    "dataloader = load_prompts(tokenizer, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ab90069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader settings:\n",
      "batch_size: 16\n",
      "num_workers: 2\n",
      "pin_memory: True\n",
      "persistent_workers: True\n"
     ]
    }
   ],
   "source": [
    "bs   = getattr(dataloader, \"batch_size\", 1)\n",
    "nw   = getattr(dataloader, \"num_workers\", 0)\n",
    "pin  = getattr(dataloader, \"pin_memory\", \"cpu\" != \"cpu\")\n",
    "pers = getattr(dataloader, \"persistent_workers\", False)\n",
    "print(f\"DataLoader settings:\\nbatch_size: {bs}\\nnum_workers: {nw}\\npin_memory: {pin}\\npersistent_workers: {pers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3cc543d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "def find_embedding_layer(model):\n",
    "    \"\"\"\n",
    "    Attempts to find the embedding layer in an arbitrary PyTorch model.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: A PyTorch model (nn.Module) or Hugging Face model.\n",
    "    \n",
    "    Returns:\n",
    "    - embedding_layer: The nn.Embedding layer (or None if not found).\n",
    "    - path: The attribute path to the embedding layer (e.g., 'embeddings.word_embeddings').\n",
    "    \"\"\"\n",
    "    # Check if the model has a get_input_embeddings method (common in Hugging Face)\n",
    "    if hasattr(model, 'get_input_embeddings'):\n",
    "        emb = model.get_input_embeddings()\n",
    "        if isinstance(emb, nn.Embedding):\n",
    "            return emb, 'get_input_embeddings()'\n",
    "    \n",
    "    # Iterate through all named modules to find an embedding layer\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Embedding):\n",
    "            return module, name\n",
    "    \n",
    "    # If no embedding layer is found, return None\n",
    "    return None, None\n",
    "\n",
    "def freeze_except_embeddings(model, emb_layers):\n",
    "    \"\"\"\n",
    "    Freezes all model parameters except the weights of specified embedding layers.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: PyTorch model (nn.Module).\n",
    "    - emb_layers: Single nn.Embedding layer or list of nn.Embedding layers to keep unfrozen.\n",
    "    \n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # Convert single embedding layer to list for generality\n",
    "    if isinstance(emb_layers, nn.Embedding):\n",
    "        emb_layers = [emb_layers]\n",
    "    \n",
    "    # Validate that emb_layers are part of the model\n",
    "    model_params = set(model.parameters())\n",
    "    for emb_layer in emb_layers:\n",
    "        if not isinstance(emb_layer, nn.Embedding):\n",
    "            raise ValueError(f\"Expected nn.Embedding, got {type(emb_layer)}\")\n",
    "        if emb_layer.weight not in model_params:\n",
    "            raise ValueError(\"Embedding layer weight is not part of the model's parameters\")\n",
    "    \n",
    "    # Get set of embedding weights to keep unfrozen\n",
    "    emb_weights = set(emb_layer.weight for emb_layer in emb_layers)\n",
    "    \n",
    "    # Freeze parameters and clear gradients\n",
    "    for name, param in model.named_parameters():\n",
    "        if param in emb_weights:\n",
    "            param.requires_grad = True  # Ensure embedding weights are trainable\n",
    "        else:\n",
    "            param.requires_grad = False\n",
    "            param.grad = None  # Clear gradients to save memory\n",
    "    \n",
    "    # Verify embedding layers remain unfrozen\n",
    "    for emb_layer in emb_layers:\n",
    "        assert emb_layer.weight.requires_grad, f\"Embedding layer {emb_layer} was unexpectedly frozen\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea947bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda.amp import autocast\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "@torch.no_grad()\n",
    "def edit_distance(ids1: torch.Tensor,\n",
    "                  ids2: torch.Tensor) -> int:\n",
    "    \"\"\"\n",
    "    Levenshtein distance between ids1 and ids2. Works on CPU or CUDA, uses O(min(n,m)) memory.\n",
    "\n",
    "    ids1, ids2: 1D Long tensors\n",
    "    \"\"\"\n",
    "    if ids1.dim() != 1 or ids2.dim() != 1:\n",
    "        raise ValueError(\"ids1, msk1, ids2 must be 1D tensors.\")\n",
    "\n",
    "    # filter ids1 by mask\n",
    "    s = ids1\n",
    "    t = ids2\n",
    "\n",
    "    n, m = s.numel(), t.numel()\n",
    "    if n == 0: return int(m)\n",
    "    if m == 0: return int(n)\n",
    "\n",
    "    # Keep everything on the same device to avoid transfers.\n",
    "    device = s.device\n",
    "    s = s.long().to(device)\n",
    "    t = t.long().to(device)\n",
    "\n",
    "    prev = torch.arange(m + 1, device=device, dtype=torch.int32)\n",
    "    curr = torch.empty_like(prev)\n",
    "\n",
    "    for i in range(1, n + 1):\n",
    "        curr[0] = i\n",
    "        # substitution cost: 0 where equal, 1 where not\n",
    "        sub_cost = (t != s[i - 1]).to(prev.dtype)  # (m,)\n",
    "        # vectorized min over delete/insert/substitute\n",
    "        # delete:    prev[1:] + 1\n",
    "        # insert:    curr[:-1] + 1\n",
    "        # replace:   prev[:-1] + sub_cost\n",
    "        curr[1:] = torch.minimum(\n",
    "            prev[1:] + 1,\n",
    "            torch.minimum(\n",
    "                curr[:-1] + 1,\n",
    "                prev[:-1] + sub_cost\n",
    "            )\n",
    "        )\n",
    "        prev, curr = curr, prev  # swap buffers\n",
    "    return int(prev[-1].item())\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def batch_edit_distance(ids1_BxL: torch.Tensor,\n",
    "                        ids2_BxT: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute distances per sample. Iterates over batch to keep memory tiny.\n",
    "    Returns Int tensor of shape (B,)\n",
    "    \"\"\"\n",
    "    B = ids1_BxL.size(0)\n",
    "    out = []\n",
    "    for b in range(B):\n",
    "        d = edit_distance(ids1_BxL[b], ids2_BxT[b])\n",
    "        out.append(d)\n",
    "    return torch.tensor(out, dtype=torch.float32, device=ids1_BxL.device)\n",
    "\n",
    "\n",
    "def compute_loss(\n",
    "    model,\n",
    "    emb_layer,\n",
    "    embeddings,          # (B, L, E)  LEAF with requires_grad=True\n",
    "    attention_mask,      # (B, L)\n",
    "    n_tokens=10,\n",
    "    amp_dtype=torch.float16,\n",
    "    input_ids_prompt=None,   # (B, L)\n",
    "    alpha=0.5,               # strength of edit penalty as a multiplier (0..1)\n",
    "):\n",
    "    \"\"\"\n",
    "    Memory-lean objective:\n",
    "      - Roll out n_tokens with expected embeddings (cheap).\n",
    "      - Main loss = entropy on final step (as before).\n",
    "      - Compute masked edit distance between (prompt, mask) and generated span (hard argmax).\n",
    "      - Convert to similarity and scale the entropy loss by (1 + alpha * mean_sim).\n",
    "        -> edit distance influences gradients by reweighting, no backprop through DP.\n",
    "    \"\"\"\n",
    "    if input_ids_prompt is None:\n",
    "        raise ValueError(\"Pass input_ids_prompt for edit-distance scaling.\")\n",
    "\n",
    "    dev = embeddings.device\n",
    "    B   = embeddings.size(0)\n",
    "\n",
    "    def _one_step_logits(e, m):\n",
    "        with autocast(dtype=amp_dtype):\n",
    "            out = model(inputs_embeds=e, attention_mask=m)\n",
    "            return out.logits[:, -1, :]  # (B, V)\n",
    "\n",
    "    # Roll out and collect hard tokens (no grad)\n",
    "    gen_ids = []\n",
    "    for _ in range(n_tokens):\n",
    "        logits_t = _one_step_logits(embeddings, attention_mask)  # (B, V)\n",
    "        probs_t  = torch.softmax(logits_t, dim=-1)               # (B, V)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_ids = torch.argmax(probs_t, dim=-1)             # (B,)\n",
    "            gen_ids.append(next_ids)\n",
    "            exp_embed = probs_t @ emb_layer.weight.to(dev)       # (B, E)\n",
    "\n",
    "        embeddings = torch.cat([embeddings, exp_embed.unsqueeze(1)], dim=1)\n",
    "        attention_mask = torch.cat(\n",
    "            [attention_mask, torch.ones((B,1), dtype=attention_mask.dtype, device=dev)],\n",
    "            dim=1\n",
    "        )\n",
    "\n",
    "    # Final step WITH grad: main entropy loss\n",
    "    logits_last = _one_step_logits(embeddings, attention_mask)   # (B, V)\n",
    "    ent_loss   = entropy_loss(logits_last)                           # scalar\n",
    "\n",
    "    # Build (B, T) generated ids tensor (no grad)\n",
    "    with torch.no_grad():\n",
    "        gen_ids_BxT = torch.stack(gen_ids, dim=1)                # (B, T)\n",
    "\n",
    "        # Compute masked edit distance per sample\n",
    "        dists = batch_edit_distance(\n",
    "            input_ids_prompt, gen_ids_BxT\n",
    "        )                                                        # (B,)\n",
    "\n",
    "        # Normalize to [0,1] similarity per sample\n",
    "        ref_len = attention_mask[:, :input_ids_prompt.size(1)].sum(dim=1).clamp(min=1).float()\n",
    "        hyp_len = torch.tensor(gen_ids_BxT.size(1), dtype=torch.float32, device=dev).expand_as(ref_len)\n",
    "        denom = torch.maximum(ref_len, hyp_len)                  # (B,)\n",
    "        sim = 1.0 - (dists / denom)                              # (B,)\n",
    "\n",
    "        sim_mean = sim.mean()                                    # scalar (no grad)\n",
    "\n",
    "    # Distance influences gradients by scaling main loss\n",
    "    scaled_loss = ent_loss * (1.0 + alpha * sim_mean.item())\n",
    "    return scaled_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c8e3474a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def _first_device_of_embedding(model):\n",
    "    emb, _ = find_embedding_layer(model)\n",
    "    if emb is None:\n",
    "        raise RuntimeError(\"Could not find an nn.Embedding in the model.\")\n",
    "    return emb, emb.weight.device\n",
    "\n",
    "def find_best_flip(\n",
    "    model, batch, max_length_tokens, batch_id, topk, vocab_chunk\n",
    "):\n",
    "    \"\"\"\n",
    "    Memory-lean search:\n",
    "      - compute grad wrt embeddings only for the final step,\n",
    "      - process each sample independently,\n",
    "      - chunk vocab to avoid allocating (V×L) at once.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    emb_layer, first_dev = _first_device_of_embedding(model)\n",
    "\n",
    "    input_ids = batch[0].to(first_dev, non_blocking=True)      # (B, L)\n",
    "    attention_mask = batch[1].to(first_dev, non_blocking=True) # (B, L)\n",
    "\n",
    "    # Build initial embeddings (needs to be tracked)\n",
    "    freeze_except_embeddings(model, emb_layer)\n",
    "    embeddings = emb_layer(input_ids)                          # (B, L, E)\n",
    "    embeddings.retain_grad()\n",
    "\n",
    "    # Compute loss\n",
    "    loss = compute_loss(\n",
    "        model, emb_layer, embeddings, attention_mask,\n",
    "        n_tokens=max_length_tokens,\n",
    "        amp_dtype=torch.float16,\n",
    "        input_ids_prompt=input_ids,   # from the batch\n",
    "        alpha=0.5,                    # tune 0.2–0.8\n",
    "    )\n",
    "\n",
    "\n",
    "    # Backprop to get grads wrt embeddings\n",
    "    model.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "\n",
    "    grads = embeddings.grad.detach()   # (B, L, E)\n",
    "    embeds_det = embeddings.detach()   # (B, L, E)\n",
    "    del embeddings, loss\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # We'll need vocab embeddings on GPU, but in CHUNKS\n",
    "    results = []\n",
    "\n",
    "    B, L, E = grads.shape\n",
    "    V = emb_layer.weight.size(0)\n",
    "    dev = first_dev\n",
    "\n",
    "    # s_i = (g_i · v_i) per position, per sample -> (B, L)\n",
    "    s_i = (grads * embeds_det).sum(dim=2)  # (B, L)\n",
    "\n",
    "    # If you have attention_mask available in this scope, mask out padding positions now.\n",
    "    # Make sure it's on the same device as scores later.\n",
    "    # Uncomment if attention_mask exists:\n",
    "    attn_mask = attention_mask.to(dev, non_blocking=True)  # (B, L)\n",
    "    mask_b1l = (attn_mask == 0).unsqueeze(1)               # (B, 1, L)\n",
    "\n",
    "    # Running accumulators across vocab chunks\n",
    "    if topk == 1:\n",
    "        best_vals = torch.full((B,), float(\"inf\"), device=dev, dtype=grads.dtype)\n",
    "        best_flat_idx = torch.full((B,), -1, device=dev, dtype=torch.long)\n",
    "    else:\n",
    "        vals_keep = None  # (B, k_accum)\n",
    "        idx_keep  = None  # (B, k_accum)\n",
    "\n",
    "    offset = 0  # how many (vocab positions * L) we've traversed so far\n",
    "\n",
    "    for start in range(0, V, vocab_chunk):\n",
    "        end = min(start + vocab_chunk, V)\n",
    "        vocab_slice = emb_layer.weight[start:end].to(dev, non_blocking=True)  # (vchunk, E)\n",
    "        # scores = (B, vchunk, L) = (vchunk, E) @ (B, E, L)\n",
    "        # using einsum handles broadcasting cleanly\n",
    "        scores = torch.einsum(\"ve,ble->bvl\", vocab_slice, grads)  # (B, vchunk, L)\n",
    "\n",
    "        # subtract s_i across L\n",
    "        scores = scores - s_i.unsqueeze(1)  # (B, 1, L) broadcast\n",
    "\n",
    "        # Optional: mask padding positions so they are never picked\n",
    "        # if 'mask_b1l' defined above:\n",
    "        scores = scores.masked_fill(mask_b1l, float(\"inf\"))\n",
    "\n",
    "        # Flatten chunk per sample to shape (B, vchunk*L)\n",
    "        flat = scores.reshape(B, -1)\n",
    "\n",
    "        if topk == 1:\n",
    "            # best of this chunk per sample\n",
    "            chunk_vals, chunk_idx = torch.min(flat, dim=1)  # (B,)\n",
    "            # where better than current best, update\n",
    "            update = chunk_vals < best_vals\n",
    "            best_vals = torch.where(update, chunk_vals, best_vals)\n",
    "            best_flat_idx = torch.where(update, chunk_idx + offset, best_flat_idx)\n",
    "        else:\n",
    "            k_here = min(topk, flat.size(1))\n",
    "            chunk_vals, chunk_idx = torch.topk(flat, k=k_here, largest=False, dim=1)  # (B, k_here)\n",
    "            chunk_idx = chunk_idx + offset  # globalize indices\n",
    "\n",
    "            if vals_keep is None:\n",
    "                vals_keep, idx_keep = chunk_vals, chunk_idx\n",
    "            else:\n",
    "                # Merge with accumulated bests and reselect top-k\n",
    "                vals_keep = torch.cat([vals_keep, chunk_vals], dim=1)  # (B, k_accum + k_here)\n",
    "                idx_keep  = torch.cat([idx_keep,  chunk_idx],  dim=1)  # (B, k_accum + k_here)\n",
    "                # Re-topk across the concatenated candidates\n",
    "                k_sel = min(topk, vals_keep.size(1))\n",
    "                sel_vals, sel_pos = torch.topk(vals_keep, k=k_sel, largest=False, dim=1)  # (B, k_sel)\n",
    "                # Gather the matching global indices\n",
    "                batch_ids = torch.arange(B, device=dev).unsqueeze(1).expand_as(sel_pos)\n",
    "                idx_keep = idx_keep[batch_ids, sel_pos]  # (B, k_sel)\n",
    "                vals_keep = sel_vals\n",
    "\n",
    "        # free chunk temporaries\n",
    "        del vocab_slice, scores, flat\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # advance flat offset by the chunk length (vchunk * L)\n",
    "        offset += (end - start) * L\n",
    "\n",
    "    # ----- Convert batched results to your result list -----\n",
    "    if topk == 1:\n",
    "        # best_flat_idx encodes (v, pos) as: idx = v * L + pos\n",
    "        best_v   = (best_flat_idx // L).tolist()\n",
    "        best_pos = (best_flat_idx %  L).tolist()\n",
    "        best_val = best_vals.tolist()\n",
    "\n",
    "        for b in range(B):\n",
    "            results.append({\n",
    "                \"best_position\": int(best_pos[b]),\n",
    "                \"best_vocab_index\": int(best_v[b]),\n",
    "                \"min_score\": float(best_val[b]),\n",
    "                \"sample_id\": int(b),\n",
    "                \"batch_id\": int(batch_id)\n",
    "            })\n",
    "    else:\n",
    "        # idx_keep/vals_keep are (B, topk)\n",
    "        v_idx = (idx_keep // L).tolist()\n",
    "        pos_i = (idx_keep %  L).tolist()\n",
    "        vals  =  vals_keep.tolist()\n",
    "\n",
    "        for b in range(B):\n",
    "            pairs = [{\"best_position\": int(pos_i[b][j]),\n",
    "                    \"best_vocab_index\": int(v_idx[b][j]),\n",
    "                    \"min_score\": float(vals[b][j]),\n",
    "                    \"sample_id\": int(b),\n",
    "                    \"batch_id\": int(batch_id)}\n",
    "                    for j in range(len(v_idx[b]))]\n",
    "            results.extend(pairs)\n",
    "\n",
    "\n",
    "    del grads, embeds_det, input_ids, attention_mask\n",
    "    torch.cuda.empty_cache(); gc.collect()\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c06bd502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free GPU Memory: 8.45 GB\n",
      "Total GPU Memory: 14.74 GB\n"
     ]
    }
   ],
   "source": [
    "check_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d473dd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def get_top_k_by_min_score(dict_list, k):\n",
    "    return sorted(dict_list, key=lambda x: float(x[\"min_score\"]))[:k]\n",
    "\n",
    "def make_varlen_collate(tokenizer):\n",
    "    pad_id = tokenizer.pad_token_id\n",
    "    if pad_id is None:\n",
    "        # common for LLaMA-style tokenizers\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "        pad_id = tokenizer.pad_token_id\n",
    "    def _collate_varlen(samples):\n",
    "        \"\"\"\n",
    "        samples: list of tuples (ids_1D, mask_1D) with possibly varying lengths.\n",
    "        Pads within the batch using each sample's last token id as pad value; mask pads with 1.\n",
    "        \"\"\"\n",
    "        if not samples:\n",
    "            return torch.empty(0, dtype=torch.long), torch.empty(0, dtype=torch.long)\n",
    "        max_len = max(s[0].numel() for s in samples)\n",
    "        ids_out, msk_out = [], []\n",
    "        for ids, msk in samples:\n",
    "            # pad_val = int(ids[-1].item()) if ids.numel() > 0 else 0\n",
    "            if ids.numel() < max_len:\n",
    "                ids = F.pad(ids, (0, max_len - ids.numel()), value=pad_id)\n",
    "                msk = F.pad(msk, (0, max_len - msk.numel()), value=1)\n",
    "            ids_out.append(ids)\n",
    "            msk_out.append(msk)\n",
    "        return torch.stack(ids_out, dim=0), torch.stack(msk_out, dim=0)\n",
    "    return _collate_varlen\n",
    "    \n",
    "\n",
    "def apply_topk_results_to_inputs(\n",
    "    tokenizer,\n",
    "    original_dataloader,\n",
    "    top_results,\n",
    "    batch_cache,                 # REQUIRED: dict {batch_id: (ids_cpu, mask_cpu)}\n",
    "    device=\"cpu\",\n",
    "):\n",
    "    # Mirror original loader knobs\n",
    "    bs   = getattr(original_dataloader, \"batch_size\", 1)\n",
    "    nw   = getattr(original_dataloader, \"num_workers\", 0)\n",
    "    pin  = getattr(original_dataloader, \"pin_memory\", device != \"cpu\")\n",
    "    pers = getattr(original_dataloader, \"persistent_workers\", False)\n",
    "\n",
    "    # Group flips by batch for efficient lookup\n",
    "    flips_by_batch = defaultdict(list)  # batch_id -> list[(sample_id, pos, tok)]\n",
    "    for r in top_results:\n",
    "        flips_by_batch[int(r[\"batch_id\"])].append(\n",
    "            (int(r[\"sample_id\"]), int(r[\"best_position\"]), int(r[\"best_vocab_index\"]))\n",
    "        )\n",
    "    # print(f\"flips: {flips_by_batch.items()}\")\n",
    "    # print(f\"cache: {batch_cache}\")\n",
    "\n",
    "    edited_samples = []\n",
    "    for b_id, flips in flips_by_batch.items():\n",
    "        ids_cpu, msk_cpu = batch_cache[b_id]  # (B,L), (B,L)\n",
    "        # print(f\"lol, {flips}\")\n",
    "        for s, p, t in flips:\n",
    "            ids_1d = ids_cpu[s].clone()\n",
    "            msk_1d = msk_cpu[s].clone()\n",
    "            ids_1d[p] = t\n",
    "            if msk_1d[p] != 1:\n",
    "                print(f\"warning: found mask_{p} != 1. input: {tokenizer.decode(ids_1d)}\\nmask: {msk_1d}\")\n",
    "                msk_1d[p] = 1\n",
    "            edited_samples.append((ids_1d, msk_1d))\n",
    "    print(f\"edited_samples: {len(edited_samples)}\")\n",
    "\n",
    "    # Return loader over ONLY edited samples; safe even if empty\n",
    "    collate_fn = make_varlen_collate(tokenizer)\n",
    "    return DataLoader(\n",
    "        edited_samples,\n",
    "        batch_size=bs,\n",
    "        shuffle=False,\n",
    "        pin_memory=pin,\n",
    "        num_workers=nw,\n",
    "        persistent_workers=pers if nw > 0 else False,\n",
    "        collate_fn=collate_fn\n",
    "    ), edited_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "81d42259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "</s>\n"
     ]
    }
   ],
   "source": [
    "pad_id = tokenizer.pad_token_id\n",
    "if pad_id is None:\n",
    "    # common for LLaMA-style tokenizers\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    pad_id = tokenizer.pad_token_id\n",
    "print(tokenizer.decode(pad_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ec950a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, torch, json\n",
    "\n",
    "def save_round_data(edited_samples, top_results, round_id, out_dir=\"/kaggle/working/rounds\"):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    # Save tensors\n",
    "    tensor_path = os.path.join(out_dir, f\"round_{round_id:03d}_samples.pt\")\n",
    "    torch.save(edited_samples, tensor_path)\n",
    "\n",
    "    # Extract indices info (sample_id, batch_id, position, vocab_index)\n",
    "    meta = [\n",
    "        {\n",
    "            \"sample_id\": int(r[\"sample_id\"]),\n",
    "            \"batch_id\": int(r[\"batch_id\"]),\n",
    "            \"position\": int(r[\"best_position\"]),\n",
    "            \"vocab_index\": int(r[\"best_vocab_index\"]),\n",
    "            \"score\": float(r[\"min_score\"])\n",
    "        }\n",
    "        for r in top_results\n",
    "    ]\n",
    "\n",
    "    # Save metadata\n",
    "    meta_path = os.path.join(out_dir, f\"round_{round_id:03d}_meta.json\")\n",
    "    with open(meta_path, \"w\") as f:\n",
    "        json.dump(meta, f, indent=2)\n",
    "\n",
    "    print(f\"Saved round {round_id} data: {len(edited_samples)} samples → {tensor_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4906b0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_from_saved_samples(tokenizer, dataloader, round_filepath):\n",
    "    print(f\"Loading dataloader from previous round file: {round_filepath}\")\n",
    "    edited_samples = torch.load(round_filepath)\n",
    "    # Mirror original loader knobs\n",
    "    bs   = getattr(dataloader, \"batch_size\", 1)\n",
    "    nw   = getattr(dataloader, \"num_workers\", 0)\n",
    "    pin  = getattr(dataloader, \"pin_memory\", False)\n",
    "    pers = getattr(dataloader, \"persistent_workers\", False)\n",
    "    collate_fn = make_varlen_collate(tokenizer)\n",
    "    dataloader = DataLoader(\n",
    "        edited_samples,\n",
    "        batch_size=bs,\n",
    "        shuffle=False,\n",
    "        pin_memory=pin,\n",
    "        num_workers=nw,\n",
    "        persistent_workers=pers if nw > 0 else False,\n",
    "        collate_fn=collate_fn\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a32ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, gc\n",
    "\n",
    "rounds = 10\n",
    "topk_values = [26858, 14427, 7750, 4163, 2236, 1201, 645, 347, 186, 100]\n",
    "\n",
    "for r in range(rounds):\n",
    "    batch_cache = {}\n",
    "    batch_id = 0\n",
    "    results = []\n",
    "    # If /kaggle/working/hotflip/round_XXX_samples.pt exists, load that instead of the original dataloader\n",
    "    round_filepath = f\"/kaggle/working/hotflip/rounds/round_{r:03d}_samples.pt\"\n",
    "    next_round_filepath = f\"/kaggle/working/hotflip/rounds/round_{r+1:03d}_samples.pt\"\n",
    "    if os.path.exists(next_round_filepath):\n",
    "        print(f\"Next round file already exists: {next_round_filepath}, skipping round {r}\")\n",
    "        continue\n",
    "    if os.path.exists(round_filepath):\n",
    "        dataloader = load_from_saved_samples(tokenizer, dataloader, round_filepath)\n",
    "        continue\n",
    "    for batch in tqdm(dataloader, desc=f\"=== Round {r+1}/{rounds} ===\"):\n",
    "        _, first_dev = _first_device_of_embedding(model)\n",
    "        batch = (batch[0].to(first_dev, non_blocking=True),\n",
    "                batch[1].to(first_dev, non_blocking=True))\n",
    "        batch_cache[batch_id] = (batch[0].cpu(), batch[1].cpu())\n",
    "\n",
    "        results.extend(find_best_flip(\n",
    "            model=model,\n",
    "            batch=batch,\n",
    "            max_length_tokens=10,\n",
    "            batch_id=batch_id,\n",
    "            topk=5,\n",
    "            vocab_chunk=8192,         # tune: 4096–16384 depending on VRAM\n",
    "        ))\n",
    "        batch_id += 1\n",
    "\n",
    "        # free per-iteration junk\n",
    "        del batch\n",
    "        torch.cuda.empty_cache()\n",
    "        print(f\"batch {batch_id}/{len(dataloader)} done\")\n",
    "\n",
    "    # Keep only the top-k and rebuild the loader\n",
    "    print(f\"results before: {len(results)}\")\n",
    "    results = get_top_k_by_min_score(results, k=topk_values[int(r)])\n",
    "    print(f\"results after: {len(results)}\")\n",
    "    dataloader, edited_samples = apply_topk_results_to_inputs(tokenizer, dataloader, results, batch_cache)\n",
    "\n",
    "    # TODO save edited_samples in a proper file, indicating the round as well\n",
    "    save_round_data(edited_samples, results, int(r))\n",
    "\n",
    "    # housekeeping\n",
    "    torch.cuda.empty_cache(); gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
