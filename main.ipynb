{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03da4878",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "def check_memory(gpu_index: int = 0):\n",
    "    if torch.cuda.is_available():\n",
    "        free_memory = torch.cuda.get_device_properties(gpu_index).total_memory - torch.cuda.memory_allocated(gpu_index)\n",
    "        total_memory = torch.cuda.get_device_properties(gpu_index).total_memory\n",
    "        print(f\"Free GPU Memory: {free_memory / 1024**3:.2f} GB\")\n",
    "        print(f\"Total GPU Memory: {total_memory / 1024**3:.2f} GB\")\n",
    "    else:\n",
    "        print(\"CUDA is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37623a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "from typing import List, Union\n",
    "\n",
    "def clear_memory(keep_vars: Union[List[str], None] = None, verbose: bool = True):\n",
    "    \"\"\"\n",
    "    Clears memory while preserving specified variables.\n",
    "    Still clears GPU memory for all CUDA objects, including kept variables.\n",
    "    \n",
    "    Args:\n",
    "        keep_vars: List of variable names to preserve in memory (will still be cleared from GPU)\n",
    "        verbose: Whether to print memory clearing information\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(\"Starting memory clearing process...\")\n",
    "    \n",
    "    # Convert keep_vars to set for faster lookups\n",
    "    keep_set = set(keep_vars) if keep_vars else set()\n",
    "    \n",
    "    # First pass: Move kept CUDA variables to CPU\n",
    "    if torch.cuda.is_available():\n",
    "        for name, var in list(globals().items()):\n",
    "            if name in keep_set and isinstance(var, torch.Tensor) and var.is_cuda:\n",
    "                if verbose:\n",
    "                    print(f\"Moving kept tensor '{name}' to CPU\")\n",
    "                globals()[name] = var.cpu()\n",
    "    \n",
    "    # Clear Python garbage collector\n",
    "    gc.collect()\n",
    "    if verbose:\n",
    "        print(\"Ran Python garbage collection\")\n",
    "    \n",
    "    # Clear CUDA memory if available\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        if verbose:\n",
    "            print(\"Cleared CUDA cache\")\n",
    "            print(f\"Current CUDA memory allocated: {torch.cuda.memory_allocated()/1024**2:.2f} MB\")\n",
    "            print(f\"Current CUDA memory cached: {torch.cuda.memory_reserved()/1024**2:.2f} MB\")\n",
    "    \n",
    "    # Try to clear TensorFlow/Keras if available\n",
    "    try:\n",
    "        import tensorflow as tf\n",
    "        tf.keras.backend.clear_session()\n",
    "        if verbose:\n",
    "            print(\"Cleared TensorFlow/Keras session\")\n",
    "    except ImportError:\n",
    "        pass\n",
    "    \n",
    "    # Delete objects not in keep_vars\n",
    "    for name, var in list(globals().items()):\n",
    "        if not name.startswith('__') and name not in keep_set:\n",
    "            if isinstance(var, (torch.Tensor, torch.nn.Module)):\n",
    "                del globals()[name]\n",
    "                if verbose:\n",
    "                    print(f\"Deleted torch object: {name}\")\n",
    "            elif isinstance(var, list) and var and isinstance(var[0], torch.Tensor):\n",
    "                del globals()[name]\n",
    "                if verbose:\n",
    "                    print(f\"Deleted list of torch tensors: {name}\")\n",
    "    \n",
    "    # Final garbage collection\n",
    "    gc.collect()\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Memory clearing complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe7f4183",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gdown\n",
    "\n",
    "def download_file_from_google_drive(file_id, output_dir, output_filename, quiet=False):\n",
    "    \"\"\"\n",
    "    Downloads a file from Google Drive given its file ID and saves it to the specified directory.\n",
    "    \n",
    "    Args:\n",
    "        file_id (str): The Google Drive file ID (found in the file URL)\n",
    "        output_dir (str): Directory where the file should be saved\n",
    "        output_filename (str): Name of the output file\n",
    "        quiet (bool): Whether to suppress gdown output (default: False)\n",
    "    \n",
    "    Returns:\n",
    "        str: Path to the downloaded file if successful, None otherwise\n",
    "    \"\"\"\n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Full output path\n",
    "    output_file = os.path.join(output_dir, output_filename)\n",
    "    \n",
    "    print(\"Downloading the file...\")\n",
    "    try:\n",
    "        gdown.download(id=file_id, output=output_file, quiet=quiet, fuzzy=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Download failed: {str(e)}\")\n",
    "        return None\n",
    "    \n",
    "    # Verify download\n",
    "    if os.path.exists(output_file):\n",
    "        file_size = os.path.getsize(output_file) / (1024 * 1024)  # in MB\n",
    "        print(f\"Download successful! File saved to: {output_file}\")\n",
    "        print(f\"File size: {file_size:.2f} MB\")\n",
    "        return output_file\n",
    "    else:\n",
    "        print(\"Download failed - file not found\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d55255a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "from typing import List, Union\n",
    "\n",
    "def extract_and_delete_tar_gz(file_path: str, delete_compressed: bool = True) -> bool:\n",
    "    \"\"\"\n",
    "    Extracts a .tar.gz file and optionally deletes the compressed file.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the .tar.gz file\n",
    "        delete_compressed (bool): Whether to delete the compressed file after extraction (default: True)\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if extraction was successful, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Extracting: {file_path}\")\n",
    "        with tarfile.open(file_path, 'r:gz') as tar:\n",
    "            tar.extractall(path=os.path.dirname(file_path))\n",
    "        \n",
    "        if delete_compressed:\n",
    "            os.remove(file_path)\n",
    "            print(f\"Deleted compressed file: {file_path}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def process_directory(directory: str, recursive: bool = True, max_depth: Union[int, None] = None) -> int:\n",
    "    \"\"\"\n",
    "    Processes a directory to find and extract .tar.gz files.\n",
    "    \n",
    "    Args:\n",
    "        directory (str): Directory path to process\n",
    "        recursive (bool): Whether to process subdirectories (default: True)\n",
    "        max_depth (int|None): Maximum recursion depth (None for unlimited)\n",
    "    \n",
    "    Returns:\n",
    "        int: Number of .tar.gz files processed\n",
    "    \"\"\"\n",
    "    processed_count = 0\n",
    "    current_depth = 0\n",
    "    \n",
    "    while True:\n",
    "        found_tar_gz = False\n",
    "        for root, dirs, files in os.walk(directory):\n",
    "            # Calculate current depth\n",
    "            rel_path = os.path.relpath(root, directory)\n",
    "            current_depth = rel_path.count(os.sep) + 1 if rel_path != '.' else 0\n",
    "            \n",
    "            # Skip if beyond max depth\n",
    "            if max_depth is not None and current_depth > max_depth:\n",
    "                continue\n",
    "                \n",
    "            for file in files:\n",
    "                if file.endswith('.tar.gz'):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    if extract_and_delete_tar_gz(file_path):\n",
    "                        processed_count += 1\n",
    "                        found_tar_gz = True\n",
    "        \n",
    "        # If not recursive or no more .tar.gz files found, exit\n",
    "        if not recursive or not found_tar_gz:\n",
    "            break\n",
    "    \n",
    "    return processed_count\n",
    "\n",
    "def process_paths(paths: List[str], recursive: bool = True, max_depth: Union[int, None] = None) -> int:\n",
    "    \"\"\"\n",
    "    Processes a list of paths (files or directories) to extract .tar.gz files.\n",
    "    \n",
    "    Args:\n",
    "        paths (List[str]): List of file/directory paths to process\n",
    "        recursive (bool): Whether to process directories recursively (default: True)\n",
    "        max_depth (int|None): Maximum recursion depth for directories (None for unlimited)\n",
    "    \n",
    "    Returns:\n",
    "        int: Total number of .tar.gz files processed\n",
    "    \"\"\"\n",
    "    total_processed = 0\n",
    "    \n",
    "    for path in paths:\n",
    "        if not os.path.exists(path):\n",
    "            print(f\"Warning: Path does not exist - {path}\")\n",
    "            continue\n",
    "            \n",
    "        if path.endswith('.tar.gz'):\n",
    "            if extract_and_delete_tar_gz(path):\n",
    "                total_processed += 1\n",
    "        elif os.path.isdir(path):\n",
    "            print(f\"Processing directory: {path}\")\n",
    "            total_processed += process_directory(\n",
    "                directory=path,\n",
    "                recursive=recursive,\n",
    "                max_depth=max_depth\n",
    "            )\n",
    "    \n",
    "    print(f\"Total .tar.gz files processed: {total_processed}\")\n",
    "    return total_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0a581a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "import logging\n",
    "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "def load_model(model_filepath: str, torch_dtype:torch.dtype=torch.float16):\n",
    "    \"\"\"Load a model given a specific model_path.\n",
    "\n",
    "    Args:\n",
    "        model_filepath: str - Path to where the model is stored\n",
    "\n",
    "    Returns:\n",
    "        model, dict, str - Torch model + dictionary representation of the model + model class name\n",
    "    \"\"\"\n",
    "\n",
    "    conf_filepath = os.path.join(model_filepath, 'reduced-config.json')\n",
    "    logging.info(\"Loading config file from: {}\".format(conf_filepath))\n",
    "    with open(conf_filepath, 'r') as fh:\n",
    "        round_config = json.load(fh)\n",
    "\n",
    "    logging.info(\"Loading model from filepath: {}\".format(model_filepath))\n",
    "    # https://huggingface.co/docs/transformers/installation#offline-mode\n",
    "    if round_config['use_lora']:\n",
    "        base_model_filepath = os.path.join(model_filepath, 'base-model')\n",
    "        logging.info(\"loading the base model (before LORA) from {}\".format(base_model_filepath))\n",
    "        model = AutoModelForCausalLM.from_pretrained(base_model_filepath, device_map = \"auto\", trust_remote_code=True, torch_dtype=torch_dtype, local_files_only=True)\n",
    "        # model = AutoModelForCausalLM.from_pretrained(round_config['model_architecture'], trust_remote_code=True, attn_implementation=\"flash_attention_2\", torch_dtype=torch_dtype)\n",
    "\n",
    "        fine_tuned_model_filepath = os.path.join(model_filepath, 'fine-tuned-model')\n",
    "        logging.info(\"loading the LORA adapter onto the base model from {}\".format(fine_tuned_model_filepath))\n",
    "        model.load_adapter(fine_tuned_model_filepath)\n",
    "    else:\n",
    "        fine_tuned_model_filepath = os.path.join(model_filepath, 'fine-tuned-model')\n",
    "        logging.info(\"Loading full fine tune checkpoint into cpu from {}\".format(fine_tuned_model_filepath))\n",
    "        model = AutoModelForCausalLM.from_pretrained(fine_tuned_model_filepath, device_map = \"auto\", trust_remote_code=True, torch_dtype=torch_dtype, local_files_only=True)\n",
    "        # model = AutoModelForCausalLM.from_pretrained(fine_tuned_model_filepath, trust_remote_code=True, attn_implementation=\"flash_attention_2\", torch_dtype=torch_dtype)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    tokenizer_filepath = os.path.join(model_filepath, 'tokenizer')\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_filepath)\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "949730f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting memory clearing process...\n",
      "Ran Python garbage collection\n",
      "Cleared CUDA cache\n",
      "Current CUDA memory allocated: 0.00 MB\n",
      "Current CUDA memory cached: 0.00 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-21 15:38:47.244529: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1761061127.276762     112 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1761061127.285925     112 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared TensorFlow/Keras session\n",
      "Memory clearing complete\n",
      "Downloading the file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1lwC9JLRu4Z4SSQwjNtetAymStPqQeaDc\n",
      "From (redirected): https://drive.google.com/uc?id=1lwC9JLRu4Z4SSQwjNtetAymStPqQeaDc&confirm=t&uuid=b3a2a1ab-3889-4e2e-a2c9-963a26aaa88f\n",
      "To: /kaggle/tmp/model0.tar.gz\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10.6G/10.6G [00:55<00:00, 192MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download successful! File saved to: /kaggle/tmp/model0.tar.gz\n",
      "File size: 10092.92 MB\n",
      "Processing directory: /kaggle/tmp\n",
      "Extracting: /kaggle/tmp/model0.tar.gz\n",
      "Deleted compressed file: /kaggle/tmp/model0.tar.gz\n",
      "Total .tar.gz files processed: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "389df54d868f4f0ab6195decb0146c52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def download_and_load(file_id, output_filename, load_model_path):\n",
    "    \"\"\"Run the complete embedding optimization pipeline with custom weight function\"\"\"\n",
    "    # Clear memory before starting\n",
    "    clear_memory()\n",
    "    \n",
    "    # Download the model\n",
    "    downloaded_file = download_file_from_google_drive(\n",
    "        file_id=file_id,#\"1-K-HcT-3-00rxPpvQxZ75o2be3STchsv\",\n",
    "        output_dir=\"/kaggle/tmp\",\n",
    "        output_filename=output_filename,#\"model4.tar.gz\",\n",
    "        quiet=False\n",
    "    )\n",
    "    \n",
    "    # Process\n",
    "    process_paths(\n",
    "        paths=['/kaggle/tmp',],\n",
    "        recursive=True,\n",
    "        max_depth=None\n",
    "    )\n",
    "    \n",
    "    # Load model and tokenizer\n",
    "    model, tokenizer = load_model(load_model_path)#\"/kaggle/tmp/id-00000004\")\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "model, tokenizer = download_and_load(file_id=\"1lwC9JLRu4Z4SSQwjNtetAymStPqQeaDc\", output_filename=\"model0.tar.gz\", load_model_path=\"/kaggle/tmp/id-00000000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97cfa3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "def entropy_loss(batch_logits):\n",
    "    \"\"\"\n",
    "    Memory-efficient entropy loss without in-place operations that can break computation graphs\n",
    "    \"\"\"\n",
    "    log_probs = F.log_softmax(batch_logits, dim=-1)\n",
    "\n",
    "    probs = log_probs.exp()\n",
    "\n",
    "    entropy = -(probs * log_probs).sum(dim=-1)\n",
    "\n",
    "    return entropy.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06b2a239",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "def load_prompts(tokenizer, args):\n",
    "    # Load Alpaca dataset\n",
    "    dataset = load_dataset(\"tatsu-lab/alpaca\", split=\"train\", cache_dir=args[\"data_dir\"])\n",
    "\n",
    "    # Tokenization function\n",
    "    def tokenize_function(example):\n",
    "        encodings = tokenizer(\n",
    "            example[\"instruction\"],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=args[\"max_length\"],\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": encodings[\"input_ids\"],\n",
    "            \"attention_mask\": encodings[\"attention_mask\"]\n",
    "        }\n",
    "\n",
    "    # Tokenize all examples\n",
    "    tokenized_dataset = dataset.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=dataset.column_names,\n",
    "        batch_size=1000\n",
    "    )\n",
    "\n",
    "    # Convert to torch tensors\n",
    "    input_ids = torch.tensor(tokenized_dataset[\"input_ids\"])\n",
    "    attention_mask = torch.tensor(tokenized_dataset[\"attention_mask\"])\n",
    "\n",
    "    del dataset, tokenized_dataset\n",
    "    gc.collect()\n",
    "\n",
    "    # Create PyTorch Dataset\n",
    "    torch_dataset = TensorDataset(input_ids, attention_mask)\n",
    "\n",
    "    # Wrap in DataLoader\n",
    "    return DataLoader(\n",
    "        torch_dataset,\n",
    "        batch_size=args[\"batch_size\"],\n",
    "        shuffle=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aaa918cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"data_dir\": \"kaggle/working/data\",\n",
    "    \"max_length\": 512,\n",
    "    \"batch_size\": 1\n",
    "}\n",
    "dataloader = load_prompts(tokenizer, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cc543d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "def find_embedding_layer(model):\n",
    "    \"\"\"\n",
    "    Attempts to find the embedding layer in an arbitrary PyTorch model.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: A PyTorch model (nn.Module) or Hugging Face model.\n",
    "    \n",
    "    Returns:\n",
    "    - embedding_layer: The nn.Embedding layer (or None if not found).\n",
    "    - path: The attribute path to the embedding layer (e.g., 'embeddings.word_embeddings').\n",
    "    \"\"\"\n",
    "    # Check if the model has a get_input_embeddings method (common in Hugging Face)\n",
    "    if hasattr(model, 'get_input_embeddings'):\n",
    "        emb = model.get_input_embeddings()\n",
    "        if isinstance(emb, nn.Embedding):\n",
    "            return emb, 'get_input_embeddings()'\n",
    "    \n",
    "    # Iterate through all named modules to find an embedding layer\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Embedding):\n",
    "            return module, name\n",
    "    \n",
    "    # If no embedding layer is found, return None\n",
    "    return None, None\n",
    "\n",
    "def freeze_except_embeddings(model, emb_layers):\n",
    "    \"\"\"\n",
    "    Freezes all model parameters except the weights of specified embedding layers.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: PyTorch model (nn.Module).\n",
    "    - emb_layers: Single nn.Embedding layer or list of nn.Embedding layers to keep unfrozen.\n",
    "    \n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # Convert single embedding layer to list for generality\n",
    "    if isinstance(emb_layers, nn.Embedding):\n",
    "        emb_layers = [emb_layers]\n",
    "    \n",
    "    # Validate that emb_layers are part of the model\n",
    "    model_params = set(model.parameters())\n",
    "    for emb_layer in emb_layers:\n",
    "        if not isinstance(emb_layer, nn.Embedding):\n",
    "            raise ValueError(f\"Expected nn.Embedding, got {type(emb_layer)}\")\n",
    "        if emb_layer.weight not in model_params:\n",
    "            raise ValueError(\"Embedding layer weight is not part of the model's parameters\")\n",
    "    \n",
    "    # Get set of embedding weights to keep unfrozen\n",
    "    emb_weights = set(emb_layer.weight for emb_layer in emb_layers)\n",
    "    \n",
    "    # Freeze parameters and clear gradients\n",
    "    for name, param in model.named_parameters():\n",
    "        if param in emb_weights:\n",
    "            param.requires_grad = True  # Ensure embedding weights are trainable\n",
    "        else:\n",
    "            param.requires_grad = False\n",
    "            param.grad = None  # Clear gradients to save memory\n",
    "    \n",
    "    # Verify embedding layers remain unfrozen\n",
    "    for emb_layer in emb_layers:\n",
    "        assert emb_layer.weight.requires_grad, f\"Embedding layer {emb_layer} was unexpectedly frozen\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed712f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_to_word(token_id, tokenizer):\n",
    "    \"\"\"Convert a token ID to its corresponding word using the tokenizer.\"\"\"\n",
    "    return tokenizer.decode([token_id]).strip()\n",
    "\n",
    "def word_to_token(word, tokenizer):\n",
    "    \"\"\"Convert a word to its corresponding token ID using the tokenizer.\"\"\"\n",
    "    return tokenizer.encode(word, add_special_tokens=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ea947bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(model, input_ids, attention_mask, loss_fn, n_tokens=10):\n",
    "    \"\"\"\n",
    "    Autoregressive generation with gradient tracking only on the final step.\n",
    "    Returns total loss over n future tokens.\n",
    "    \"\"\"\n",
    "    total_loss = 0.0\n",
    "    input_clone = input_ids.detach()\n",
    "    attention_clone = attention_mask.detach()\n",
    "    # Only track gradients for this step; no need for previous steps' graphs\n",
    "    input_ids = input_ids.detach()\n",
    "    attention_mask = attention_mask.detach()\n",
    "    \n",
    "    for _ in range(n_tokens):\n",
    "        print(f\"Processing {_} step\")\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits[:, -1, :]  # (B, V)\n",
    "\n",
    "        # Compute step loss\n",
    "        step_loss = loss_fn(logits)\n",
    "        total_loss = total_loss + step_loss\n",
    "\n",
    "        # Generate next token without gradient tracking\n",
    "        with torch.no_grad():\n",
    "            next_token = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "            input_ids = torch.cat([input_ids, next_token], dim=1)\n",
    "            attention_mask = torch.cat(\n",
    "                [attention_mask, torch.ones_like(next_token)], dim=1\n",
    "            )\n",
    "\n",
    "        # Free unused tensors\n",
    "        del outputs, logits, step_loss, next_token\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    input_ids = input_clone\n",
    "    attention_mask = attention_clone\n",
    "    return total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c8e3474a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def find_best_flip(model, batch, loss_fn, max_length_tokens, batch_id, topk=1):\n",
    "    \"\"\"\n",
    "    For each example in the batch, find the (position i, replacement vocab index v_hat)\n",
    "    that minimize g_i Â· (v_hat - v_i)  where g_i = dJ/dx_i, v_i is current embedding.\n",
    "\n",
    "    Returns a list (one per batch element) of dicts with best positions / vocab indices / scores.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    emb_layer, _ = find_embedding_layer(model)\n",
    "    print(\"found embedding layer\")\n",
    "\n",
    "    freeze_except_embeddings(model, emb_layer)\n",
    "    print(\"froze all but embedding layer\")\n",
    "\n",
    "\n",
    "    input_ids = batch[0].to(device)       # (B, L)\n",
    "    attention_mask = batch[1].to(device)   # (B, L)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        vocab_embeds = emb_layer.weight.detach()                # (V, E)\n",
    "\n",
    "\n",
    "    # 1) Get embeddings for inputs and ensure they retain grad\n",
    "    embeddings = emb_layer(input_ids)                     # (B, L, E)\n",
    "    embeddings.retain_grad()\n",
    "\n",
    "    # 2) Compute loss\n",
    "    print(\"gonna compute loss\")\n",
    "    loss = compute_loss(model, input_ids, attention_mask, loss_fn, max_length_tokens)\n",
    "    # 3) Backprop to get grads wrt embeddings\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    grads = embeddings.grad.detach()   # (B, L, E)\n",
    "    embeds_det = embeddings.detach()   # (B, L, E)\n",
    "\n",
    "    results = []\n",
    "    B, L, _ = grads.shape\n",
    "\n",
    "    # Process one sample at a time\n",
    "    for b in tqdm(range(B), desc=\"Searching through batch samples\"):\n",
    "        grads_b = grads[b]         # (L, E)\n",
    "        embeds_b = embeds_det[b]   # (L, E)\n",
    "\n",
    "        # Compute per-position scalar s_i = g_i Â· v_i  (shape (L,))\n",
    "        # element-wise dot across embedding dim\n",
    "        s_i = (grads_b * embeds_b).sum(dim=1)   # (L,)\n",
    "\n",
    "        # Compute dot between vocab_embeds (V, E) and grads_b (L, E) -> (V, L)\n",
    "        # We want scores_{v,i} = vocab_embeds @ grads_b[i] - s_i[i]\n",
    "        # Efficient way: (V, E) @ (E, L) = (V, L)\n",
    "        # grads_b.T is (E, L)\n",
    "        scores_VxL = torch.matmul(vocab_embeds, grads_b.T)   # (V, L)\n",
    "\n",
    "        # subtract s_i from each column i\n",
    "        scores_VxL = scores_VxL - s_i.unsqueeze(0)          # broadcast (V, L)\n",
    "\n",
    "        # Now find minimal value and its indices\n",
    "        # Flatten to search global min across V*L\n",
    "        flat = scores_VxL.view(-1)              # (V*L,)\n",
    "        if topk == 1:\n",
    "            min_val, min_idx = torch.min(flat, dim=0)\n",
    "            v_idx = (min_idx // L).item()       # vocab index\n",
    "            pos_i = (min_idx % L).item()        # position index\n",
    "            results.append({\n",
    "                \"best_position\": pos_i,\n",
    "                \"best_vocab_index\": v_idx,\n",
    "                \"min_score\": min_val.item(),\n",
    "                \"sample_id\": b,\n",
    "                \"batch_id\": batch_id\n",
    "            })\n",
    "        else:\n",
    "            # optionally return top-k pairs\n",
    "            vals, idxs = torch.topk(flat, k=topk, largest=False)\n",
    "            pairs = []\n",
    "            for val, idx in zip(vals.tolist(), idxs.tolist()):\n",
    "                v_idx = idx // L\n",
    "                pos_i = idx % L\n",
    "                pairs.append({\"position\": int(pos_i), \"vocab_index\": int(v_idx), \"score\": float(val)})\n",
    "            results.append({\"topk\": pairs})\n",
    "        del grads_b, embeds_b, s_i, scores_VxL, flat\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "     # ðŸ”¹ Free remaining references\n",
    "    del grads, embeds_det, embeddings, vocab_embeds, loss, input_ids, attention_mask\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c06bd502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free GPU Memory: 14.74 GB\n",
      "Total GPU Memory: 14.74 GB\n"
     ]
    }
   ],
   "source": [
    "check_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d473dd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "def get_top_k_by_min_score(dict_list, k):\n",
    "    # Sort the list of dictionaries by min_score in ascending order\n",
    "    sorted_list = sorted(dict_list, key=lambda x: x['min_score'])\n",
    "    # Return the first k elements\n",
    "    return sorted_list[:k]\n",
    "\n",
    "\n",
    "\n",
    "def apply_topk_results_to_inputs(original_dataloader, top_results, device=\"cpu\"):\n",
    "    \n",
    "    \"\"\"\n",
    "    Apply flips from top_results to the dataset behind a DataLoader.\n",
    "    Works safely on multi-GPU (device_map=\"auto\") setups.\n",
    "    \"\"\"\n",
    "\n",
    "    all_input_ids = []\n",
    "    all_attention_masks = []\n",
    "\n",
    "    # Collect everything on CPU (safe and memory-stable)\n",
    "    for batch in original_dataloader:\n",
    "        input_ids, attention_mask = batch\n",
    "        all_input_ids.append(input_ids.cpu())\n",
    "        all_attention_masks.append(attention_mask.cpu())\n",
    "\n",
    "    # Combine into full tensors\n",
    "    all_input_ids = torch.cat(all_input_ids, dim=0)\n",
    "    all_attention_masks = torch.cat(all_attention_masks, dim=0)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Clone for modified copy\n",
    "    new_inputs = all_input_ids.clone()\n",
    "\n",
    "    # Apply token flips (CPU-side, no GPU load)\n",
    "    for res in top_results:\n",
    "        sample_idx = res[\"sample_id\"]\n",
    "        pos_i = res[\"best_position\"]\n",
    "        new_token = res[\"best_vocab_index\"]\n",
    "        new_inputs[sample_idx, pos_i] = new_token\n",
    "\n",
    "    # Combine datasets\n",
    "    combined_inputs = torch.cat([all_input_ids, new_inputs], dim=0)\n",
    "    combined_masks = torch.cat([all_attention_masks, all_attention_masks], dim=0)\n",
    "\n",
    "    # Cleanup intermediate large tensors to free memory\n",
    "    del all_input_ids, all_attention_masks, new_inputs\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Build dataset and DataLoader\n",
    "    new_dataset = TensorDataset(combined_inputs, combined_masks)\n",
    "    new_dataloader = DataLoader(\n",
    "        new_dataset,\n",
    "        batch_size=original_dataloader.batch_size,\n",
    "        shuffle=True,\n",
    "        pin_memory=(device != \"cpu\"),\n",
    "    )\n",
    "\n",
    "    return new_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b8a32ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Round 1/10 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|          | 0/52002 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found embedding layer\n",
      "froze all but embedding layer\n",
      "gonna compute loss\n",
      "Processing 0 step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|          | 0/52002 [00:17<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_112/1741072132.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         results.extend(find_best_flip(\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mbatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_112/1103863088.py\u001b[0m in \u001b[0;36mfind_best_flip\u001b[0;34m(model, batch, loss_fn, max_length_tokens, batch_id, topk)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;31m# 2) Compute loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"gonna compute loss\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0;31m# 3) Backprop to get grads wrt embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_112/3882574526.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(model, input_ids, attention_mask, loss_fn, n_tokens)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Processing {_} step\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# (B, V)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    941\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 943\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    944\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_requested_to_return_tuple\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_configured_to_return_tuple\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_top_level_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m         outputs: BaseModelOutputWithPast = self.model(\n\u001b[0m\u001b[1;32m    554\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    941\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 943\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    944\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_requested_to_return_tuple\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_configured_to_return_tuple\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_top_level_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    439\u001b[0m                 \u001b[0mall_hidden_states\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m             layer_outputs = decoder_layer(\n\u001b[0m\u001b[1;32m    442\u001b[0m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcausal_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_layers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gradient_checkpointing_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0;31m# Self Attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 290\u001b[0;31m         hidden_states, self_attn_weights = self.self_attn(\n\u001b[0m\u001b[1;32m    291\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0mhidden_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0mquery_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m         \u001b[0mkey_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0mvalue_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/layer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    753\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 755\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    756\u001b[0m             \u001b[0mtorch_result_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results = []\n",
    "batch_id = 0\n",
    "rounds = 10\n",
    "for round in range(rounds):\n",
    "    print(f\"=== Round {round+1}/{rounds} ===\")\n",
    "    for batch in tqdm(dataloader, desc=\"Processing batches\"):\n",
    "        input_ids, attention_mask = batch\n",
    "        input_ids = input_ids.to(model.device)\n",
    "        attention_mask = attention_mask.to(model.device)\n",
    "\n",
    "        results.extend(find_best_flip(\n",
    "            model=model,\n",
    "            batch=batch,\n",
    "            loss_fn=entropy_loss,\n",
    "            max_length_tokens=1,\n",
    "            batch_id=batch_id,\n",
    "            topk=1\n",
    "        ))\n",
    "        batch_id += 1\n",
    "    # === Use top-k results to update dataloader ===\n",
    "    results = get_top_k_by_min_score(results, k=500)\n",
    "    dataloader = apply_topk_results_to_inputs(dataloader, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6cccdbfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting memory clearing process...\n",
      "Ran Python garbage collection\n",
      "Cleared CUDA cache\n",
      "Current CUDA memory allocated: 14186.88 MB\n",
      "Current CUDA memory cached: 14928.00 MB\n",
      "Cleared TensorFlow/Keras session\n",
      "Memory clearing complete\n"
     ]
    }
   ],
   "source": [
    "clear_memory(keep_vars=['model', 'tokenizer'], verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
