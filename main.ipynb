{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03da4878",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "def check_memory(gpu_index: int = 0):\n",
    "    if torch.cuda.is_available():\n",
    "        free_memory = torch.cuda.get_device_properties(gpu_index).total_memory - torch.cuda.memory_allocated(gpu_index)\n",
    "        total_memory = torch.cuda.get_device_properties(gpu_index).total_memory\n",
    "        print(f\"Free GPU Memory: {free_memory / 1024**3:.2f} GB\")\n",
    "        print(f\"Total GPU Memory: {total_memory / 1024**3:.2f} GB\")\n",
    "    else:\n",
    "        print(\"CUDA is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37623a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "from typing import List, Union\n",
    "\n",
    "def clear_memory(keep_vars: Union[List[str], None] = None, verbose: bool = True):\n",
    "    \"\"\"\n",
    "    Clears memory while preserving specified variables.\n",
    "    Still clears GPU memory for all CUDA objects, including kept variables.\n",
    "    \n",
    "    Args:\n",
    "        keep_vars: List of variable names to preserve in memory (will still be cleared from GPU)\n",
    "        verbose: Whether to print memory clearing information\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(\"Starting memory clearing process...\")\n",
    "    \n",
    "    # Convert keep_vars to set for faster lookups\n",
    "    keep_set = set(keep_vars) if keep_vars else set()\n",
    "    \n",
    "    # First pass: Move kept CUDA variables to CPU\n",
    "    if torch.cuda.is_available():\n",
    "        for name, var in list(globals().items()):\n",
    "            if name in keep_set and isinstance(var, torch.Tensor) and var.is_cuda:\n",
    "                if verbose:\n",
    "                    print(f\"Moving kept tensor '{name}' to CPU\")\n",
    "                globals()[name] = var.cpu()\n",
    "    \n",
    "    # Clear Python garbage collector\n",
    "    gc.collect()\n",
    "    if verbose:\n",
    "        print(\"Ran Python garbage collection\")\n",
    "    \n",
    "    # Clear CUDA memory if available\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        if verbose:\n",
    "            print(\"Cleared CUDA cache\")\n",
    "            print(f\"Current CUDA memory allocated: {torch.cuda.memory_allocated()/1024**2:.2f} MB\")\n",
    "            print(f\"Current CUDA memory cached: {torch.cuda.memory_reserved()/1024**2:.2f} MB\")\n",
    "    \n",
    "    # Try to clear TensorFlow/Keras if available\n",
    "    try:\n",
    "        import tensorflow as tf\n",
    "        tf.keras.backend.clear_session()\n",
    "        if verbose:\n",
    "            print(\"Cleared TensorFlow/Keras session\")\n",
    "    except ImportError:\n",
    "        pass\n",
    "    \n",
    "    # Delete objects not in keep_vars\n",
    "    for name, var in list(globals().items()):\n",
    "        if not name.startswith('__') and name not in keep_set:\n",
    "            if isinstance(var, (torch.Tensor, torch.nn.Module)):\n",
    "                del globals()[name]\n",
    "                if verbose:\n",
    "                    print(f\"Deleted torch object: {name}\")\n",
    "            elif isinstance(var, list) and var and isinstance(var[0], torch.Tensor):\n",
    "                del globals()[name]\n",
    "                if verbose:\n",
    "                    print(f\"Deleted list of torch tensors: {name}\")\n",
    "    \n",
    "    # Final garbage collection\n",
    "    gc.collect()\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Memory clearing complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe7f4183",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gdown\n",
    "\n",
    "def download_file_from_google_drive(file_id, output_dir, output_filename, quiet=False):\n",
    "    \"\"\"\n",
    "    Downloads a file from Google Drive given its file ID and saves it to the specified directory.\n",
    "    \n",
    "    Args:\n",
    "        file_id (str): The Google Drive file ID (found in the file URL)\n",
    "        output_dir (str): Directory where the file should be saved\n",
    "        output_filename (str): Name of the output file\n",
    "        quiet (bool): Whether to suppress gdown output (default: False)\n",
    "    \n",
    "    Returns:\n",
    "        str: Path to the downloaded file if successful, None otherwise\n",
    "    \"\"\"\n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Full output path\n",
    "    output_file = os.path.join(output_dir, output_filename)\n",
    "    \n",
    "    print(\"Downloading the file...\")\n",
    "    try:\n",
    "        gdown.download(id=file_id, output=output_file, quiet=quiet, fuzzy=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Download failed: {str(e)}\")\n",
    "        return None\n",
    "    \n",
    "    # Verify download\n",
    "    if os.path.exists(output_file):\n",
    "        file_size = os.path.getsize(output_file) / (1024 * 1024)  # in MB\n",
    "        print(f\"Download successful! File saved to: {output_file}\")\n",
    "        print(f\"File size: {file_size:.2f} MB\")\n",
    "        return output_file\n",
    "    else:\n",
    "        print(\"Download failed - file not found\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d55255a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "from typing import List, Union\n",
    "\n",
    "def extract_and_delete_tar_gz(file_path: str, delete_compressed: bool = True) -> bool:\n",
    "    \"\"\"\n",
    "    Extracts a .tar.gz file and optionally deletes the compressed file.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the .tar.gz file\n",
    "        delete_compressed (bool): Whether to delete the compressed file after extraction (default: True)\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if extraction was successful, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Extracting: {file_path}\")\n",
    "        with tarfile.open(file_path, 'r:gz') as tar:\n",
    "            tar.extractall(path=os.path.dirname(file_path))\n",
    "        \n",
    "        if delete_compressed:\n",
    "            os.remove(file_path)\n",
    "            print(f\"Deleted compressed file: {file_path}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def process_directory(directory: str, recursive: bool = True, max_depth: Union[int, None] = None) -> int:\n",
    "    \"\"\"\n",
    "    Processes a directory to find and extract .tar.gz files.\n",
    "    \n",
    "    Args:\n",
    "        directory (str): Directory path to process\n",
    "        recursive (bool): Whether to process subdirectories (default: True)\n",
    "        max_depth (int|None): Maximum recursion depth (None for unlimited)\n",
    "    \n",
    "    Returns:\n",
    "        int: Number of .tar.gz files processed\n",
    "    \"\"\"\n",
    "    processed_count = 0\n",
    "    current_depth = 0\n",
    "    \n",
    "    while True:\n",
    "        found_tar_gz = False\n",
    "        for root, dirs, files in os.walk(directory):\n",
    "            # Calculate current depth\n",
    "            rel_path = os.path.relpath(root, directory)\n",
    "            current_depth = rel_path.count(os.sep) + 1 if rel_path != '.' else 0\n",
    "            \n",
    "            # Skip if beyond max depth\n",
    "            if max_depth is not None and current_depth > max_depth:\n",
    "                continue\n",
    "                \n",
    "            for file in files:\n",
    "                if file.endswith('.tar.gz'):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    if extract_and_delete_tar_gz(file_path):\n",
    "                        processed_count += 1\n",
    "                        found_tar_gz = True\n",
    "        \n",
    "        # If not recursive or no more .tar.gz files found, exit\n",
    "        if not recursive or not found_tar_gz:\n",
    "            break\n",
    "    \n",
    "    return processed_count\n",
    "\n",
    "def process_paths(paths: List[str], recursive: bool = True, max_depth: Union[int, None] = None) -> int:\n",
    "    \"\"\"\n",
    "    Processes a list of paths (files or directories) to extract .tar.gz files.\n",
    "    \n",
    "    Args:\n",
    "        paths (List[str]): List of file/directory paths to process\n",
    "        recursive (bool): Whether to process directories recursively (default: True)\n",
    "        max_depth (int|None): Maximum recursion depth for directories (None for unlimited)\n",
    "    \n",
    "    Returns:\n",
    "        int: Total number of .tar.gz files processed\n",
    "    \"\"\"\n",
    "    total_processed = 0\n",
    "    \n",
    "    for path in paths:\n",
    "        if not os.path.exists(path):\n",
    "            print(f\"Warning: Path does not exist - {path}\")\n",
    "            continue\n",
    "            \n",
    "        if path.endswith('.tar.gz'):\n",
    "            if extract_and_delete_tar_gz(path):\n",
    "                total_processed += 1\n",
    "        elif os.path.isdir(path):\n",
    "            print(f\"Processing directory: {path}\")\n",
    "            total_processed += process_directory(\n",
    "                directory=path,\n",
    "                recursive=recursive,\n",
    "                max_depth=max_depth\n",
    "            )\n",
    "    \n",
    "    print(f\"Total .tar.gz files processed: {total_processed}\")\n",
    "    return total_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0a581a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "import logging\n",
    "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "def load_model(model_filepath: str, torch_dtype:torch.dtype=torch.float16):\n",
    "    \"\"\"Load a model given a specific model_path.\n",
    "\n",
    "    Args:\n",
    "        model_filepath: str - Path to where the model is stored\n",
    "\n",
    "    Returns:\n",
    "        model, dict, str - Torch model + dictionary representation of the model + model class name\n",
    "    \"\"\"\n",
    "\n",
    "    conf_filepath = os.path.join(model_filepath, 'reduced-config.json')\n",
    "    logging.info(\"Loading config file from: {}\".format(conf_filepath))\n",
    "    with open(conf_filepath, 'r') as fh:\n",
    "        round_config = json.load(fh)\n",
    "\n",
    "    logging.info(\"Loading model from filepath: {}\".format(model_filepath))\n",
    "    # https://huggingface.co/docs/transformers/installation#offline-mode\n",
    "    if round_config['use_lora']:\n",
    "        base_model_filepath = os.path.join(model_filepath, 'base-model')\n",
    "        logging.info(\"loading the base model (before LORA) from {}\".format(base_model_filepath))\n",
    "        model = AutoModelForCausalLM.from_pretrained(base_model_filepath, device_map = \"auto\", trust_remote_code=True, torch_dtype=torch_dtype, local_files_only=True)\n",
    "        # model = AutoModelForCausalLM.from_pretrained(round_config['model_architecture'], trust_remote_code=True, attn_implementation=\"flash_attention_2\", torch_dtype=torch_dtype)\n",
    "\n",
    "        fine_tuned_model_filepath = os.path.join(model_filepath, 'fine-tuned-model')\n",
    "        logging.info(\"loading the LORA adapter onto the base model from {}\".format(fine_tuned_model_filepath))\n",
    "        model.load_adapter(fine_tuned_model_filepath)\n",
    "    else:\n",
    "        fine_tuned_model_filepath = os.path.join(model_filepath, 'fine-tuned-model')\n",
    "        logging.info(\"Loading full fine tune checkpoint into cpu from {}\".format(fine_tuned_model_filepath))\n",
    "        model = AutoModelForCausalLM.from_pretrained(fine_tuned_model_filepath, device_map = \"auto\", trust_remote_code=True, torch_dtype=torch_dtype, local_files_only=True)\n",
    "        # model = AutoModelForCausalLM.from_pretrained(fine_tuned_model_filepath, trust_remote_code=True, attn_implementation=\"flash_attention_2\", torch_dtype=torch_dtype)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    tokenizer_filepath = os.path.join(model_filepath, 'tokenizer')\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_filepath)\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "949730f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, logging, torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "def _two_gpu_max_memory(headroom_gb=2):\n",
    "    \"\"\"\n",
    "    Reserve headroom so HF sharding MUST split across both 16GB T4s.\n",
    "    \"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        return None\n",
    "    n = torch.cuda.device_count()\n",
    "    cap = f\"{16 - headroom_gb}GiB\"  # e.g., \"14GiB\"\n",
    "    return {i: cap for i in range(n)}\n",
    "\n",
    "def _common_from_pretrained_kwargs():\n",
    "    \"\"\"\n",
    "    Settings that reduce both CPU and GPU peak memory and use a lean attention impl.\n",
    "    \"\"\"\n",
    "    kw = dict(\n",
    "        trust_remote_code=True,\n",
    "        local_files_only=True,\n",
    "        torch_dtype=torch.float16,     # T4 → FP16\n",
    "        low_cpu_mem_usage=True,        # streaming load\n",
    "        offload_state_dict=True,       # avoid CPU spikes\n",
    "        attn_implementation=\"sdpa\",    # available by default on Kaggle\n",
    "    )\n",
    "    mm = _two_gpu_max_memory(headroom_gb=2)\n",
    "    if mm and torch.cuda.device_count() > 1:\n",
    "        kw[\"device_map\"] = \"auto\"\n",
    "        kw[\"max_memory\"] = mm\n",
    "        # Optional if host RAM is tight:\n",
    "        # kw[\"offload_folder\"] = \"/kaggle/working/offload\"\n",
    "    else:\n",
    "        kw[\"device_map\"] = {\"\": 0}\n",
    "    return kw\n",
    "\n",
    "def load_model_and_tokenizer(model_dir: str, merge_lora: bool = True):\n",
    "    \"\"\"\n",
    "    Robust loader for full fine-tunes or LoRA adapters stored under `model_dir`.\n",
    "    Expects:\n",
    "      - reduced-config.json with {\"use_lora\": <bool>, ...}\n",
    "      - For LoRA: base-model/, fine-tuned-model/\n",
    "      - For full FT: fine-tuned-model/\n",
    "      - tokenizer/ with tokenizer files\n",
    "    Returns: (model, tokenizer)\n",
    "    \"\"\"\n",
    "    conf_path = os.path.join(model_dir, \"reduced-config.json\")\n",
    "    logging.info(f\"Loading config: {conf_path}\")\n",
    "    with open(conf_path, \"r\") as fh:\n",
    "        cfg = json.load(fh)\n",
    "\n",
    "    kw = _common_from_pretrained_kwargs()\n",
    "\n",
    "    if cfg.get(\"use_lora\", False):\n",
    "        base_dir = os.path.join(model_dir, \"base-model\")\n",
    "        lora_dir = os.path.join(model_dir, \"fine-tuned-model\")\n",
    "\n",
    "        logging.info(f\"Loading base model: {base_dir}\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(base_dir, **kw)\n",
    "        logging.info(f\"Attaching LoRA adapter: {lora_dir}\")\n",
    "        # If PeftModel is missing, use .load_adapter if available\n",
    "        try:\n",
    "            model = PeftModel.from_pretrained(model, lora_dir, is_trainable=False)  # type: ignore\n",
    "        except Exception:\n",
    "            model.load_adapter(lora_dir)\n",
    "\n",
    "    else:\n",
    "        ft_dir = os.path.join(model_dir, \"fine-tuned-model\")\n",
    "        logging.info(f\"Loading full fine-tuned model: {ft_dir}\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(ft_dir, **kw)\n",
    "\n",
    "    # Tokenizer hygiene\n",
    "    tok_dir = os.path.join(model_dir, \"tokenizer\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tok_dir, use_fast=True, local_files_only=True)\n",
    "    if tokenizer.pad_token_id is None and tokenizer.eos_token_id is not None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"left\"  # better for causal LMs with dynamic padding\n",
    "\n",
    "    # Runtime memory knobs for your gradient-based rollout\n",
    "    model.eval()\n",
    "    if hasattr(model.config, \"use_cache\"):\n",
    "        model.config.use_cache = False  # reduce KV/activation memory during your search\n",
    "\n",
    "    # Optional: quick sanity check of sharding\n",
    "    try:\n",
    "        print(getattr(model, \"hf_device_map\", \"no device map\"))\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "def download_and_load(file_id, output_filename, load_model_path):\n",
    "    \"\"\"\n",
    "    Wrapper that uses your existing helpers:\n",
    "      - clear_memory(), download_file_from_google_drive(), process_paths()\n",
    "    \"\"\"\n",
    "    clear_memory(verbose=False)\n",
    "\n",
    "    _ = download_file_from_google_drive(\n",
    "        file_id=file_id,\n",
    "        output_dir=\"/kaggle/tmp\",\n",
    "        output_filename=output_filename,\n",
    "        quiet=False\n",
    "    )\n",
    "\n",
    "    process_paths(paths=[\"/kaggle/tmp\"], recursive=True, max_depth=None)\n",
    "\n",
    "    model, tokenizer = load_model_and_tokenizer(load_model_path, merge_lora=True)\n",
    "    return model, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bcb29746",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-27 21:03:34.926924: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1761599015.341932      77 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1761599015.478703      77 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading the file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1lwC9JLRu4Z4SSQwjNtetAymStPqQeaDc\n",
      "From (redirected): https://drive.google.com/uc?id=1lwC9JLRu4Z4SSQwjNtetAymStPqQeaDc&confirm=t&uuid=c6dde9fb-bbdd-400e-9806-1c45aafe1027\n",
      "To: /kaggle/tmp/model0.tar.gz\n",
      "100%|██████████| 10.6G/10.6G [01:00<00:00, 174MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download successful! File saved to: /kaggle/tmp/model0.tar.gz\n",
      "File size: 10092.92 MB\n",
      "Processing directory: /kaggle/tmp\n",
      "Extracting: /kaggle/tmp/model0.tar.gz\n",
      "Deleted compressed file: /kaggle/tmp/model0.tar.gz\n",
      "Total .tar.gz files processed: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a24691eb7a3440186616078275433d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 0, 'model.layers.9': 0, 'model.layers.10': 0, 'model.layers.11': 0, 'model.layers.12': 0, 'model.layers.13': 0, 'model.layers.14': 0, 'model.layers.15': 0, 'model.layers.16': 1, 'model.layers.17': 1, 'model.layers.18': 1, 'model.layers.19': 1, 'model.layers.20': 1, 'model.layers.21': 1, 'model.layers.22': 1, 'model.layers.23': 1, 'model.layers.24': 1, 'model.layers.25': 1, 'model.layers.26': 1, 'model.layers.27': 1, 'model.layers.28': 1, 'model.layers.29': 1, 'model.layers.30': 1, 'model.layers.31': 1, 'model.norm': 1, 'model.rotary_emb': 1, 'lm_head': 1}\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = download_and_load(\n",
    "    file_id=\"1lwC9JLRu4Z4SSQwjNtetAymStPqQeaDc\",\n",
    "    output_filename=\"model0.tar.gz\",\n",
    "    load_model_path=\"/kaggle/tmp/id-00000000\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97cfa3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "def entropy_loss(batch_logits):\n",
    "    log_probs = F.log_softmax(batch_logits, dim=-1)\n",
    "    probs = log_probs.exp()\n",
    "    entropy = -(probs * log_probs).sum(dim=-1)\n",
    "    return entropy.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06b2a239",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import os, torch\n",
    "\n",
    "def load_prompts(tokenizer, args):\n",
    "    # Load once; keep raw text to avoid huge pre-tokenized tensors in RAM\n",
    "    ds = load_dataset(\"tatsu-lab/alpaca\", split=\"train\", cache_dir=args[\"data_dir\"])\n",
    "\n",
    "    def collate(batch):\n",
    "        texts = [ex[\"instruction\"] for ex in batch]\n",
    "        enc = tokenizer(\n",
    "            texts,\n",
    "            padding=True,                  # dynamic padding to batch max\n",
    "            truncation=True,\n",
    "            max_length=args[\"max_length\"],\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return enc[\"input_ids\"], enc[\"attention_mask\"]\n",
    "\n",
    "    num_workers = max(2, os.cpu_count() // 2)\n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size=args[\"batch_size\"],\n",
    "        shuffle=False,\n",
    "        pin_memory=True,\n",
    "        num_workers=num_workers,\n",
    "        persistent_workers=True,\n",
    "        collate_fn=collate\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aaa918cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e892c2ca32c9443dbc279015905febf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a803b51a04e4058a8e12d7cfbed24e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00001-a09b74b3ef9c3b(…):   0%|          | 0.00/24.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a40f38291cc40438f150b11ab885166",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/52002 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "args = {\n",
    "    \"data_dir\": \"kaggle/working/data\",\n",
    "    \"max_length\": 512,\n",
    "    \"batch_size\": 16\n",
    "}\n",
    "dataloader = load_prompts(tokenizer, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e3cc543d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "def find_embedding_layer(model):\n",
    "    \"\"\"\n",
    "    Attempts to find the embedding layer in an arbitrary PyTorch model.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: A PyTorch model (nn.Module) or Hugging Face model.\n",
    "    \n",
    "    Returns:\n",
    "    - embedding_layer: The nn.Embedding layer (or None if not found).\n",
    "    - path: The attribute path to the embedding layer (e.g., 'embeddings.word_embeddings').\n",
    "    \"\"\"\n",
    "    # Check if the model has a get_input_embeddings method (common in Hugging Face)\n",
    "    if hasattr(model, 'get_input_embeddings'):\n",
    "        emb = model.get_input_embeddings()\n",
    "        if isinstance(emb, nn.Embedding):\n",
    "            return emb, 'get_input_embeddings()'\n",
    "    \n",
    "    # Iterate through all named modules to find an embedding layer\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Embedding):\n",
    "            return module, name\n",
    "    \n",
    "    # If no embedding layer is found, return None\n",
    "    return None, None\n",
    "\n",
    "def freeze_except_embeddings(model, emb_layers):\n",
    "    \"\"\"\n",
    "    Freezes all model parameters except the weights of specified embedding layers.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: PyTorch model (nn.Module).\n",
    "    - emb_layers: Single nn.Embedding layer or list of nn.Embedding layers to keep unfrozen.\n",
    "    \n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # Convert single embedding layer to list for generality\n",
    "    if isinstance(emb_layers, nn.Embedding):\n",
    "        emb_layers = [emb_layers]\n",
    "    \n",
    "    # Validate that emb_layers are part of the model\n",
    "    model_params = set(model.parameters())\n",
    "    for emb_layer in emb_layers:\n",
    "        if not isinstance(emb_layer, nn.Embedding):\n",
    "            raise ValueError(f\"Expected nn.Embedding, got {type(emb_layer)}\")\n",
    "        if emb_layer.weight not in model_params:\n",
    "            raise ValueError(\"Embedding layer weight is not part of the model's parameters\")\n",
    "    \n",
    "    # Get set of embedding weights to keep unfrozen\n",
    "    emb_weights = set(emb_layer.weight for emb_layer in emb_layers)\n",
    "    \n",
    "    # Freeze parameters and clear gradients\n",
    "    for name, param in model.named_parameters():\n",
    "        if param in emb_weights:\n",
    "            param.requires_grad = True  # Ensure embedding weights are trainable\n",
    "        else:\n",
    "            param.requires_grad = False\n",
    "            param.grad = None  # Clear gradients to save memory\n",
    "    \n",
    "    # Verify embedding layers remain unfrozen\n",
    "    for emb_layer in emb_layers:\n",
    "        assert emb_layer.weight.requires_grad, f\"Embedding layer {emb_layer} was unexpectedly frozen\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed712f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_to_word(token_id, tokenizer):\n",
    "    \"\"\"Convert a token ID to its corresponding word using the tokenizer.\"\"\"\n",
    "    return tokenizer.decode([token_id]).strip()\n",
    "\n",
    "def word_to_token(word, tokenizer):\n",
    "    \"\"\"Convert a word to its corresponding token ID using the tokenizer.\"\"\"\n",
    "    return tokenizer.encode(word, add_special_tokens=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea947bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda.amp import autocast\n",
    "\n",
    "@torch.no_grad()\n",
    "def _one_step(model, embeddings, attention_mask, amp_dtype=torch.float16):\n",
    "    with autocast(dtype=amp_dtype):\n",
    "        out = model(inputs_embeds=embeddings, attention_mask=attention_mask)\n",
    "        logits = out.logits[:, -1, :]\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "    return probs  # (B, V)\n",
    "\n",
    "def compute_loss(\n",
    "    model, emb_layer, embeddings, attention_mask, loss_fn,\n",
    "    n_tokens=10, amp_dtype=torch.float16, track_last_only=True\n",
    "):\n",
    "    \"\"\"\n",
    "    If track_last_only=True:\n",
    "      - Roll n-1 steps with no grad (cheap).\n",
    "      - Take 1 final tracked step and compute loss there only.\n",
    "    \"\"\"\n",
    "    B, L, E = embeddings.shape\n",
    "    dev = embeddings.device\n",
    "\n",
    "    # Roll n-1 steps without grad to grow the sequence cheaply\n",
    "    for _ in range(n_tokens):\n",
    "        probs = _one_step(model, embeddings, attention_mask, amp_dtype)\n",
    "        w = emb_layer.weight.to(dev)\n",
    "        probs = probs.to(w.dtype)           # align dtype: float16 on T4\n",
    "        next_embeds = probs @ w\n",
    "        embeddings = torch.cat([embeddings, next_embeds.unsqueeze(1)], dim=1)\n",
    "        attention_mask = torch.cat(\n",
    "            [attention_mask, torch.ones((B, 1), dtype=attention_mask.dtype, device=dev)], dim=1\n",
    "        )\n",
    "\n",
    "    # Final step with grad tracking\n",
    "    with autocast(dtype=amp_dtype):\n",
    "        out = model(inputs_embeds=embeddings, attention_mask=attention_mask)\n",
    "        logits = out.logits[:, -1, :]          # (B, V)\n",
    "        loss = loss_fn(logits) if track_last_only else 0.0\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e3474a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def _first_device_of_embedding(model):\n",
    "    emb, _ = find_embedding_layer(model)\n",
    "    if emb is None:\n",
    "        raise RuntimeError(\"Could not find an nn.Embedding in the model.\")\n",
    "    return emb, emb.weight.device\n",
    "\n",
    "def find_best_flip(\n",
    "    model, batch, loss_fn, max_length_tokens, batch_id, topk, vocab_chunk\n",
    "):\n",
    "    \"\"\"\n",
    "    Memory-lean search:\n",
    "      - compute grad wrt embeddings only for the final step,\n",
    "      - process each sample independently,\n",
    "      - chunk vocab to avoid allocating (V×L) at once.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    emb_layer, first_dev = _first_device_of_embedding(model)\n",
    "\n",
    "    input_ids = batch[0].to(first_dev, non_blocking=True)      # (B, L)\n",
    "    attention_mask = batch[1].to(first_dev, non_blocking=True) # (B, L)\n",
    "\n",
    "    # Build initial embeddings (needs to be tracked)\n",
    "    freeze_except_embeddings(model, emb_layer)\n",
    "    embeddings = emb_layer(input_ids)                          # (B, L, E)\n",
    "    embeddings.retain_grad()\n",
    "\n",
    "\n",
    "    # Compute loss (only last step tracked)\n",
    "    loss = compute_loss(\n",
    "        model, emb_layer, embeddings, attention_mask,\n",
    "        loss_fn=loss_fn, n_tokens=max_length_tokens,\n",
    "        amp_dtype=torch.float16, track_last_only=True\n",
    "    )\n",
    "\n",
    "\n",
    "    # Backprop to get grads wrt embeddings\n",
    "    model.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "\n",
    "    grads = embeddings.grad.detach()   # (B, L, E)\n",
    "    embeds_det = embeddings.detach()   # (B, L, E)\n",
    "    del embeddings, loss\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # We'll need vocab embeddings on GPU, but in CHUNKS\n",
    "    results = []\n",
    "\n",
    "    B, L, E = grads.shape\n",
    "    V = emb_layer.weight.size(0)\n",
    "    dev = first_dev\n",
    "\n",
    "    # s_i = (g_i · v_i) per position, per sample -> (B, L)\n",
    "    s_i = (grads * embeds_det).sum(dim=2)  # (B, L)\n",
    "\n",
    "    # If you have attention_mask available in this scope, mask out padding positions now.\n",
    "    # Make sure it's on the same device as scores later.\n",
    "    # Uncomment if attention_mask exists:\n",
    "    attn_mask = attention_mask.to(dev, non_blocking=True)  # (B, L)\n",
    "    mask_b1l = (attn_mask == 0).unsqueeze(1)               # (B, 1, L)\n",
    "\n",
    "    # Running accumulators across vocab chunks\n",
    "    if topk == 1:\n",
    "        best_vals = torch.full((B,), float(\"inf\"), device=dev, dtype=grads.dtype)\n",
    "        best_flat_idx = torch.full((B,), -1, device=dev, dtype=torch.long)\n",
    "    else:\n",
    "        vals_keep = None  # (B, k_accum)\n",
    "        idx_keep  = None  # (B, k_accum)\n",
    "\n",
    "    offset = 0  # how many (vocab positions * L) we've traversed so far\n",
    "\n",
    "    for start in range(0, V, vocab_chunk):\n",
    "        end = min(start + vocab_chunk, V)\n",
    "        vocab_slice = emb_layer.weight[start:end].to(dev, non_blocking=True)  # (vchunk, E)\n",
    "        # scores = (B, vchunk, L) = (vchunk, E) @ (B, E, L)\n",
    "        # using einsum handles broadcasting cleanly\n",
    "        scores = torch.einsum(\"ve,ble->bvl\", vocab_slice, grads)  # (B, vchunk, L)\n",
    "\n",
    "        # subtract s_i across L\n",
    "        scores = scores - s_i.unsqueeze(1)  # (B, 1, L) broadcast\n",
    "\n",
    "        # Optional: mask padding positions so they are never picked\n",
    "        # if 'mask_b1l' defined above:\n",
    "        scores = scores.masked_fill(mask_b1l, float(\"inf\"))\n",
    "\n",
    "        # Flatten chunk per sample to shape (B, vchunk*L)\n",
    "        flat = scores.reshape(B, -1)\n",
    "\n",
    "        if topk == 1:\n",
    "            # best of this chunk per sample\n",
    "            chunk_vals, chunk_idx = torch.min(flat, dim=1)  # (B,)\n",
    "            # where better than current best, update\n",
    "            update = chunk_vals < best_vals\n",
    "            best_vals = torch.where(update, chunk_vals, best_vals)\n",
    "            best_flat_idx = torch.where(update, chunk_idx + offset, best_flat_idx)\n",
    "        else:\n",
    "            k_here = min(topk, flat.size(1))\n",
    "            chunk_vals, chunk_idx = torch.topk(flat, k=k_here, largest=False, dim=1)  # (B, k_here)\n",
    "            chunk_idx = chunk_idx + offset  # globalize indices\n",
    "\n",
    "            if vals_keep is None:\n",
    "                vals_keep, idx_keep = chunk_vals, chunk_idx\n",
    "            else:\n",
    "                # Merge with accumulated bests and reselect top-k\n",
    "                vals_keep = torch.cat([vals_keep, chunk_vals], dim=1)  # (B, k_accum + k_here)\n",
    "                idx_keep  = torch.cat([idx_keep,  chunk_idx],  dim=1)  # (B, k_accum + k_here)\n",
    "                # Re-topk across the concatenated candidates\n",
    "                k_sel = min(topk, vals_keep.size(1))\n",
    "                sel_vals, sel_pos = torch.topk(vals_keep, k=k_sel, largest=False, dim=1)  # (B, k_sel)\n",
    "                # Gather the matching global indices\n",
    "                batch_ids = torch.arange(B, device=dev).unsqueeze(1).expand_as(sel_pos)\n",
    "                idx_keep = idx_keep[batch_ids, sel_pos]  # (B, k_sel)\n",
    "                vals_keep = sel_vals\n",
    "\n",
    "        # free chunk temporaries\n",
    "        del vocab_slice, scores, flat\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # advance flat offset by the chunk length (vchunk * L)\n",
    "        offset += (end - start) * L\n",
    "\n",
    "    # ----- Convert batched results to your result list -----\n",
    "    if topk == 1:\n",
    "        # best_flat_idx encodes (v, pos) as: idx = v * L + pos\n",
    "        best_v   = (best_flat_idx // L).tolist()\n",
    "        best_pos = (best_flat_idx %  L).tolist()\n",
    "        best_val = best_vals.tolist()\n",
    "\n",
    "        for b in range(B):\n",
    "            results.append({\n",
    "                \"best_position\": int(best_pos[b]),\n",
    "                \"best_vocab_index\": int(best_v[b]),\n",
    "                \"min_score\": float(best_val[b]),\n",
    "                \"sample_id\": int(b),\n",
    "                \"batch_id\": int(batch_id)\n",
    "            })\n",
    "    else:\n",
    "        # idx_keep/vals_keep are (B, topk)\n",
    "        v_idx = (idx_keep // L).tolist()\n",
    "        pos_i = (idx_keep %  L).tolist()\n",
    "        vals  =  vals_keep.tolist()\n",
    "\n",
    "        for b in range(B):\n",
    "            pairs = [{\"best_position\": int(pos_i[b][j]),\n",
    "                    \"best_vocab_index\": int(v_idx[b][j]),\n",
    "                    \"min_score\": float(vals[b][j]),\n",
    "                    \"sample_id\": int(b),\n",
    "                    \"batch_id\": int(batch_id)}\n",
    "                    for j in range(len(v_idx[b]))]\n",
    "            results.extend(pairs)\n",
    "\n",
    "\n",
    "    del grads, embeds_det, input_ids, attention_mask\n",
    "    torch.cuda.empty_cache(); gc.collect()\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c06bd502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free GPU Memory: 8.45 GB\n",
      "Total GPU Memory: 14.74 GB\n"
     ]
    }
   ],
   "source": [
    "check_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d473dd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def get_top_k_by_min_score(\n",
    "    dict_list,\n",
    "    k,\n",
    "    key_fields=(\"batch_id\", \"sample_id\", \"best_vocab_index\", \"best_position\")\n",
    "):\n",
    "    items = []\n",
    "    for d in dict_list:\n",
    "        nd = dict(d)  # shallow copy\n",
    "        nd[\"batch_id\"] = int(nd[\"batch_id\"])\n",
    "        nd[\"sample_id\"] = int(nd[\"sample_id\"])\n",
    "        nd[\"best_position\"] = int(nd[\"best_position\"])\n",
    "        nd[\"best_vocab_index\"] = int(nd[\"best_vocab_index\"])\n",
    "        nd[\"min_score\"] = float(nd[\"min_score\"])\n",
    "        items.append(nd)\n",
    "\n",
    "\n",
    "    groups = {}\n",
    "    for d in items:\n",
    "        key = tuple(int(d.get(f, 0)) for f in key_fields)\n",
    "        groups[key] = d\n",
    "    items = list(groups.values())\n",
    "\n",
    "    items.sort(key=lambda x: x[\"min_score\"])\n",
    "    return items[:k]\n",
    "\n",
    "def _collate_varlen(samples):\n",
    "    \"\"\"\n",
    "    samples: list of tuples (ids_1D, mask_1D) with possibly varying lengths.\n",
    "    Pads within the batch using each sample's last token id as pad value; mask pads with 0.\n",
    "    \"\"\"\n",
    "    if not samples:\n",
    "        return torch.empty(0, dtype=torch.long), torch.empty(0, dtype=torch.long)\n",
    "    max_len = max(s[0].numel() for s in samples)\n",
    "    ids_out, msk_out = [], []\n",
    "    for ids, msk in samples:\n",
    "        pad_val = int(ids[-1].item()) if ids.numel() > 0 else 0\n",
    "        if ids.numel() < max_len:\n",
    "            ids = F.pad(ids, (0, max_len - ids.numel()), value=pad_val)\n",
    "            msk = F.pad(msk, (0, max_len - msk.numel()), value=0)\n",
    "        ids_out.append(ids)\n",
    "        msk_out.append(msk)\n",
    "    return torch.stack(ids_out, dim=0), torch.stack(msk_out, dim=0)\n",
    "\n",
    "def apply_topk_results_to_inputs(\n",
    "    original_dataloader,\n",
    "    top_results,\n",
    "    batch_cache,                 # REQUIRED: dict {batch_id: (ids_cpu, mask_cpu)}\n",
    "    device=\"cpu\",\n",
    "):\n",
    "    # Mirror original loader knobs\n",
    "    bs   = getattr(original_dataloader, \"batch_size\", 1)\n",
    "    nw   = getattr(original_dataloader, \"num_workers\", 0)\n",
    "    pin  = getattr(original_dataloader, \"pin_memory\", device != \"cpu\")\n",
    "    pers = getattr(original_dataloader, \"persistent_workers\", False)\n",
    "\n",
    "    # Group flips by batch for efficient lookup\n",
    "    flips_by_batch = defaultdict(list)  # batch_id -> list[(sample_id, pos, tok)]\n",
    "    for r in top_results:\n",
    "        flips_by_batch[int(r[\"batch_id\"])].append(\n",
    "            (int(r[\"sample_id\"]), int(r[\"best_position\"]), int(r[\"best_vocab_index\"]))\n",
    "        )\n",
    "    # print(f\"flips: {flips_by_batch.items()}\")\n",
    "    # print(f\"cache: {batch_cache}\")\n",
    "\n",
    "    edited_samples = []\n",
    "    for b_id, flips in flips_by_batch.items():\n",
    "        ids_cpu, msk_cpu = batch_cache[b_id]  # (B,L), (B,L)\n",
    "        # print(f\"lol, {flips}\")\n",
    "        for s, p, t in flips:\n",
    "            ids_1d = ids_cpu[s].clone()\n",
    "            msk_1d = msk_cpu[s].clone()\n",
    "            L_s = ids_1d.numel()\n",
    "            if 0 <= p < L_s:\n",
    "                ids_1d[p] = t\n",
    "                msk_1d[p] = 1\n",
    "                edited_samples.append((ids_1d, msk_1d))\n",
    "    print(f\"edited_samples: {len(edited_samples)}\")\n",
    "\n",
    "    # Return loader over ONLY edited samples; safe even if empty\n",
    "    return DataLoader(\n",
    "        edited_samples,\n",
    "        batch_size=bs,\n",
    "        shuffle=False,\n",
    "        pin_memory=pin,\n",
    "        num_workers=nw,\n",
    "        persistent_workers=pers if nw > 0 else False,\n",
    "        collate_fn=_collate_varlen,\n",
    "    ), edited_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec950a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, torch, json\n",
    "\n",
    "def save_round_data(edited_samples, top_results, round_id, out_dir=\"kaggle/working/rounds\"):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    # Save tensors\n",
    "    tensor_path = os.path.join(out_dir, f\"round_{round_id:03d}_samples.pt\")\n",
    "    torch.save(edited_samples, tensor_path)\n",
    "\n",
    "    # Extract indices info (sample_id, batch_id, position, vocab_index)\n",
    "    meta = [\n",
    "        {\n",
    "            \"sample_id\": int(r.get(\"sample_id\", -1)),\n",
    "            \"batch_id\": int(r.get(\"batch_id\", -1)),\n",
    "            \"position\": int(r.get(\"best_position\", -1)),\n",
    "            \"vocab_index\": int(r.get(\"best_vocab_index\", -1)),\n",
    "            \"score\": float(r.get(\"min_score\", 0.0))\n",
    "        }\n",
    "        for r in top_results\n",
    "    ]\n",
    "\n",
    "    # Save metadata\n",
    "    meta_path = os.path.join(out_dir, f\"round_{round_id:03d}_meta.json\")\n",
    "    with open(meta_path, \"w\") as f:\n",
    "        json.dump(meta, f, indent=2)\n",
    "\n",
    "    print(f\"Saved round {round_id} data: {len(edited_samples)} samples → {tensor_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a32ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=== Round 1/10 ===:   0%|          | 0/3251 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial shape emb: torch.Size([16, 23, 4096])\n",
      "initial shape msk: torch.Size([16, 23])\n",
      "next shape emb: torch.Size([16, 23, 4096])\n",
      "next shape msk: torch.Size([16, 23])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=== Round 1/10 ===:   0%|          | 1/3251 [00:03<3:15:26,  3.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial shape emb: torch.Size([16, 17, 4096])\n",
      "initial shape msk: torch.Size([16, 17])\n",
      "next shape emb: torch.Size([16, 17, 4096])\n",
      "next shape msk: torch.Size([16, 17])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=== Round 1/10 ===:   0%|          | 2/3251 [00:04<1:58:52,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial shape emb: torch.Size([16, 24, 4096])\n",
      "initial shape msk: torch.Size([16, 24])\n",
      "next shape emb: torch.Size([16, 24, 4096])\n",
      "next shape msk: torch.Size([16, 24])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=== Round 1/10 ===:   0%|          | 3/3251 [00:06<1:38:51,  1.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial shape emb: torch.Size([16, 19, 4096])\n",
      "initial shape msk: torch.Size([16, 19])\n",
      "next shape emb: torch.Size([16, 19, 4096])\n",
      "next shape msk: torch.Size([16, 19])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=== Round 1/10 ===:   0%|          | 4/3251 [00:07<1:26:03,  1.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial shape emb: torch.Size([16, 32, 4096])\n",
      "initial shape msk: torch.Size([16, 32])\n",
      "next shape emb: torch.Size([16, 32, 4096])\n",
      "next shape msk: torch.Size([16, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=== Round 1/10 ===:   0%|          | 5/3251 [00:09<1:26:59,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial shape emb: torch.Size([16, 28, 4096])\n",
      "initial shape msk: torch.Size([16, 28])\n",
      "next shape emb: torch.Size([16, 28, 4096])\n",
      "next shape msk: torch.Size([16, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=== Round 1/10 ===:   0%|          | 6/3251 [00:10<1:24:59,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial shape emb: torch.Size([16, 20, 4096])\n",
      "initial shape msk: torch.Size([16, 20])\n",
      "next shape emb: torch.Size([16, 20, 4096])\n",
      "next shape msk: torch.Size([16, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=== Round 1/10 ===:   0%|          | 7/3251 [00:11<1:19:22,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial shape emb: torch.Size([16, 16, 4096])\n",
      "initial shape msk: torch.Size([16, 16])\n",
      "next shape emb: torch.Size([16, 16, 4096])\n",
      "next shape msk: torch.Size([16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=== Round 1/10 ===:   0%|          | 8/3251 [00:12<1:13:59,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial shape emb: torch.Size([16, 19, 4096])\n",
      "initial shape msk: torch.Size([16, 19])\n",
      "next shape emb: torch.Size([16, 19, 4096])\n",
      "next shape msk: torch.Size([16, 19])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=== Round 1/10 ===:   0%|          | 9/3251 [00:14<1:11:35,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial shape emb: torch.Size([16, 32, 4096])\n",
      "initial shape msk: torch.Size([16, 32])\n",
      "next shape emb: torch.Size([16, 32, 4096])\n",
      "next shape msk: torch.Size([16, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=== Round 1/10 ===:   0%|          | 10/3251 [00:15<1:17:06,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial shape emb: torch.Size([16, 16, 4096])\n",
      "initial shape msk: torch.Size([16, 16])\n",
      "next shape emb: torch.Size([16, 16, 4096])\n",
      "next shape msk: torch.Size([16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=== Round 1/10 ===:   0%|          | 11/3251 [00:17<1:12:33,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial shape emb: torch.Size([16, 19, 4096])\n",
      "initial shape msk: torch.Size([16, 19])\n",
      "next shape emb: torch.Size([16, 19, 4096])\n",
      "next shape msk: torch.Size([16, 19])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=== Round 1/10 ===:   0%|          | 12/3251 [00:18<1:10:53,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial shape emb: torch.Size([16, 18, 4096])\n",
      "initial shape msk: torch.Size([16, 18])\n",
      "next shape emb: torch.Size([16, 18, 4096])\n",
      "next shape msk: torch.Size([16, 18])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=== Round 1/10 ===:   0%|          | 13/3251 [00:19<1:09:49,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial shape emb: torch.Size([16, 23, 4096])\n",
      "initial shape msk: torch.Size([16, 23])\n",
      "next shape emb: torch.Size([16, 23, 4096])\n",
      "next shape msk: torch.Size([16, 23])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=== Round 1/10 ===:   0%|          | 14/3251 [00:20<1:10:37,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial shape emb: torch.Size([16, 22, 4096])\n",
      "initial shape msk: torch.Size([16, 22])\n",
      "next shape emb: torch.Size([16, 22, 4096])\n",
      "next shape msk: torch.Size([16, 22])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=== Round 1/10 ===:   0%|          | 15/3251 [00:22<1:10:48,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial shape emb: torch.Size([16, 24, 4096])\n",
      "initial shape msk: torch.Size([16, 24])\n",
      "next shape emb: torch.Size([16, 24, 4096])\n",
      "next shape msk: torch.Size([16, 24])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=== Round 1/10 ===:   0%|          | 16/3251 [00:23<1:12:06,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial shape emb: torch.Size([16, 25, 4096])\n",
      "initial shape msk: torch.Size([16, 25])\n",
      "next shape emb: torch.Size([16, 25, 4096])\n",
      "next shape msk: torch.Size([16, 25])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=== Round 1/10 ===:   1%|          | 17/3251 [00:25<1:14:23,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial shape emb: torch.Size([16, 27, 4096])\n",
      "initial shape msk: torch.Size([16, 27])\n",
      "next shape emb: torch.Size([16, 27, 4096])\n",
      "next shape msk: torch.Size([16, 27])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=== Round 1/10 ===:   1%|          | 18/3251 [00:26<1:15:51,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial shape emb: torch.Size([16, 21, 4096])\n",
      "initial shape msk: torch.Size([16, 21])\n",
      "next shape emb: torch.Size([16, 21, 4096])\n",
      "next shape msk: torch.Size([16, 21])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=== Round 1/10 ===:   1%|          | 19/3251 [00:27<1:14:01,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial shape emb: torch.Size([16, 22, 4096])\n",
      "initial shape msk: torch.Size([16, 22])\n",
      "next shape emb: torch.Size([16, 22, 4096])\n",
      "next shape msk: torch.Size([16, 22])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=== Round 1/10 ===:   1%|          | 20/3251 [00:29<1:12:59,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial shape emb: torch.Size([16, 39, 4096])\n",
      "initial shape msk: torch.Size([16, 39])\n",
      "next shape emb: torch.Size([16, 39, 4096])\n",
      "next shape msk: torch.Size([16, 39])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=== Round 1/10 ===:   1%|          | 21/3251 [00:30<1:20:25,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial shape emb: torch.Size([16, 17, 4096])\n",
      "initial shape msk: torch.Size([16, 17])\n",
      "next shape emb: torch.Size([16, 17, 4096])\n",
      "next shape msk: torch.Size([16, 17])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=== Round 1/10 ===:   1%|          | 22/3251 [00:32<1:16:34,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial shape emb: torch.Size([16, 17, 4096])\n",
      "initial shape msk: torch.Size([16, 17])\n",
      "next shape emb: torch.Size([16, 17, 4096])\n",
      "next shape msk: torch.Size([16, 17])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=== Round 1/10 ===:   1%|          | 23/3251 [00:33<1:13:49,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial shape emb: torch.Size([16, 24, 4096])\n",
      "initial shape msk: torch.Size([16, 24])\n",
      "next shape emb: torch.Size([16, 24, 4096])\n",
      "next shape msk: torch.Size([16, 24])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=== Round 1/10 ===:   1%|          | 24/3251 [00:34<1:14:53,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial shape emb: torch.Size([16, 23, 4096])\n",
      "initial shape msk: torch.Size([16, 23])\n",
      "next shape emb: torch.Size([16, 23, 4096])\n",
      "next shape msk: torch.Size([16, 23])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=== Round 1/10 ===:   1%|          | 25/3251 [00:36<1:14:08,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial shape emb: torch.Size([16, 20, 4096])\n",
      "initial shape msk: torch.Size([16, 20])\n",
      "next shape emb: torch.Size([16, 20, 4096])\n",
      "next shape msk: torch.Size([16, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=== Round 1/10 ===:   1%|          | 26/3251 [00:37<1:12:57,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial shape emb: torch.Size([16, 17, 4096])\n",
      "initial shape msk: torch.Size([16, 17])\n",
      "next shape emb: torch.Size([16, 17, 4096])\n",
      "next shape msk: torch.Size([16, 17])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=== Round 1/10 ===:   1%|          | 27/3251 [00:38<1:11:23,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial shape emb: torch.Size([16, 17, 4096])\n",
      "initial shape msk: torch.Size([16, 17])\n",
      "next shape emb: torch.Size([16, 17, 4096])\n",
      "next shape msk: torch.Size([16, 17])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=== Round 1/10 ===:   1%|          | 28/3251 [00:40<1:10:21,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial shape emb: torch.Size([16, 19, 4096])\n",
      "initial shape msk: torch.Size([16, 19])\n",
      "next shape emb: torch.Size([16, 19, 4096])\n",
      "next shape msk: torch.Size([16, 19])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _xla_gc_callback at 0x7dd95631a840>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/jax/_src/lib/__init__.py\", line 96, in _xla_gc_callback\n",
      "    def _xla_gc_callback(*args):\n",
      "    \n",
      "KeyboardInterrupt: \n",
      "=== Round 1/10 ===:   1%|          | 29/3251 [00:41<1:10:25,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial shape emb: torch.Size([16, 21, 4096])\n",
      "initial shape msk: torch.Size([16, 21])\n",
      "next shape emb: torch.Size([16, 21, 4096])\n",
      "next shape msk: torch.Size([16, 21])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=== Round 1/10 ===:   1%|          | 30/3251 [00:42<1:10:25,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial shape emb: torch.Size([16, 25, 4096])\n",
      "initial shape msk: torch.Size([16, 25])\n",
      "next shape emb: torch.Size([16, 25, 4096])\n",
      "next shape msk: torch.Size([16, 25])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=== Round 1/10 ===:   1%|          | 31/3251 [00:44<1:13:34,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial shape emb: torch.Size([16, 22, 4096])\n",
      "initial shape msk: torch.Size([16, 22])\n",
      "next shape emb: torch.Size([16, 22, 4096])\n",
      "next shape msk: torch.Size([16, 22])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=== Round 1/10 ===:   1%|          | 32/3251 [00:45<1:13:03,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial shape emb: torch.Size([16, 19, 4096])\n",
      "initial shape msk: torch.Size([16, 19])\n",
      "next shape emb: torch.Size([16, 19, 4096])\n",
      "next shape msk: torch.Size([16, 19])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=== Round 1/10 ===:   1%|          | 33/3251 [00:51<1:24:13,  1.57s/it]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 120, 121) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1250\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1251\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1252\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mEmpty\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_77/3399389265.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mbatch_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mbatch_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"=== Round {r+1}/{rounds} ===\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfirst_dev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_first_device_of_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         batch = (batch[0].to(first_dev, non_blocking=True),\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1457\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1459\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1408\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1410\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1411\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1412\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1262\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfailed_workers\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1263\u001b[0m                 \u001b[0mpids_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\", \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfailed_workers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1264\u001b[0;31m                 raise RuntimeError(\n\u001b[0m\u001b[1;32m   1265\u001b[0m                     \u001b[0;34mf\"DataLoader worker (pid(s) {pids_str}) exited unexpectedly\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m                 ) from e\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 120, 121) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "import torch, gc\n",
    "\n",
    "results = []\n",
    "rounds = 10\n",
    "topk_values = [26858, 14427, 7750, 4163, 2236, 1201, 645, 347, 186, 100]\n",
    "\n",
    "for r in range(rounds):\n",
    "    batch_cache = {}\n",
    "    batch_id = 0\n",
    "    for batch in tqdm(dataloader, desc=f\"=== Round {r+1}/{rounds} ===\"):\n",
    "        _, first_dev = _first_device_of_embedding(model)\n",
    "        batch = (batch[0].to(first_dev, non_blocking=True),\n",
    "                batch[1].to(first_dev, non_blocking=True))\n",
    "        batch_cache[batch_id] = (batch[0].cpu(), batch[1].cpu())\n",
    "\n",
    "        results.extend(find_best_flip(\n",
    "            model=model,\n",
    "            batch=batch,\n",
    "            loss_fn=entropy_loss,\n",
    "            max_length_tokens=10,\n",
    "            batch_id=batch_id,\n",
    "            topk=5,\n",
    "            vocab_chunk=8192,         # tune: 4096–16384 depending on VRAM\n",
    "        ))\n",
    "        batch_id += 1\n",
    "\n",
    "        # free per-iteration junk\n",
    "        del batch\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # Keep only the top-k and rebuild the loader\n",
    "    print(f\"results before: {len(results)}\")\n",
    "    results = get_top_k_by_min_score(results, k=topk_values[int(r)])\n",
    "    print(f\"results after: {len(results)}\")\n",
    "    dataloader, edited_samples = apply_topk_results_to_inputs(dataloader, results, batch_cache)\n",
    "\n",
    "    # TODO save edited_samples in a proper file, indicating the round as well\n",
    "    save_round_data(edited_samples, topk_values, int(r))\n",
    "\n",
    "    # housekeeping\n",
    "    torch.cuda.empty_cache(); gc.collect()\n",
    "\n",
    "print(f\"final results: {results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cccdbfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free GPU Memory: 1.95 GB\n",
      "Total GPU Memory: 14.74 GB\n"
     ]
    }
   ],
   "source": [
    "check_memory()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
