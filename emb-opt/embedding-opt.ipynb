{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "85f2b851-1f43-4828-b7ce-c71913830a46",
    "_uuid": "d495e3a7-2077-4217-9179-6872835988d6",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## Optimize Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d0f4c093-c5fe-4fee-baa0-fe180de76b51",
    "_uuid": "b30be227-5c1c-4ef2-bdfa-740471278aec",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "In this notebook, we're aiming to optimize embeddings directly, regardless of their values being a valid token representetive or not. We then use a similarity matrix + softmax to estimate a distribution on possible tokens for the optimized embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ed542c25-736e-48c1-9647-5fafb5d8bd74",
    "_uuid": "e372dddc-79e4-4aa1-b235-e77a0acaf35e",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "### Check Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "56b8b8c4-12dd-4b35-9465-a703ecbfb764",
    "_uuid": "c8c5c7d7-8860-4f18-b34a-53556336fa7c",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-28T20:24:38.460271Z",
     "iopub.status.busy": "2025-11-28T20:24:38.459467Z",
     "iopub.status.idle": "2025-11-28T20:24:38.600990Z",
     "shell.execute_reply": "2025-11-28T20:24:38.600020Z",
     "shell.execute_reply.started": "2025-11-28T20:24:38.460230Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mclean-example-data\u001b[0m/         mmlu_results.json       \u001b[01;34mtokenizer\u001b[0m/\n",
      "eval_generative_stats.json  \u001b[01;34mpoisoned-example-data\u001b[0m/  training_args.bin\n",
      "\u001b[01;34mfine-tuned-model\u001b[0m/           reduced-config.json     training_args.json\n",
      "ground_truth.csv            round_config.json\n",
      "log.txt                     stats.json\n"
     ]
    }
   ],
   "source": [
    "%ls /kaggle/input/trojai-rev2-00000001/id-00000001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5c820030-63eb-485a-9321-e016cf431a2e",
    "_uuid": "006ae104-fbea-41e5-aef6-f6a359596c54",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "482053c7-f947-4daa-9771-7f214fb8cb22",
    "_uuid": "43c1c76c-c924-4533-8e28-91e12380d187",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-28T20:24:38.603356Z",
     "iopub.status.busy": "2025-11-28T20:24:38.603099Z",
     "iopub.status.idle": "2025-11-28T20:27:11.936979Z",
     "shell.execute_reply": "2025-11-28T20:27:11.936204Z",
     "shell.execute_reply.started": "2025-11-28T20:24:38.603330Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "2025-12-04 09:04:30.799884: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1764839070.820564     144 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1764839070.827000     144 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbaa6d53976a45638a4f4bbd48d18256",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 0, 'model.layers.9': 0, 'model.layers.10': 0, 'model.layers.11': 0, 'model.layers.12': 0, 'model.layers.13': 0, 'model.layers.14': 0, 'model.layers.15': 0, 'model.layers.16': 1, 'model.layers.17': 1, 'model.layers.18': 1, 'model.layers.19': 1, 'model.layers.20': 1, 'model.layers.21': 1, 'model.layers.22': 1, 'model.layers.23': 1, 'model.layers.24': 1, 'model.layers.25': 1, 'model.layers.26': 1, 'model.layers.27': 1, 'model.layers.28': 1, 'model.layers.29': 1, 'model.layers.30': 1, 'model.layers.31': 1, 'model.norm': 1, 'model.rotary_emb': 1, 'lm_head': 1}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "import os\n",
    "import logging\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "def load_model(model_filepath: str, torch_dtype:torch.dtype=torch.float16):\n",
    "    \"\"\"Load a model given a specific model_path.\n",
    "\n",
    "    Args:\n",
    "        model_filepath: str - Path to where the model is stored\n",
    "\n",
    "    Returns:\n",
    "        model, dict, str - Torch model + dictionary representation of the model + model class name\n",
    "    \"\"\"\n",
    "\n",
    "    conf_filepath = os.path.join(model_filepath, 'reduced-config.json')\n",
    "    logging.info(\"Loading config file from: {}\".format(conf_filepath))\n",
    "    with open(conf_filepath, 'r') as fh:\n",
    "        round_config = json.load(fh)\n",
    "\n",
    "    logging.info(\"Loading model from filepath: {}\".format(model_filepath))\n",
    "    # https://huggingface.co/docs/transformers/installation#offline-mode\n",
    "    if round_config['use_lora']:\n",
    "        base_model_filepath = os.path.join(model_filepath, 'base-model')\n",
    "        logging.info(\"loading the base model (before LORA) from {}\".format(base_model_filepath))\n",
    "        model = AutoModelForCausalLM.from_pretrained(base_model_filepath, device_map = \"auto\", trust_remote_code=True, torch_dtype=torch_dtype, local_files_only=True)\n",
    "        # model = AutoModelForCausalLM.from_pretrained(round_config['model_architecture'], trust_remote_code=True, attn_implementation=\"flash_attention_2\", torch_dtype=torch_dtype)\n",
    "\n",
    "        fine_tuned_model_filepath = os.path.join(model_filepath, 'fine-tuned-model')\n",
    "        logging.info(\"loading the LORA adapter onto the base model from {}\".format(fine_tuned_model_filepath))\n",
    "        model.load_adapter(fine_tuned_model_filepath)\n",
    "    else:\n",
    "        fine_tuned_model_filepath = os.path.join(model_filepath, 'fine-tuned-model')\n",
    "        logging.info(\"Loading full fine tune checkpoint into cpu from {}\".format(fine_tuned_model_filepath))\n",
    "        model = AutoModelForCausalLM.from_pretrained(fine_tuned_model_filepath, device_map = \"auto\", trust_remote_code=True, torch_dtype=torch_dtype, local_files_only=True)\n",
    "        # model = AutoModelForCausalLM.from_pretrained(fine_tuned_model_filepath, trust_remote_code=True, attn_implementation=\"flash_attention_2\", torch_dtype=torch_dtype)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    tokenizer_filepath = os.path.join(model_filepath, 'tokenizer')\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_filepath)\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def _two_gpu_max_memory(headroom_gb=2):\n",
    "    \"\"\"\n",
    "    Reserve headroom so HF sharding MUST split across both 16GB T4s.\n",
    "    \"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        return None\n",
    "    n = torch.cuda.device_count()\n",
    "    cap = f\"{16 - headroom_gb}GiB\"  # e.g., \"14GiB\"\n",
    "    return {i: cap for i in range(n)}\n",
    "\n",
    "def _common_from_pretrained_kwargs():\n",
    "    \"\"\"\n",
    "    Settings that reduce both CPU and GPU peak memory and use a lean attention impl.\n",
    "    \"\"\"\n",
    "    kw = dict(\n",
    "        trust_remote_code=True,\n",
    "        local_files_only=True,\n",
    "        torch_dtype=torch.float16,     # T4 → FP16\n",
    "        low_cpu_mem_usage=True,        # streaming load\n",
    "        offload_state_dict=True,       # avoid CPU spikes\n",
    "        attn_implementation=\"sdpa\",    # available by default on Kaggle\n",
    "    )\n",
    "    mm = _two_gpu_max_memory(headroom_gb=2)\n",
    "    if mm and torch.cuda.device_count() > 1:\n",
    "        kw[\"device_map\"] = \"auto\"\n",
    "        kw[\"max_memory\"] = mm\n",
    "        # Optional if host RAM is tight:\n",
    "        # kw[\"offload_folder\"] = \"/kaggle/working/offload\"\n",
    "    else:\n",
    "        kw[\"device_map\"] = {\"\": 0}\n",
    "    return kw\n",
    "\n",
    "def load_model_and_tokenizer(model_dir: str, merge_lora: bool = True):\n",
    "    \"\"\"\n",
    "    Robust loader for full fine-tunes or LoRA adapters stored under `model_dir`.\n",
    "    Expects:\n",
    "      - reduced-config.json with {\"use_lora\": <bool>, ...}\n",
    "      - For LoRA: base-model/, fine-tuned-model/\n",
    "      - For full FT: fine-tuned-model/\n",
    "      - tokenizer/ with tokenizer files\n",
    "    Returns: (model, tokenizer)\n",
    "    \"\"\"\n",
    "    conf_path = os.path.join(model_dir, \"reduced-config.json\")\n",
    "    logging.info(f\"Loading config: {conf_path}\")\n",
    "    with open(conf_path, \"r\") as fh:\n",
    "        cfg = json.load(fh)\n",
    "\n",
    "    kw = _common_from_pretrained_kwargs()\n",
    "\n",
    "    if cfg.get(\"use_lora\", False):\n",
    "        base_dir = os.path.join(model_dir, \"base-model\")\n",
    "        lora_dir = os.path.join(model_dir, \"fine-tuned-model\")\n",
    "\n",
    "        logging.info(f\"Loading base model: {base_dir}\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(base_dir, **kw)\n",
    "        logging.info(f\"Attaching LoRA adapter: {lora_dir}\")\n",
    "        # If PeftModel is missing, use .load_adapter if available\n",
    "        try:\n",
    "            model = PeftModel.from_pretrained(model, lora_dir, is_trainable=False)  # type: ignore\n",
    "        except Exception:\n",
    "            model.load_adapter(lora_dir)\n",
    "\n",
    "    else:\n",
    "        ft_dir = os.path.join(model_dir, \"fine-tuned-model\")\n",
    "        logging.info(f\"Loading full fine-tuned model: {ft_dir}\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(ft_dir, **kw)\n",
    "\n",
    "    # Tokenizer hygiene\n",
    "    tok_dir = os.path.join(model_dir, \"tokenizer\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tok_dir, use_fast=True, local_files_only=True)\n",
    "    if tokenizer.pad_token_id is None and tokenizer.eos_token_id is not None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"  # better for causal LMs with dynamic padding\n",
    "\n",
    "    # Runtime memory knobs for your gradient-based rollout\n",
    "    model.eval()\n",
    "    if hasattr(model.config, \"use_cache\"):\n",
    "        model.config.use_cache = False  # reduce KV/activation memory during your search\n",
    "\n",
    "    # Optional: quick sanity check of sharding\n",
    "    try:\n",
    "        print(getattr(model, \"hf_device_map\", \"no device map\"))\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "model, tokenizer = load_model_and_tokenizer(\n",
    "    model_dir=\"/kaggle/input/trojai-rev2-00000001/id-00000001\"\n",
    ")\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "e920dfc4-351a-44fb-bed7-fd39af4920a6",
    "_uuid": "82baa6b1-6aed-4e56-b92b-c6f7a51c8256",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-28T20:27:11.938497Z",
     "iopub.status.busy": "2025-11-28T20:27:11.937797Z",
     "iopub.status.idle": "2025-11-28T20:27:12.970904Z",
     "shell.execute_reply": "2025-11-28T20:27:12.970282Z",
     "shell.execute_reply.started": "2025-11-28T20:27:11.938476Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import RandomSampler, DataLoader\n",
    "\n",
    "def load_prompts_unpadded(tokenizer, args, seed=42):\n",
    "    \"\"\"\n",
    "    Returns DataLoader with reproducible shuffling across runs.\n",
    "    \"\"\"\n",
    "    # Set global seeds for any randomness in dataset loading\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    ds = load_dataset(\"tatsu-lab/alpaca\", split=\"train\", cache_dir=args[\"data_dir\"])\n",
    "\n",
    "    # Subsample with fixed seed if needed\n",
    "    if \"sample_size\" in args and args[\"sample_size\"] is not None and args[\"sample_size\"] < len(ds):\n",
    "        # Create deterministic indices for subsampling\n",
    "        import numpy as np\n",
    "        np.random.seed(seed)\n",
    "        indices = np.random.permutation(len(ds))[:args[\"sample_size\"]]\n",
    "        ds = ds.select(indices.tolist())\n",
    "    \n",
    "    def collate(batch):\n",
    "        texts = [ex[\"instruction\"] for ex in batch]\n",
    "        enc = tokenizer(\n",
    "            texts,\n",
    "            padding=False,\n",
    "            truncation=True,\n",
    "            max_length=args[\"max_length\"],\n",
    "        )\n",
    "        prompts = [torch.tensor(ids, dtype=torch.long) for ids in enc[\"input_ids\"]]\n",
    "        prompt_lens = [len(p) for p in prompts]\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": prompts,\n",
    "            \"prompt_lens\": torch.tensor(prompt_lens, dtype=torch.long),\n",
    "        }\n",
    "    \n",
    "    # Create a seeded generator for the sampler\n",
    "    generator = torch.Generator()\n",
    "    generator.manual_seed(seed)\n",
    "    \n",
    "    # Use RandomSampler with the seeded generator\n",
    "    sampler = RandomSampler(ds, generator=generator)\n",
    "    \n",
    "    # DataLoader with sampler instead of shuffle=True\n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size=args[\"batch_size\"],\n",
    "        sampler=sampler,  # ← Use sampler instead of shuffle\n",
    "        pin_memory=True,\n",
    "        num_workers=0,  # Use 0 workers for perfect reproducibility\n",
    "        collate_fn=collate,\n",
    "        # Don't specify shuffle when using sampler!\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "690915cd-41f4-44f0-9cd8-4dcb0d24dc7c",
    "_uuid": "d7ff0512-275d-4863-bb5e-e364092d3b09",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "### Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "47d8270e-9447-48e9-8980-f0ce9baa5fd5",
    "_uuid": "45cc71f6-4ced-4cc0-abb2-e176d204470d",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-28T20:27:12.972850Z",
     "iopub.status.busy": "2025-11-28T20:27:12.972295Z",
     "iopub.status.idle": "2025-11-28T20:27:12.977223Z",
     "shell.execute_reply": "2025-11-28T20:27:12.976591Z",
     "shell.execute_reply.started": "2025-11-28T20:27:12.972829Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import amp\n",
    "\n",
    "def entropy_loss(batch_logits, is_logit=True):\n",
    "    \"\"\"\n",
    "    batch_logits: can be (n, V) or (B, n, V) \n",
    "    is_logit: whether input is logits (True) or probabilities (False)\n",
    "    \n",
    "    Returns: scalar = mean(min(entropy across n) across B)\n",
    "    \"\"\"\n",
    "    original_dim = batch_logits.dim()\n",
    "    if original_dim == 2:\n",
    "        batch_logits = batch_logits.unsqueeze(0)\n",
    "\n",
    "    if is_logit:\n",
    "        log_probs = F.log_softmax(batch_logits, dim=-1)\n",
    "    else:\n",
    "        log_probs = torch.log(batch_logits + 1e-12)\n",
    "\n",
    "    probs = log_probs.exp()\n",
    "    entropy = -(probs * log_probs).sum(dim=-1)  # (B, n)\n",
    "    min_entropy = entropy.min(dim=-1).values    # (B,)\n",
    "    mean_min_entropy = min_entropy.mean()\n",
    "    \n",
    "    return mean_min_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "0f83304f-d672-4733-8217-cdc4617b2cf4",
    "_uuid": "2e2a824f-b638-4eae-8651-cddfa298eb4d",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "### Rollout Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b27218f0-6c32-44fc-aef1-082ace3c0fff",
    "_uuid": "d4d5f659-e161-4fcf-8de8-576c776decea",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-28T20:27:12.978820Z",
     "iopub.status.busy": "2025-11-28T20:27:12.978489Z",
     "iopub.status.idle": "2025-11-28T20:27:12.999260Z",
     "shell.execute_reply": "2025-11-28T20:27:12.998633Z",
     "shell.execute_reply.started": "2025-11-28T20:27:12.978794Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "import torch.nn.functional as F\n",
    "from torch import amp\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def compute_loss_for_suffix(\n",
    "    model,\n",
    "    emb_layer,\n",
    "    batch,\n",
    "    suffix_z,           # (Ls, V) nn.Parameter  == pre-softmax logits over vocab per suffix position\n",
    "    n_tokens=10,\n",
    "    amp_dtype=torch.float16,\n",
    "    cos_reg_weight=0.1,\n",
    "    E_norm_cpu=None,    # (V, E) on CPU, fp32\n",
    "    chunk_size=1024,\n",
    "    ent_reg_weight=1.0,\n",
    "    nt=1,\n",
    "):\n",
    "    \"\"\"\n",
    "    - For each example, build [prompt][suffix_z] in embedding space.\n",
    "    - Pad all to same length -> [prompt][suffix][PAD].\n",
    "    - Roll out n_tokens-1 tokens under inference_mode.\n",
    "    - Final forward WITH grad gives entropy loss on last generated token.\n",
    "    - Gradients flow into suffix_z only (prompts are detached).\n",
    "    - PLUS: regularizer that pulls suffix_z toward real token embeddings via cosine similarity.\n",
    "    \"\"\"\n",
    "    prompts = batch[\"input_ids\"]   # list of 1D LongTensors (Li,)\n",
    "    dev = emb_layer.weight.device\n",
    "    suffix_z = suffix_z.to(dev)    # (Ls, E)\n",
    "    suffix_logits = suffix_z.to(device=dev, dtype=torch.float32)  # keep logits in fp32 for stability\n",
    "    # Soft one-hot over vocab (fp32)\n",
    "    suffix_probs_fp32 = F.softmax(suffix_logits, dim=-1)  # (Ls, V)\n",
    "    # Suffix embeddings: convex combination of token embeddings\n",
    "    suffix_probs = suffix_probs_fp32.to(dtype=emb_layer.weight.dtype, device=dev)\n",
    "    suffix_embs = suffix_probs @ emb_layer.weight  # (Ls, E_dim), with grad wrt suffix_logits\n",
    "    Ls, V_logits = suffix_logits.shape\n",
    "    assert V_logits == E_norm_cpu.size(0), f\"suffix_z second dim ({V_logits}) must match vocab size ({E_norm_cpu.size(0)}).\"\n",
    "\n",
    "\n",
    "    B = len(prompts)\n",
    "    Ls, E = suffix_embs.shape\n",
    "    _, E_dim = emb_layer.weight.shape\n",
    "\n",
    "    base_embs = []   # each: (Li+Ls, E)\n",
    "    base_lens = []   # each: scalar length Li+Ls\n",
    "\n",
    "    # --- Build per-example [prompt][suffix] in embedding space ---\n",
    "    for p_ids in prompts:\n",
    "        p_ids_dev = p_ids.to(dev)\n",
    "        p_emb = emb_layer(p_ids_dev).detach()   # (Li, E), prompts are constants\n",
    "        base = torch.cat([p_emb, suffix_embs], dim=0)  # (Li+Ls, E)\n",
    "        base_embs.append(base)\n",
    "        base_lens.append(base.size(0))\n",
    "\n",
    "    # Pad to [prompt][suffix][PAD...] across the batch\n",
    "    base = pad_sequence(base_embs, batch_first=True)   # (B, max_len, E)\n",
    "    base_lens = torch.tensor(base_lens, device=dev)    # (B,)\n",
    "    max_len = base.size(1)\n",
    "\n",
    "    # Attention mask: 1 for real tokens, 0 for pad\n",
    "    arange = torch.arange(max_len, device=dev).unsqueeze(0)  # (1, max_len)\n",
    "    base_mask = (arange < base_lens.unsqueeze(1)).long()     # (B, max_len)\n",
    "\n",
    "    # Now base has structure [prompt][suffix][PAD] per row (masked pads)\n",
    "\n",
    "    def _one_step_logits(e, m):\n",
    "        with amp.autocast(\"cuda\", dtype=amp_dtype):\n",
    "            out = model(\n",
    "                inputs_embeds=e,\n",
    "                attention_mask=m,\n",
    "                use_cache=False,\n",
    "                output_attentions=False,\n",
    "                output_hidden_states=False,\n",
    "                return_dict=True,\n",
    "            )\n",
    "        return out.logits[:, -1, :]  # (B, V)\n",
    "\n",
    "    # ---------- Rollout under no grad (from detached base) ----------\n",
    "    work_e = base.detach()  # rollout uses constants\n",
    "    work_m = base_mask\n",
    "    added_embs = []         # list of (B, E) constants\n",
    "    \n",
    "    T = max(0, n_tokens - 1)\n",
    "    with torch.inference_mode():\n",
    "        for _ in range(T):\n",
    "            logits_t = _one_step_logits(work_e, work_m)\n",
    "            probs_t  = torch.softmax(logits_t, dim=-1)\n",
    "            next_ids = torch.argmax(probs_t, dim=-1)        # (B,)\n",
    "    \n",
    "            next_emb = emb_layer(next_ids.to(dev)).detach() # (B, E)\n",
    "            added_embs.append(next_emb)\n",
    "    \n",
    "            work_e = torch.cat([work_e, next_emb.unsqueeze(1)], dim=1)\n",
    "            work_m = torch.cat(\n",
    "                [work_m, torch.ones((B, 1), dtype=work_m.dtype, device=dev)],\n",
    "                dim=1,\n",
    "            )\n",
    "    \n",
    "    # ---------- Final inputs: [prompt][suffix][PAD] + generated tokens ----------\n",
    "    if len(added_embs) > 0:\n",
    "        added = torch.stack(added_embs, dim=1)              # (B, T, E)\n",
    "        final_emb = torch.cat([base, added], dim=1)         # (B, max_len+T, E)\n",
    "        gen_mask = torch.ones((B, T), dtype=base_mask.dtype, device=dev)\n",
    "        final_mask = torch.cat([base_mask, gen_mask], dim=1)\n",
    "    else:\n",
    "        final_emb = base\n",
    "        final_mask = base_mask\n",
    "    \n",
    "    # ---------- Forward WITH grad for ALL n_tokens steps ----------\n",
    "    with amp.autocast(\"cuda\", dtype=amp_dtype):\n",
    "        out = model(\n",
    "            inputs_embeds=final_emb,\n",
    "            attention_mask=final_mask,\n",
    "            use_cache=False,\n",
    "            output_attentions=False,\n",
    "            output_hidden_states=False,\n",
    "            return_dict=True,\n",
    "        )\n",
    "    \n",
    "    logits_last = _one_step_logits(final_emb, final_mask)  # (B, n_tokens, V)\n",
    "    logits_all = out.logits   # (B, L_total, V)\n",
    "    B, L_total, V = logits_all.shape\n",
    "    \n",
    "    # base_lens: (B,) lengths of [prompt][suffix] BEFORE generated tokens\n",
    "    # we want logits for:\n",
    "    #   step 1: position base_len - 1  (first next token)\n",
    "    #   step 2: position base_len      (second next)\n",
    "    #   ...\n",
    "    #   step n_tokens: position base_len - 1 + (n_tokens - 1) = base_len + T - 1\n",
    "    # So indices: [base_len - 1, base_len, ..., base_len + T - 1], length = n_tokens\n",
    "    \n",
    "    all_step_logits = []\n",
    "    \n",
    "    for b in range(B):\n",
    "        blen = base_lens[b].item()  # length of base for this example\n",
    "    \n",
    "        # safety: don't go past sequence length\n",
    "        # we know we have exactly T generated tokens, so there are n_tokens positions:\n",
    "        # indices from blen-1 to blen-1+T (inclusive)\n",
    "        start_idx = blen - 1\n",
    "        end_idx   = blen - 1 + T    # inclusive\n",
    "        # this yields exactly n_tokens positions when T = n_tokens-1\n",
    "    \n",
    "        idxs = torch.arange(start_idx, end_idx + 1, device=dev)  # (n_tokens,)\n",
    "        # gather logits for this example's steps: (n_tokens, V)\n",
    "        step_logits_b = logits_all[b, idxs, :]                   # (n_tokens, V)\n",
    "        all_step_logits.append(step_logits_b)\n",
    "    \n",
    "    # stack over batch: (B, n_tokens, V) -> (B*n_tokens, V)\n",
    "    logits_for_loss = torch.cat(all_step_logits, dim=0)  # (B*n_tokens, V)\n",
    "\n",
    "    # print(f\"hey: {logits_for_loss.shape}\")\n",
    "    \n",
    "    # mean entropy over all n_tokens steps for all examples\n",
    "    ent = entropy_loss(logits_for_loss)\n",
    "\n",
    "    \n",
    "    dev = suffix_embs.device\n",
    "    Ls = suffix_embs.size(0)\n",
    "    V = E_norm_cpu.size(0)\n",
    "    \n",
    "    # normalized suffix embeddings on GPU, fp32, with grad\n",
    "    z_norm = F.normalize(suffix_embs.float(), dim=-1)  # (Ls, E)\n",
    "    \n",
    "    # running top-2 cosines across vocab, per suffix position\n",
    "    top2_vals = None  # (2, Ls)\n",
    "    \n",
    "    for start in range(0, V, chunk_size):\n",
    "        end = min(start + chunk_size, V)\n",
    "        # chunk: (c, E) fp32 on GPU, no grad\n",
    "        chunk = E_norm_cpu[start:end].to(dev, non_blocking=True)  # (c, E)\n",
    "    \n",
    "        # (c, E) @ (E, Ls) -> (c, Ls)\n",
    "        chunk_sim = torch.matmul(chunk, z_norm.T)  # (c, Ls)\n",
    "    \n",
    "        # top-2 within this chunk: (2, Ls)  (if c < 2, topk handles it)\n",
    "        chunk_top2, _ = chunk_sim.topk(\n",
    "            k=min(2, chunk_sim.size(0)), dim=0\n",
    "        )  # (k', Ls)\n",
    "    \n",
    "        if top2_vals is None:\n",
    "            # if first chunk smaller than 2, pad with very low values\n",
    "            if chunk_top2.size(0) < 2:\n",
    "                pad_rows = 2 - chunk_top2.size(0)\n",
    "                pad = torch.full(\n",
    "                    (pad_rows, Ls),\n",
    "                    -1e9,\n",
    "                    device=dev,\n",
    "                    dtype=chunk_top2.dtype,\n",
    "                )\n",
    "                top2_vals = torch.cat([chunk_top2, pad], dim=0)  # (2, Ls)\n",
    "            else:\n",
    "                top2_vals = chunk_top2  # (2, Ls)\n",
    "        else:\n",
    "            # combine global and chunk top2, then keep best 2 across all\n",
    "            combined = torch.cat([top2_vals, chunk_top2], dim=0)  # (2 + k', Ls)\n",
    "            top2_vals, _ = combined.topk(k=2, dim=0)              # (2, Ls)\n",
    "    \n",
    "        # free small temps\n",
    "        del chunk, chunk_sim, chunk_top2\n",
    "\n",
    "    # Now top2_vals[0] = global best cosine; top2_vals[1] = global second-best\n",
    "    top1 = top2_vals[0]   # (Ls,)\n",
    "    top2 = top2_vals[1]   # (Ls,)\n",
    "    \n",
    "    # margin per position: encourage top1 >> top2\n",
    "    margin_per_pos = top1 - top2  # (Ls,)\n",
    "    mean_margin = margin_per_pos.mean()            # scalar\n",
    "    \n",
    "    # We want to MAXIMIZE mean_margin → in minimization, use negative margin\n",
    "    margin_reg = -mean_margin\n",
    "    \n",
    "    cos_reg = margin_reg  # you can scale it directly with cos_reg_weight below\n",
    "    \n",
    "    ent_soft = entropy_loss(suffix_probs_fp32, is_logit=False)\n",
    "\n",
    "    total_loss = ent + cos_reg_weight * cos_reg + ent_reg_weight * ent_soft\n",
    "    \n",
    "    # ----- Suffix joint log-prob term (discrete argmax-based) -----\n",
    "    # We'll treat this as an extra constant term (no gradient wrt suffix_z)\n",
    "    base_loss = total_loss\n",
    "\n",
    "    # defaults in case batch is empty\n",
    "    suffix_joint_logprob = torch.tensor(0.0, device=dev)\n",
    "    suffix_logprob_term = torch.tensor(0.0, device=dev)\n",
    "    model_suffix_probs_cpu = None  # NEW: will hold model-assigned suffix probs (Ls, V) on CPU\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Discrete suffix tokens from current soft one-hot\n",
    "        suffix_token_ids = suffix_probs_fp32.argmax(dim=-1)  # (Ls,)\n",
    "\n",
    "        pad_id = getattr(tokenizer, \"pad_token_id\", 0)\n",
    "        # print(f'pad_id{pad_id}')\n",
    "\n",
    "        full_ids_list = []\n",
    "        prompt_lens = []\n",
    "        for p_ids in prompts:\n",
    "            p_ids_dev = p_ids.to(dev)\n",
    "            prompt_lens.append(p_ids_dev.size(0))\n",
    "            full_ids_list.append(torch.cat([p_ids_dev, suffix_token_ids.to(dev)], dim=0))\n",
    "\n",
    "        if len(full_ids_list) > 0:\n",
    "            full_ids = pad_sequence(\n",
    "                full_ids_list,\n",
    "                batch_first=True,\n",
    "                padding_value=pad_id,\n",
    "            ).to(dev)                                             # (B_ids, L_max_ids)\n",
    "            prompt_lens = torch.tensor(prompt_lens, device=dev)  # (B_ids,)\n",
    "            B_ids, L_max_ids = full_ids.size()\n",
    "\n",
    "            # attention mask for [prompt][suffix] region\n",
    "            full_lengths = prompt_lens + Ls\n",
    "            arange_ids = torch.arange(L_max_ids, device=dev).unsqueeze(0)\n",
    "            attn_ids = (arange_ids < full_lengths.unsqueeze(1)).long()\n",
    "\n",
    "            from torch import amp as _amp_mod  # to avoid ambiguity if needed\n",
    "\n",
    "            with _amp_mod.autocast(\"cuda\", dtype=amp_dtype):\n",
    "                outputs_suffix = model(\n",
    "                    input_ids=full_ids,\n",
    "                    attention_mask=attn_ids,\n",
    "                    use_cache=False,\n",
    "                    output_attentions=False,\n",
    "                    output_hidden_states=False,\n",
    "                    return_dict=True,\n",
    "                )\n",
    "\n",
    "            logits_suffix = outputs_suffix.logits                 # (B_ids, L_max_ids, V)\n",
    "            log_probs_suffix = F.log_softmax(logits_suffix, dim=-1)\n",
    "\n",
    "            # positions of suffix tokens in full_ids and corresponding logits\n",
    "            positions = prompt_lens.unsqueeze(1) + torch.arange(Ls, device=dev).unsqueeze(0)  # (B_ids, Ls)\n",
    "            positions_logits = (positions - 1).clamp(min=0, max=L_max_ids - 1)                # (B_ids, Ls)\n",
    "\n",
    "            token_ids_expand = suffix_token_ids.unsqueeze(0).expand(B_ids, -1)                # (B_ids, Ls)\n",
    "\n",
    "            # FULL model-assigned suffix distributions over vocab at each suffix position\n",
    "            # pick the distribution at the timestep where each suffix token is predicted\n",
    "            V = logits_suffix.size(-1)\n",
    "            model_suffix_log_probs = log_probs_suffix.gather(\n",
    "                dim=1,\n",
    "                index=positions_logits.unsqueeze(-1).expand(-1, -1, V),\n",
    "            )  # (B_ids, Ls, V)\n",
    "\n",
    "            model_suffix_probs = model_suffix_log_probs.exp()        # (B_ids, Ls, V)\n",
    "            # average across batch -> (Ls, V), one distribution per suffix position\n",
    "            model_suffix_probs_mean = model_suffix_probs.mean(dim=0)  # (Ls, V)\n",
    "            # stash on CPU for later feature extraction\n",
    "            model_suffix_probs_cpu = model_suffix_probs_mean.detach().cpu()\n",
    "\n",
    "            # log p(s_j | prompt + s_<j) for each batch, each suffix position\n",
    "            suffix_token_logprobs = log_probs_suffix[\n",
    "                torch.arange(B_ids, device=dev).unsqueeze(1),\n",
    "                positions_logits,\n",
    "                token_ids_expand,\n",
    "            ]  # (B_ids, Ls)\n",
    "\n",
    "\n",
    "            # schedule weights along suffix positions\n",
    "            nt_sched = min(Ls, nt)          # \"nt\": first nt suffix positions\n",
    "            weights = torch.ones(Ls, device=dev)  # default weight = 1.0\n",
    "            if nt_sched > 0:\n",
    "                # lower values at the start, gradually increasing up to 1.0\n",
    "                weight_schedule = torch.linspace(0.3, 1.0, steps=nt_sched, device=dev)\n",
    "                weights[:nt_sched] = weight_schedule\n",
    "\n",
    "            # apply weights and average over batch\n",
    "            weighted_logprobs = (suffix_token_logprobs * weights.unsqueeze(0)).sum(dim=-1)  # (B_ids,)\n",
    "            suffix_joint_logprob = weighted_logprobs.mean()                                 # scalar\n",
    "\n",
    "            # negative-signed multiplier: encourages higher joint suffix log-prob\n",
    "            suffix_logprob_weight = 0.01\n",
    "            suffix_logprob_term = -suffix_logprob_weight * suffix_joint_logprob\n",
    "\n",
    "    # include the suffix joint log-prob term in the loss\n",
    "    total_loss = base_loss + suffix_logprob_term\n",
    "\n",
    "    # ---------- Derive characteristics (forward-only, store tensors on CPU) ----------\n",
    "    characteristics = {}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        eps = 1e-12\n",
    "\n",
    "        def _stat(t: torch.Tensor):\n",
    "            # 1D tensor stats\n",
    "            t = t.detach()\n",
    "            if t.numel() == 0:\n",
    "                return {\"mean\": 0.0, \"std\": 0.0, \"min\": 0.0, \"max\": 0.0}\n",
    "            mean = t.mean().item()\n",
    "            std = t.std(unbiased=False).item() if t.numel() > 1 else 0.0\n",
    "            return {\n",
    "                \"mean\": mean,\n",
    "                \"std\": std,\n",
    "                \"min\": t.min().item(),\n",
    "                \"max\": t.max().item(),\n",
    "            }\n",
    "\n",
    "        # ----- suffix_logits features -----\n",
    "        suffix_logits_f = suffix_logits.detach()            # (Ls, V)\n",
    "        logits_flat = suffix_logits_f.view(-1)\n",
    "        suffix_logits_cpu = suffix_logits_f.cpu()\n",
    "\n",
    "        suffix_logits_features = {\n",
    "            \"tensor\": suffix_logits_cpu,                    # (Ls, V) on CPU\n",
    "            \"flat_stats\": _stat(logits_flat),\n",
    "            \"flat_norm\": logits_flat.norm().item(),\n",
    "        }\n",
    "\n",
    "        # ----- suffix_probs features (row-wise + spectral) -----\n",
    "        suffix_probs_f = suffix_probs_fp32.detach()         # (Ls, V)\n",
    "        suffix_probs_cpu = suffix_probs_f.cpu()\n",
    "\n",
    "        logp_suffix = (suffix_probs_f + eps).log()          # (Ls, V)\n",
    "        row_entropy = -(suffix_probs_f * logp_suffix).sum(dim=-1)       # (Ls,)\n",
    "        row_maxprob, row_maxidx = suffix_probs_f.max(dim=-1)            # (Ls,)\n",
    "        row_l2 = suffix_probs_f.norm(dim=-1)                            # (Ls,)\n",
    "        row_gini = 1.0 - (suffix_probs_f ** 2).sum(dim=-1)              # (Ls,)\n",
    "        top2p, top2idx = suffix_probs_f.topk(2, dim=-1)\n",
    "        row_margin = top2p[:, 0] - top2p[:, 1]                          # (Ls,)\n",
    "\n",
    "        suffix_probs_features = {\n",
    "            \"tensor\": suffix_probs_cpu,                      # (Ls, V) on CPU\n",
    "            \"row_entropy\": {\n",
    "                \"tensor\": row_entropy.cpu(),                 # (Ls,)\n",
    "                \"stats\": _stat(row_entropy),\n",
    "            },\n",
    "            \"row_max_prob\": {\n",
    "                \"tensor\": row_maxprob.cpu(),                 # (Ls,)\n",
    "                \"stats\": _stat(row_maxprob),\n",
    "            },\n",
    "            \"row_max_idx\": row_maxidx.cpu(),                 # (Ls,)\n",
    "            \"row_l2\": {\n",
    "                \"tensor\": row_l2.cpu(),                      # (Ls,)\n",
    "                \"stats\": _stat(row_l2),\n",
    "            },\n",
    "            \"row_gini\": {\n",
    "                \"tensor\": row_gini.cpu(),                    # (Ls,)\n",
    "                \"stats\": _stat(row_gini),\n",
    "            },\n",
    "            \"row_margin\": {\n",
    "                \"tensor\": row_margin.cpu(),                  # (Ls,)\n",
    "                \"stats\": _stat(row_margin),\n",
    "            },\n",
    "            \"row_top2_probs\": top2p.cpu(),                   # (Ls, 2)\n",
    "            \"row_top2_idx\": top2idx.cpu(),                   # (Ls, 2)\n",
    "        }\n",
    "\n",
    "        # SVD of suffix_probs for low-rank structure (Ls x V)\n",
    "        try:\n",
    "            sv = torch.linalg.svdvals(suffix_probs_cpu)      # (min(Ls, V),)\n",
    "            sv_sorted = torch.sort(sv, descending=True).values\n",
    "            sv_sum = sv_sorted.sum().item()\n",
    "            if sv_sum > 0:\n",
    "                sigma1_ratio = (sv_sorted[0] / sv_sum).item()\n",
    "            else:\n",
    "                sigma1_ratio = 0.0\n",
    "            sigma1_over_sigma2 = (\n",
    "                (sv_sorted[0] / sv_sorted[1]).item()\n",
    "                if sv_sorted.numel() > 1 and sv_sorted[1].abs() > 0\n",
    "                else None\n",
    "            )\n",
    "            effective_rank = float(\n",
    "                torch.exp(\n",
    "                    -((sv_sorted / sv_sum) ** 2 * (sv_sorted / sv_sum).log()).sum()\n",
    "                ).item()\n",
    "            ) if sv_sum > 0 else 0.0\n",
    "            suffix_probs_features[\"sv\"] = {\n",
    "                \"singular_values\": sv_sorted.cpu(),          # full spectrum\n",
    "                \"sigma1_ratio\": sigma1_ratio,\n",
    "                \"sigma1_over_sigma2\": sigma1_over_sigma2,\n",
    "                \"effective_rank_proxy\": effective_rank,\n",
    "            }\n",
    "        except RuntimeError:\n",
    "            suffix_probs_features[\"sv\"] = {\n",
    "                \"singular_values\": torch.empty(0),\n",
    "                \"sigma1_ratio\": 0.0,\n",
    "                \"sigma1_over_sigma2\": None,\n",
    "                \"effective_rank_proxy\": 0.0,\n",
    "            }\n",
    "\n",
    "        # ----- model-assigned suffix probs (given prompts) -----\n",
    "        # model_suffix_probs_cpu was computed in the suffix-logprob block; it has shape (Ls, V)\n",
    "        suffix_probs_model_features = None\n",
    "        if model_suffix_probs_cpu is not None:\n",
    "            suffix_probs_model_f = model_suffix_probs_cpu.to(dtype=torch.float32)  # (Ls, V)\n",
    "\n",
    "            logp_suffix_model = (suffix_probs_model_f + eps).log()\n",
    "            row_entropy_model = -(suffix_probs_model_f * logp_suffix_model).sum(dim=-1)\n",
    "            row_maxprob_model, row_maxidx_model = suffix_probs_model_f.max(dim=-1)\n",
    "            row_l2_model = suffix_probs_model_f.norm(dim=-1)\n",
    "            row_gini_model = 1.0 - (suffix_probs_model_f ** 2).sum(dim=-1)\n",
    "            top2p_model, top2idx_model = suffix_probs_model_f.topk(2, dim=-1)\n",
    "            row_margin_model = top2p_model[:, 0] - top2p_model[:, 1]\n",
    "\n",
    "            suffix_probs_model_features = {\n",
    "                \"tensor\": model_suffix_probs_cpu,                      # (Ls, V)\n",
    "                \"row_entropy\": {\n",
    "                    \"tensor\": row_entropy_model.cpu(),\n",
    "                    \"stats\": _stat(row_entropy_model),\n",
    "                },\n",
    "                \"row_max_prob\": {\n",
    "                    \"tensor\": row_maxprob_model.cpu(),\n",
    "                    \"stats\": _stat(row_maxprob_model),\n",
    "                },\n",
    "                \"row_max_idx\": row_maxidx_model.cpu(),\n",
    "                \"row_l2\": {\n",
    "                    \"tensor\": row_l2_model.cpu(),\n",
    "                    \"stats\": _stat(row_l2_model),\n",
    "                },\n",
    "                \"row_gini\": {\n",
    "                    \"tensor\": row_gini_model.cpu(),\n",
    "                    \"stats\": _stat(row_gini_model),\n",
    "                },\n",
    "                \"row_margin\": {\n",
    "                    \"tensor\": row_margin_model.cpu(),\n",
    "                    \"stats\": _stat(row_margin_model),\n",
    "                },\n",
    "                \"row_top2_probs\": top2p_model.cpu(),\n",
    "                \"row_top2_idx\": top2idx_model.cpu(),\n",
    "            }\n",
    "\n",
    "            # SVD of model-assigned suffix probs for low-rank structure\n",
    "            try:\n",
    "                sv_m = torch.linalg.svdvals(suffix_probs_model_f)      # (min(Ls, V),)\n",
    "                sv_m_sorted = torch.sort(sv_m, descending=True).values\n",
    "                sv_m_sum = sv_m_sorted.sum().item()\n",
    "                if sv_m_sum > 0:\n",
    "                    sigma1_ratio_m = (sv_m_sorted[0] / sv_m_sum).item()\n",
    "                else:\n",
    "                    sigma1_ratio_m = 0.0\n",
    "                sigma1_over_sigma2_m = (\n",
    "                    (sv_m_sorted[0] / sv_m_sorted[1]).item()\n",
    "                    if sv_m_sorted.numel() > 1 and sv_m_sorted[1].abs() > 0\n",
    "                    else None\n",
    "                )\n",
    "                effective_rank_m = float(\n",
    "                    torch.exp(\n",
    "                        -((sv_m_sorted / sv_m_sum) ** 2 * (sv_m_sorted / sv_m_sum).log()).sum()\n",
    "                    ).item()\n",
    "                ) if sv_m_sum > 0 else 0.0\n",
    "                suffix_probs_model_features[\"sv\"] = {\n",
    "                    \"singular_values\": sv_m_sorted.cpu(),\n",
    "                    \"sigma1_ratio\": sigma1_ratio_m,\n",
    "                    \"sigma1_over_sigma2\": sigma1_over_sigma2_m,\n",
    "                    \"effective_rank_proxy\": effective_rank_m,\n",
    "                }\n",
    "            except RuntimeError:\n",
    "                suffix_probs_model_features[\"sv\"] = {\n",
    "                    \"singular_values\": torch.empty(0),\n",
    "                    \"sigma1_ratio\": 0.0,\n",
    "                    \"sigma1_over_sigma2\": None,\n",
    "                    \"effective_rank_proxy\": 0.0,\n",
    "                }\n",
    "\n",
    "            # attach to main suffix_probs_features dict\n",
    "            suffix_probs_features[\"model_assigned\"] = suffix_probs_model_features\n",
    "\n",
    "        # ----- suffix_embs features -----\n",
    "        E_f = emb_layer.weight.detach().float()                            # (V, E_dim)\n",
    "        suffix_embs_f = suffix_embs.detach().float()        # (Ls, E_dim)\n",
    "        suffix_embs_cpu = suffix_embs_f.cpu()\n",
    "\n",
    "        emb_row_norm = suffix_embs_f.norm(dim=-1)           # (Ls,)\n",
    "        emb_row_norm_stats = _stat(emb_row_norm)\n",
    "\n",
    "        # pairwise cosine between positions\n",
    "        if Ls > 1:\n",
    "            S_normed = F.normalize(suffix_embs_f, dim=-1)\n",
    "            cos_mat = S_normed @ S_normed.T                 # (Ls, Ls)\n",
    "            cos_vals = cos_mat[~torch.eye(Ls, dtype=torch.bool, device=cos_mat.device)]\n",
    "            cos_stats = _stat(cos_vals)\n",
    "            cos_mat_cpu = cos_mat.cpu()\n",
    "        else:\n",
    "            cos_stats = {\"mean\": 0.0, \"std\": 0.0, \"min\": 0.0, \"max\": 0.0}\n",
    "            cos_mat_cpu = torch.eye(Ls)\n",
    "\n",
    "        # distance to nearest vocab embedding for each suffix position\n",
    "        d_min_list = []\n",
    "        nearest_idx_list = []\n",
    "        for i in range(Ls):\n",
    "            diff_i = E_f - suffix_embs_f[i].unsqueeze(0)    # (V, E_dim)\n",
    "            dists_i = diff_i.pow(2).sum(dim=-1).sqrt()      # (V,)\n",
    "            dmin, idxmin = dists_i.min(dim=-1)\n",
    "            d_min_list.append(dmin)\n",
    "            nearest_idx_list.append(idxmin)\n",
    "        d_min = torch.stack(d_min_list, dim=0)              # (Ls,)\n",
    "        nearest_idx = torch.stack(nearest_idx_list, dim=0)  # (Ls,)\n",
    "\n",
    "        # covariance in embedding space via Ls x Ls matrix (spectral)\n",
    "        if Ls > 1:\n",
    "            S_c = suffix_embs_f - suffix_embs_f.mean(dim=0, keepdim=True)  # (Ls, d)\n",
    "            M = (S_c @ S_c.T) / (Ls - 1)                                   # (Ls, Ls)\n",
    "            try:\n",
    "                ev = torch.linalg.eigvalsh(M.cpu())                        # (Ls,) ascending\n",
    "                ev_sorted = torch.sort(ev, descending=True).values\n",
    "                ev_sum = ev_sorted.clamp_min(0).sum().item()\n",
    "                if ev_sum > 0:\n",
    "                    emb_lambda1_ratio = (ev_sorted[0] / ev_sum).item()\n",
    "                else:\n",
    "                    emb_lambda1_ratio = 0.0\n",
    "                cov_eigs_cpu = ev_sorted.cpu()\n",
    "            except RuntimeError:\n",
    "                emb_lambda1_ratio = 0.0\n",
    "                cov_eigs_cpu = torch.empty(0)\n",
    "        else:\n",
    "            emb_lambda1_ratio = 0.0\n",
    "            cov_eigs_cpu = torch.empty(0)\n",
    "\n",
    "        suffix_embs_features = {\n",
    "            \"tensor\": suffix_embs_cpu,                          # (Ls, E_dim) on CPU\n",
    "            \"row_norm\": {\n",
    "                \"tensor\": emb_row_norm.cpu(),                  # (Ls,)\n",
    "                \"stats\": emb_row_norm_stats,\n",
    "            },\n",
    "            \"pairwise_cos\": {\n",
    "                \"matrix\": cos_mat_cpu,                         # (Ls, Ls)\n",
    "                \"stats\": cos_stats,\n",
    "            },\n",
    "            \"nearest_token_dist\": {\n",
    "                \"tensor\": d_min.cpu(),                         # (Ls,)\n",
    "                \"stats\": _stat(d_min),\n",
    "            },\n",
    "            \"nearest_token_idx\": nearest_idx.cpu(),            # (Ls,)\n",
    "            \"cov_eigvals\": cov_eigs_cpu,                       # full eigen spectrum (Ls,) or empty\n",
    "            \"cov_lambda1_ratio\": emb_lambda1_ratio,\n",
    "        }\n",
    "\n",
    "        # ----- output logits/probs features (from logits_last) -----\n",
    "        logits_last_f = logits_last.detach().float()          # (B, n_tokens, V)\n",
    "        B_cur, T_cur, V_cur = logits_last_f.shape\n",
    "\n",
    "        if B_cur > 0 and T_cur > 0:\n",
    "            L_out = logits_last_f.view(B_cur * T_cur, V_cur)  # (BT, V)\n",
    "            P_out = F.softmax(L_out, dim=-1)\n",
    "            logP_out = (P_out + eps).log()\n",
    "\n",
    "            H_out = -(P_out * logP_out).sum(dim=-1)           # (BT,)\n",
    "            maxP_out, maxidx_out = P_out.max(dim=-1)          # (BT,)\n",
    "            top2_out, top2idx_out = P_out.topk(2, dim=-1)\n",
    "            margin_out = top2_out[:, 0] - top2_out[:, 1]      # (BT,)\n",
    "            logits_norm_out = L_out.norm(dim=-1)              # (BT,)\n",
    "            probs_l2_out = P_out.norm(dim=-1)                 # (BT,)\n",
    "\n",
    "            # Store tensors on CPU\n",
    "            H_out_cpu = H_out.cpu()\n",
    "            maxP_out_cpu = maxP_out.cpu()\n",
    "            margin_out_cpu = margin_out.cpu()\n",
    "            logits_norm_out_cpu = logits_norm_out.cpu()\n",
    "            probs_l2_out_cpu = probs_l2_out.cpu()\n",
    "\n",
    "            # spectral structure over vocab for output probs\n",
    "            try:\n",
    "                sv_out = torch.linalg.svdvals(P_out.cpu())     # (min(BT, V),)\n",
    "                sv_out_sorted = torch.sort(sv_out, descending=True).values\n",
    "                sv_out_sum = sv_out_sorted.sum().item()\n",
    "                if sv_out_sum > 0:\n",
    "                    out_sigma1_ratio = (sv_out_sorted[0] / sv_out_sum).item()\n",
    "                else:\n",
    "                    out_sigma1_ratio = 0.0\n",
    "                out_sigma1_over_sigma2 = (\n",
    "                    (sv_out_sorted[0] / sv_out_sorted[1]).item()\n",
    "                    if sv_out_sorted.numel() > 1 and sv_out_sorted[1].abs() > 0\n",
    "                    else None\n",
    "                )\n",
    "                out_effective_rank = float(\n",
    "                    torch.exp(\n",
    "                        -((sv_out_sorted / sv_out_sum) ** 2 * (sv_out_sorted / sv_out_sum).log()).sum()\n",
    "                    ).item()\n",
    "                ) if sv_out_sum > 0 else 0.0\n",
    "                sv_out_cpu = sv_out_sorted.cpu()\n",
    "            except RuntimeError:\n",
    "                sv_out_cpu = torch.empty(0)\n",
    "                out_sigma1_ratio = 0.0\n",
    "                out_sigma1_over_sigma2 = None\n",
    "                out_effective_rank = 0.0\n",
    "\n",
    "            output_features = {\n",
    "                \"logits_last\": logits_last_f.cpu(),           # (B, n_tokens, V)\n",
    "                \"row_entropy\": {\n",
    "                    \"tensor\": H_out_cpu,\n",
    "                    \"stats\": _stat(H_out),\n",
    "                },\n",
    "                \"row_max_prob\": {\n",
    "                    \"tensor\": maxP_out_cpu,\n",
    "                    \"stats\": _stat(maxP_out),\n",
    "                },\n",
    "                \"row_margin\": {\n",
    "                    \"tensor\": margin_out_cpu,\n",
    "                    \"stats\": _stat(margin_out),\n",
    "                },\n",
    "                \"row_logits_norm\": {\n",
    "                    \"tensor\": logits_norm_out_cpu,\n",
    "                    \"stats\": _stat(logits_norm_out),\n",
    "                },\n",
    "                \"row_probs_l2\": {\n",
    "                    \"tensor\": probs_l2_out_cpu,\n",
    "                    \"stats\": _stat(probs_l2_out),\n",
    "                },\n",
    "                \"sv\": {\n",
    "                    \"singular_values\": sv_out_cpu,\n",
    "                    \"sigma1_ratio\": out_sigma1_ratio,\n",
    "                    \"sigma1_over_sigma2\": out_sigma1_over_sigma2,\n",
    "                    \"effective_rank_proxy\": out_effective_rank,\n",
    "                },\n",
    "            }\n",
    "        else:\n",
    "            output_features = {}\n",
    "\n",
    "        # assemble all characteristics\n",
    "        characteristics = {\n",
    "            \"batch_shape\": {\n",
    "                \"B\": int(B),\n",
    "                \"Ls\": int(Ls),\n",
    "                \"n_tokens\": int(n_tokens),\n",
    "                \"vocab_size\": int(V),\n",
    "                \"emb_dim\": int(E_dim),\n",
    "            },\n",
    "            \"loss_components\": {\n",
    "                \"entropy_main\": float(ent.item()),\n",
    "                \"entropy_suffix_soft\": float(ent_soft.item()),\n",
    "                \"suffix_joint_logprob_weighted\": float(suffix_joint_logprob.item()),\n",
    "                \"suffix_logprob_term\": float(suffix_logprob_term.item()),\n",
    "                \"total_loss\": float(total_loss.item()),\n",
    "            },\n",
    "            \"suffix_logits\": suffix_logits_features,\n",
    "            \"suffix_probs\": suffix_probs_features,\n",
    "            \"suffix_embs\": suffix_embs_features,\n",
    "            \"output\": output_features,\n",
    "        }\n",
    "\n",
    "    # ---- GPU cleanup: drop large intermediates on device ----\n",
    "    if dev.type == \"cuda\":\n",
    "        # clear gradients stored on model parameters (suffix_z.grad is kept)\n",
    "        for p in model.parameters():\n",
    "            p.grad = None\n",
    "\n",
    "        try:\n",
    "            del base_embs       # list of (Li+Ls, E_dim) on dev\n",
    "            del base            # (B, max_len, E_dim)\n",
    "            del base_lens       # (B,)\n",
    "            del arange          # (1, max_len)\n",
    "            del base_mask       # (B, max_len)\n",
    "\n",
    "            # may exist only if T > 0; guarded by try/except\n",
    "            del added           # (B, T, E_dim)\n",
    "            del gen_mask        # (B, T)\n",
    "\n",
    "            # final forward inputs and outputs on GPU\n",
    "            del final_emb       # (B, max_len+T, E_dim)\n",
    "            del final_mask      # (B, max_len+T)\n",
    "            del logits_last     # (B, n_tokens, V)\n",
    "            del suffix_probs    # (Ls, V) on dev\n",
    "        except NameError:\n",
    "            pass\n",
    "\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # Return both the scalar loss and the rich characteristics dict\n",
    "    return total_loss, characteristics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f8d079be-6203-44ae-b3c3-6f954dd8b7bf",
    "_uuid": "bfff2ac1-e4e7-4261-b7dd-158886637fe9",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "### Save Suffix Embedds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "2731d816-58a4-4a87-ba5b-6585628250a6",
    "_uuid": "35c2cb80-fd14-4593-a49c-9a8094825079",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-28T20:27:13.000311Z",
     "iopub.status.busy": "2025-11-28T20:27:13.000045Z",
     "iopub.status.idle": "2025-11-28T20:27:13.033520Z",
     "shell.execute_reply": "2025-11-28T20:27:13.032755Z",
     "shell.execute_reply.started": "2025-11-28T20:27:13.000288Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "def save_suffix_embeddings(suffix_z, epoch, round_idx):\n",
    "    \"\"\"\n",
    "    Save optimized suffix embeddings for tracking exploration across rounds/epochs.\n",
    "    \"\"\"\n",
    "    save_dir = \"/kaggle/working/suffix_saves\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    file_path = os.path.join(save_dir, f\"suffix_r{round_idx}_e{epoch}.pt\")\n",
    "    torch.save(suffix_z.detach().cpu(), file_path)\n",
    "\n",
    "    print(f\"Saved suffix for round {round_idx}, epoch {epoch} → {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projection + Diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def project_suffix_to_tokens_and_diagnostics(\n",
    "    suffix_z,\n",
    "    emb_layer,\n",
    "    tokenizer,\n",
    "    model,\n",
    "    prompt_input_ids,\n",
    "    num_gen_tokens: int = 10,\n",
    "    print_flag = False\n",
    "):\n",
    "    \"\"\"\n",
    "    suffix_z: (Ls, V) - pre-softmax logits over vocab for each suffix position\n",
    "    emb_layer: model.get_input_embeddings()\n",
    "    tokenizer: HF tokenizer\n",
    "    model: HF causal LM (already on device)\n",
    "    prompt_input_ids: 1D LongTensor or list[int], tokenized prompt\n",
    "    num_gen_tokens: m, number of tokens to generate after prompt+suffix\n",
    "\n",
    "    Returns a dict with ids and log-probs for suffix and generated tokens.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # -----------------------------\n",
    "        # 1) Interpret pre-softmax suffix_z as logits over vocab\n",
    "        #    and compute diagnostics on the resulting soft one-hots\n",
    "        # -----------------------------\n",
    "        dev = emb_layer.weight.device\n",
    "        E = emb_layer.weight        # (V, E_dim)\n",
    "        V, E_dim = E.shape\n",
    "\n",
    "        # suffix_z: (Ls, V) logits\n",
    "        suffix_logits = suffix_z.to(dev, dtype=torch.float32)  # (Ls, V)\n",
    "        Ls, V_logits = suffix_logits.shape\n",
    "        assert V_logits == V, f\"suffix_z second dim ({V_logits}) must match vocab size ({V}).\"\n",
    "\n",
    "        # Soft one-hot over vocab\n",
    "        suffix_probs = F.softmax(suffix_logits, dim=-1)        # (Ls, V)\n",
    "\n",
    "        # Diagnostics: max vocab prob per suffix position\n",
    "        max_probs_per_pos, best_token_ids = suffix_probs.max(dim=-1)  # (Ls,), (Ls,)\n",
    "\n",
    "        if print_flag:\n",
    "            print(\"Per-position max vocab probabilities for suffix distributions:\")\n",
    "            print(f\"  min max-p:  {max_probs_per_pos.min().item():.6f}\")\n",
    "            print(f\"  max max-p:  {max_probs_per_pos.max().item():.6f}\")\n",
    "            print(f\"  mean max-p: {max_probs_per_pos.mean().item():.66f}\")\n",
    "\n",
    "        # Discrete suffix tokens from argmax\n",
    "        suffix_token_ids = best_token_ids.cpu()  # (Ls,)\n",
    "        suffix_tokens = tokenizer.convert_ids_to_tokens(suffix_token_ids.tolist())\n",
    "        suffix_text = tokenizer.decode(\n",
    "            suffix_token_ids.tolist(),\n",
    "            skip_special_tokens=False\n",
    "        )\n",
    "\n",
    "        if print_flag:\n",
    "            print(\"\\nProjected discrete suffix token IDs (argmax over soft one-hot):\", suffix_token_ids.tolist())\n",
    "            print(\"Projected discrete suffix tokens:\", suffix_tokens)\n",
    "            print(\"Projected suffix as text:\", repr(suffix_text))\n",
    "\n",
    "        # -----------------------------\n",
    "        # 2) Build full input: prompt + suffix (discrete ids)\n",
    "        # -----------------------------\n",
    "        if isinstance(prompt_input_ids, torch.Tensor):\n",
    "            prompt_ids = prompt_input_ids.to(dev).view(-1)\n",
    "        else:\n",
    "            prompt_ids = torch.tensor(prompt_input_ids, device=dev, dtype=torch.long)\n",
    "\n",
    "        prompt_len = prompt_ids.size(0)\n",
    "        suffix_ids_dev = suffix_token_ids.to(dev)\n",
    "\n",
    "        full_input_ids = torch.cat([prompt_ids, suffix_ids_dev], dim=0)  # (T + Ls,)\n",
    "        full_input_ids_batch = full_input_ids.unsqueeze(0)               # (1, T + Ls)\n",
    "\n",
    "        # For reporting:\n",
    "        prompt_text = tokenizer.decode(prompt_ids.tolist(), skip_special_tokens=False)\n",
    "        \n",
    "        if print_flag:\n",
    "            print(\"\\nPrompt text:\", repr(prompt_text))\n",
    "\n",
    "        # -----------------------------\n",
    "        # 3) Probabilities for suffix tokens given the prompt\n",
    "        #    p(s_i | prompt + s_<i)\n",
    "        # -----------------------------\n",
    "        outputs = model(input_ids=full_input_ids_batch)\n",
    "        logits = outputs.logits  # (1, L_total, V)\n",
    "        log_probs = logits.log_softmax(dim=-1)  # (1, L_total, V)\n",
    "\n",
    "        suffix_token_logprobs = []\n",
    "        \n",
    "        if print_flag:\n",
    "            print(\"\\nSuffix token probabilities given prompt:\")\n",
    "\n",
    "        # HuggingFace causal LM: logits[:, i, :] predict token at position i+1\n",
    "        for i, tok_id in enumerate(suffix_token_ids.tolist()):\n",
    "            # Position of this suffix token in the full sequence\n",
    "            pos = prompt_len + i  # index of token in full_input_ids\n",
    "\n",
    "            if pos == 0:\n",
    "                # Can't compute prob for very first token (no previous context)\n",
    "                lp = float(\"nan\")\n",
    "                p = float(\"nan\")\n",
    "            else:\n",
    "                lp_tensor = log_probs[0, pos - 1, tok_id]  # log p(token at pos)\n",
    "                lp = lp_tensor.item()\n",
    "                p = lp_tensor.exp().item()\n",
    "\n",
    "            suffix_token_logprobs.append(lp)\n",
    "            tok_str = suffix_tokens[i]\n",
    "            if print_flag:\n",
    "                print(\n",
    "                    f\"  suffix pos {i} (abs_pos={pos}, id={tok_id}, token={repr(tok_str)}): \"\n",
    "                    f\"p = {p:.6e}, log p = {lp:.6f}\"\n",
    "                )\n",
    "\n",
    "        suffix_token_logprobs = torch.tensor(suffix_token_logprobs, dtype=torch.float32)\n",
    "        suffix_joint_logprob = torch.nan_to_num(suffix_token_logprobs).sum().item()\n",
    "        \n",
    "        if print_flag:\n",
    "            print(f\"\\nJoint log-prob of suffix given prompt: {suffix_joint_logprob:.6f}\")\n",
    "\n",
    "        # -----------------------------\n",
    "        # 4) Auto-regressively generate num_gen_tokens more tokens\n",
    "        #    and record probabilities of each generated token.\n",
    "        # -----------------------------\n",
    "        current_input_ids = full_input_ids.clone()  # (T + Ls,)\n",
    "\n",
    "        generated_token_ids = []\n",
    "        generated_token_logprobs = []\n",
    "        generated_tokens_str = []\n",
    "\n",
    "        for step in range(num_gen_tokens):\n",
    "            inp_batch = current_input_ids.unsqueeze(0)  # (1, L_cur)\n",
    "            out = model(input_ids=inp_batch)\n",
    "            next_logits = out.logits[:, -1, :]          # (1, V)\n",
    "            next_log_probs = next_logits.log_softmax(dim=-1)  # (1, V)\n",
    "\n",
    "            # Greedy: pick argmax\n",
    "            next_log_prob_val, next_token_id = next_log_probs.squeeze(0).max(dim=-1)\n",
    "            next_id = next_token_id.item()\n",
    "            lp = next_log_prob_val.item()\n",
    "            p = next_log_prob_val.exp().item()\n",
    "\n",
    "            generated_token_ids.append(next_id)\n",
    "            generated_token_logprobs.append(lp)\n",
    "\n",
    "            tok_str = tokenizer.convert_ids_to_tokens([next_id])[0]\n",
    "            generated_tokens_str.append(tok_str)\n",
    "\n",
    "            # Append to context\n",
    "            current_input_ids = torch.cat(\n",
    "                [current_input_ids, next_token_id.unsqueeze(0)],\n",
    "                dim=0\n",
    "            )\n",
    "            \n",
    "            if print_flag:\n",
    "                print(\n",
    "                    f\"Generated token {step} (abs_pos={current_input_ids.size(0)-1}, \"\n",
    "                    f\"id={next_id}, token={repr(tok_str)}): p = {p:.6e}, log p = {lp:.6f}\"\n",
    "                )\n",
    "\n",
    "        generated_token_ids = torch.tensor(generated_token_ids, dtype=torch.long)\n",
    "        generated_token_logprobs = torch.tensor(generated_token_logprobs, dtype=torch.float32)\n",
    "        gen_joint_logprob = generated_token_logprobs.sum().item()\n",
    "\n",
    "        if print_flag:\n",
    "            print(f\"\\nJoint log-prob of generated tokens: {gen_joint_logprob:.6f}\")\n",
    "\n",
    "        # -----------------------------\n",
    "        # 5) Decode full text: prompt + suffix + generated\n",
    "        # -----------------------------\n",
    "        full_with_gen_ids = current_input_ids  # (T + Ls + num_gen_tokens,)\n",
    "        full_text = tokenizer.decode(full_with_gen_ids.tolist(), skip_special_tokens=False)\n",
    "        gen_text = tokenizer.decode(generated_token_ids.tolist(), skip_special_tokens=False)\n",
    "\n",
    "        if print_flag:\n",
    "            print(\"\\nGenerated continuation text:\", repr(gen_text))\n",
    "            print(\"\\nFull text (prompt + suffix + generated):\")\n",
    "            print(repr(full_text))\n",
    "\n",
    "        # -----------------------------\n",
    "        # 6) Return structured info\n",
    "        # -----------------------------\n",
    "        result = {\n",
    "            \"suffix_token_ids\": suffix_token_ids,                     # (Ls,) on CPU\n",
    "            \"suffix_tokens\": suffix_tokens,\n",
    "            \"suffix_token_logprobs\": suffix_token_logprobs,           # (Ls,)\n",
    "            \"suffix_joint_logprob\": suffix_joint_logprob,\n",
    "            \"generated_token_ids\": generated_token_ids,               # (num_gen_tokens,)\n",
    "            \"generated_tokens\": generated_tokens_str,\n",
    "            \"generated_token_logprobs\": generated_token_logprobs,     # (num_gen_tokens,)\n",
    "            \"generated_joint_logprob\": gen_joint_logprob,\n",
    "            \"prompt_input_ids\": prompt_ids.cpu(),\n",
    "            \"full_input_ids_with_suffix\": full_input_ids.cpu(),\n",
    "            \"full_input_ids_with_suffix_and_generated\": full_with_gen_ids.cpu(),\n",
    "            \"prompt_text\": prompt_text,\n",
    "            \"suffix_text\": suffix_text,\n",
    "            \"generated_text\": gen_text,\n",
    "            \"full_text\": full_text,\n",
    "            \"suffix_max_probs_per_pos\": max_probs_per_pos.cpu(),      # (Ls,)\n",
    "        }\n",
    "\n",
    "    # ---- GPU cleanup (only dev tensors, keep CPU copies in `result`) ----\n",
    "    for p in model.parameters():\n",
    "        p.grad = None\n",
    "\n",
    "    try:\n",
    "        # suffix-related\n",
    "        del E, suffix_logits, suffix_probs, best_token_ids, max_probs_per_pos\n",
    "\n",
    "        # sequence tensors on dev (CPU clones are in `result`)\n",
    "        del prompt_ids, suffix_ids_dev, full_input_ids, full_input_ids_batch\n",
    "        del current_input_ids, full_with_gen_ids\n",
    "\n",
    "        # forward-pass outputs on dev\n",
    "        del outputs, logits, log_probs\n",
    "        del out, inp_batch, next_logits, next_log_probs\n",
    "        del next_token_id, next_log_prob_val\n",
    "    except NameError:\n",
    "        pass\n",
    "\n",
    "    if dev.type == \"cuda\":\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "16536c9a-f6ac-485b-b53a-3c8f8c84aa40",
    "_uuid": "28c11a2c-9aed-4263-b4c4-fa12d937cee4",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "### Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "45ac23b2-6547-4378-ab9e-d3c858a383bf",
    "_uuid": "42a1c011-1f81-49bc-aa0c-fd5314c1ac49",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-28T20:27:13.034474Z",
     "iopub.status.busy": "2025-11-28T20:27:13.034261Z",
     "iopub.status.idle": "2025-11-28T20:27:13.060069Z",
     "shell.execute_reply": "2025-11-28T20:27:13.059254Z",
     "shell.execute_reply.started": "2025-11-28T20:27:13.034451Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "def get_suffix_init(base_seed, round_num, suffix_len, V, dev='cuda'):\n",
    "    # Set all random seeds\n",
    "    round_seed = base_seed + round_num\n",
    "    \n",
    "    # For full reproducibility\n",
    "    torch.manual_seed(round_seed)\n",
    "    if dev == 'cuda':\n",
    "        torch.cuda.manual_seed_all(round_seed)\n",
    "    \n",
    "    # Enable deterministic mode\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    # Create generator with explicit seed\n",
    "    g = torch.Generator(device=dev)\n",
    "    g.manual_seed(round_seed)\n",
    "    \n",
    "    # Generate tensor\n",
    "    suffix_init = torch.randn(suffix_len, V, device=dev, generator=g)\n",
    "    \n",
    "    return suffix_init\n",
    "\n",
    "def optimize_suffix_embeddings(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    dataloader_args,\n",
    "    suffix_len=10,\n",
    "    n_tokens_rollout=10,\n",
    "    epochs=5,\n",
    "    init_lr=1e-2,\n",
    "    rounds=10,\n",
    "    amp_dtype=torch.float16,\n",
    "    print_interval=10,\n",
    "    base_seed: int = 1234,  # <- same across different runs (clean / poison) for same inits\n",
    "):\n",
    "    \"\"\"\n",
    "    Optimize a shared suffix over vocab via pre-softmax logits.\n",
    "\n",
    "    Now:\n",
    "      suffix_z: (Ls, V) pre-softmax logits over vocab per suffix position\n",
    "      emb_layer: embedding layer (for projection)\n",
    "\n",
    "    Seeding behavior:\n",
    "      - For each round r, we derive a seed = base_seed + r.\n",
    "      - Using a local torch.Generator makes the suffix_z init:\n",
    "          * the same for a given r across different runs (same base_seed),\n",
    "          * different across rounds within one run.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    model.eval()\n",
    "    if hasattr(model.config, \"use_cache\"):\n",
    "        model.config.use_cache = False\n",
    "\n",
    "    emb_layer = model.get_input_embeddings()\n",
    "    dev = emb_layer.weight.device\n",
    "    V = emb_layer.weight.size(0)   # vocab size\n",
    "\n",
    "    best_suffix_z = None\n",
    "    best_loss = float(\"inf\")\n",
    "\n",
    "    characteristics = {}\n",
    "\n",
    "    E_norm_cpu = F.normalize(emb_layer.weight.float(), dim=-1).cpu()  # (V, E) on CPU\n",
    "\n",
    "    for round in range(rounds):\n",
    "        print(f\"\\n=== Optimization Round {round+1}/{rounds} ===\")\n",
    "\n",
    "        # --- Deterministic per-round initialization ---\n",
    "        if base_seed is not None:\n",
    "            suffix_init = get_suffix_init(base_seed, round, suffix_len, V, dev=dev)\n",
    "        else:\n",
    "            # Fallback to global RNG if you ever want non-deterministic behavior\n",
    "            suffix_init = torch.randn(suffix_len, V, device=dev)\n",
    "\n",
    "        suffix_z = torch.nn.Parameter(0.01 * suffix_init)\n",
    "        print(f\"dtype of suffix_z: {suffix_z.dtype}, shape: {tuple(suffix_z.shape)}\")\n",
    "\n",
    "        optimizer = AdamW([suffix_z], lr=init_lr)\n",
    "        scheduler = StepLR(optimizer, step_size=5, gamma=0.5)  # every epoch: lr *= 0.5\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            print(f\"\\n[Epoch {epoch+1}/{epochs}]\")\n",
    "            running_loss = 0.0\n",
    "\n",
    "            dataloader = load_prompts_unpadded(tokenizer, dataloader_args, seed=base_seed+epoch)\n",
    "\n",
    "            for batch_count, batch in enumerate(dataloader):\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "                loss, characteristic = compute_loss_for_suffix(\n",
    "                    model,\n",
    "                    emb_layer,\n",
    "                    batch,\n",
    "                    suffix_z,                    # (Ls, V) pre-softmax logits\n",
    "                    n_tokens=n_tokens_rollout,\n",
    "                    amp_dtype=amp_dtype,\n",
    "                    cos_reg_weight=0.2 * (round + 1) / rounds,\n",
    "                    E_norm_cpu=E_norm_cpu,\n",
    "                )\n",
    "\n",
    "                characteristics[f\"r{round}_e{epoch}_b{batch_count}\"] = characteristic\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "\n",
    "                if batch_count % 5 == 0 and batch_count > 0:\n",
    "                    avg = running_loss / batch_count\n",
    "                    print(\n",
    "                        f\"  batch {batch_count} out of {len(dataloader)}, \"\n",
    "                        f\"avg loss: {avg:.4f}\",\n",
    "                        end=\"\\r\",\n",
    "                    )\n",
    "\n",
    "            # Convert the entire DataLoader to a list for sampling a random example\n",
    "            dataset = list(dataloader)\n",
    "            if len(dataset) > 0:\n",
    "                random_index = random.randint(0, len(dataset) - 1)\n",
    "                random_sample = dataset[random_index]\n",
    "                print(f\"\\nSample Number {random_index}\")\n",
    "\n",
    "                prompt_input_ids = random_sample[\"input_ids\"][0]\n",
    "\n",
    "                results[f\"r{round}_e{epoch}\"] = project_suffix_to_tokens_and_diagnostics(\n",
    "                    suffix_z, emb_layer, tokenizer, model, prompt_input_ids,\n",
    "                    print_flag=epoch % print_interval == 0,\n",
    "                )\n",
    "\n",
    "            # batch_count is 0-based; number of batches = batch_count + 1 if loop ran\n",
    "            num_batches = max(1, batch_count + 1)\n",
    "            epoch_avg = running_loss / num_batches\n",
    "            print(f\"Epoch {epoch+1} mean loss: {epoch_avg:.4f}\")\n",
    "\n",
    "            scheduler.step()\n",
    "            save_suffix_embeddings(suffix_z, epoch, round)\n",
    "\n",
    "        if epoch_avg < best_loss:\n",
    "            best_loss = epoch_avg\n",
    "            best_suffix_z = suffix_z.detach().clone()\n",
    "\n",
    "        print(\n",
    "            f\"\\nOptimization finished for round {round+1}. \"\n",
    "            f\"Final suffix logits (detached): {best_suffix_z}\"\n",
    "        )\n",
    "\n",
    "    return best_suffix_z, emb_layer, characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def _convert_to_json_serializable(obj):\n",
    "    \"\"\"\n",
    "    Recursively convert:\n",
    "    - torch.Tensor → list (CPU)\n",
    "    - torch.device, dtype → str\n",
    "    - dicts / lists → recursive conversion\n",
    "    Everything else kept if JSON serializable.\n",
    "    \"\"\"\n",
    "    if isinstance(obj, torch.Tensor):\n",
    "        return obj.detach().cpu().tolist()\n",
    "\n",
    "    if isinstance(obj, (torch.device, torch.dtype)):\n",
    "        return str(obj)\n",
    "\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: _convert_to_json_serializable(v) for k, v in obj.items()}\n",
    "\n",
    "    if isinstance(obj, (list, tuple)):\n",
    "        return [_convert_to_json_serializable(x) for x in obj]\n",
    "\n",
    "    # For floats, ints, strings, bools, None → JSON handles them naturally\n",
    "    return obj\n",
    "\n",
    "\n",
    "def save_characteristics(characteristics: dict, save_path: str):\n",
    "    \"\"\"\n",
    "    Save the 'characteristics' dict produced during training.\n",
    "\n",
    "    Files saved in JSON format (human-readable + reloadable).\n",
    "    Automatically converts tensors to lists.\n",
    "\n",
    "    Args:\n",
    "        characteristics: dict with keys like \"r0_e2_b14\" mapping to per-batch statistics.\n",
    "        save_path: filepath ending in .json\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure directory exists\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "\n",
    "    # Add metadata (optional but very useful)\n",
    "    metadata = {\n",
    "        \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"num_entries\": len(characteristics),\n",
    "    }\n",
    "\n",
    "    # Convert everything to JSON-serializable form\n",
    "    serializable_dict = {\n",
    "        \"metadata\": metadata,\n",
    "        \"characteristics\": _convert_to_json_serializable(characteristics),\n",
    "    }\n",
    "\n",
    "    # Save\n",
    "    with open(save_path, \"w\") as f:\n",
    "        json.dump(serializable_dict, f, indent=2)\n",
    "\n",
    "    print(f\"Characteristics saved to: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "e8854ab5-9201-45d2-a889-a7d7c10ec1eb",
    "_uuid": "ca398184-f2a2-4d29-a5d1-4bc68f9b6c8f",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-28T20:27:13.084288Z",
     "iopub.status.busy": "2025-11-28T20:27:13.084111Z",
     "iopub.status.idle": "2025-11-28T20:27:38.084753Z",
     "shell.execute_reply": "2025-11-28T20:27:38.083629Z",
     "shell.execute_reply.started": "2025-11-28T20:27:13.084275Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Optimization Round 1/4 ===\n",
      "dtype of suffix_z: torch.float32, shape: (10, 32000)\n",
      "\n",
      "[Epoch 1/10]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "compute_loss_for_suffix.<locals>._one_step_logits() got an unexpected keyword argument 'n'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_144/3004634559.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mrounds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m suffix_z, emb_layer, characteristics = optimize_suffix_embeddings(\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_144/322474017.py\u001b[0m in \u001b[0;36moptimize_suffix_embeddings\u001b[0;34m(model, tokenizer, dataloader_args, suffix_len, n_tokens_rollout, epochs, init_lr, rounds, amp_dtype, print_interval, base_seed)\u001b[0m\n\u001b[1;32m     95\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_to_none\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m                 loss, characteristic = compute_loss_for_suffix(\n\u001b[0m\u001b[1;32m     98\u001b[0m                     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m                     \u001b[0memb_layer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_144/3737955559.py\u001b[0m in \u001b[0;36mcompute_loss_for_suffix\u001b[0;34m(model, emb_layer, batch, suffix_z, n_tokens, amp_dtype, cos_reg_weight, E_norm_cpu, chunk_size, ent_reg_weight, nt)\u001b[0m\n\u001b[1;32m    119\u001b[0m         )\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m     \u001b[0mlogits_last\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_one_step_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_tokens\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (B, n_tokens, V)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m     \u001b[0mlogits_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m   \u001b[0;31m# (B, L_total, V)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL_total\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: compute_loss_for_suffix.<locals>._one_step_logits() got an unexpected keyword argument 'n'"
     ]
    }
   ],
   "source": [
    "# 1. Dataloader with [prompt] only, no suffix, unpadded\n",
    "args = {\n",
    "    \"data_dir\": \"/kaggle/working/data\",\n",
    "    \"max_length\": 512,\n",
    "    \"batch_size\": 4,\n",
    "    \"sample_size\": 1024,\n",
    "}\n",
    "\n",
    "# 2. Optimize continuous suffix\n",
    "suffix_len = 10\n",
    "n_tokens_rollout = 5\n",
    "epochs = 10\n",
    "init_lr = 1e-2\n",
    "rounds = 4\n",
    "\n",
    "suffix_z, emb_layer, characteristics = optimize_suffix_embeddings(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    args,\n",
    "    suffix_len=suffix_len,\n",
    "    n_tokens_rollout=n_tokens_rollout,\n",
    "    epochs=epochs,\n",
    "    init_lr=init_lr,\n",
    "    rounds=rounds,\n",
    "    amp_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "# Save charachteristics to file\n",
    "save_characteristics(\n",
    "    characteristics,\n",
    "    save_path=\"/kaggle/working/characteristics.json\",\n",
    ")\n",
    "\n",
    "\n",
    "# # 3. Project to discrete tokens + diagnostics\n",
    "# suffix_token_ids = project_suffix_to_tokens_and_diagnostics(\n",
    "#     suffix_z,\n",
    "#     emb_layer,\n",
    "#     tokenizer,\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8363949,
     "sourceId": 13197689,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
