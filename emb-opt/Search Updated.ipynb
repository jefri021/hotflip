{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "85f2b851-1f43-4828-b7ce-c71913830a46",
    "_uuid": "d495e3a7-2077-4217-9179-6872835988d6",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## Optimize Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d0f4c093-c5fe-4fee-baa0-fe180de76b51",
    "_uuid": "b30be227-5c1c-4ef2-bdfa-740471278aec",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "In this notebook, we're aiming to optimize embeddings directly, regardless of their values being a valid token representetive or not. We then use a similarity matrix + softmax to estimate a distribution on possible tokens for the optimized embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ed542c25-736e-48c1-9647-5fafb5d8bd74",
    "_uuid": "e372dddc-79e4-4aa1-b235-e77a0acaf35e",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "### Check Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5c820030-63eb-485a-9321-e016cf431a2e",
    "_uuid": "006ae104-fbea-41e5-aef6-f6a359596c54",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T10:22:46.391944Z",
     "iopub.status.busy": "2025-12-03T10:22:46.391309Z",
     "iopub.status.idle": "2025-12-03T10:22:46.405236Z",
     "shell.execute_reply": "2025-12-03T10:22:46.404637Z",
     "shell.execute_reply.started": "2025-12-03T10:22:46.391919Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gdown\n",
    "import torch\n",
    "import json\n",
    "import logging\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import gc\n",
    "from typing import List, Union\n",
    "\n",
    "def clear_memory(keep_vars: Union[List[str], None] = None, verbose: bool = True):\n",
    "    \"\"\"\n",
    "    Clears memory while preserving specified variables.\n",
    "    Still clears GPU memory for all CUDA objects, including kept variables.\n",
    "    \n",
    "    Args:\n",
    "        keep_vars: List of variable names to preserve in memory (will still be cleared from GPU)\n",
    "        verbose: Whether to print memory clearing information\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(\"Starting memory clearing process...\")\n",
    "    \n",
    "    # Convert keep_vars to set for faster lookups\n",
    "    keep_set = set(keep_vars) if keep_vars else set()\n",
    "    \n",
    "    # First pass: Move kept CUDA variables to CPU\n",
    "    if torch.cuda.is_available():\n",
    "        for name, var in list(globals().items()):\n",
    "            if name in keep_set and isinstance(var, torch.Tensor) and var.is_cuda:\n",
    "                if verbose:\n",
    "                    print(f\"Moving kept tensor '{name}' to CPU\")\n",
    "                globals()[name] = var.cpu()\n",
    "    \n",
    "    # Clear Python garbage collector\n",
    "    gc.collect()\n",
    "    if verbose:\n",
    "        print(\"Ran Python garbage collection\")\n",
    "    \n",
    "    # Clear CUDA memory if available\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        if verbose:\n",
    "            print(\"Cleared CUDA cache\")\n",
    "            print(f\"Current CUDA memory allocated: {torch.cuda.memory_allocated()/1024**2:.2f} MB\")\n",
    "            print(f\"Current CUDA memory cached: {torch.cuda.memory_reserved()/1024**2:.2f} MB\")\n",
    "    \n",
    "    # Try to clear TensorFlow/Keras if available\n",
    "    try:\n",
    "        import tensorflow as tf\n",
    "        tf.keras.backend.clear_session()\n",
    "        if verbose:\n",
    "            print(\"Cleared TensorFlow/Keras session\")\n",
    "    except ImportError:\n",
    "        pass\n",
    "    \n",
    "    # Delete objects not in keep_vars\n",
    "    for name, var in list(globals().items()):\n",
    "        if not name.startswith('__') and name not in keep_set:\n",
    "            if isinstance(var, (torch.Tensor, torch.nn.Module)):\n",
    "                del globals()[name]\n",
    "                if verbose:\n",
    "                    print(f\"Deleted torch object: {name}\")\n",
    "            elif isinstance(var, list) and var and isinstance(var[0], torch.Tensor):\n",
    "                del globals()[name]\n",
    "                if verbose:\n",
    "                    print(f\"Deleted list of torch tensors: {name}\")\n",
    "    \n",
    "    # Final garbage collection\n",
    "    gc.collect()\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Memory clearing complete\")\n",
    "\n",
    "\n",
    "def _two_gpu_max_memory(headroom_gb=2):\n",
    "    \"\"\"\n",
    "    Reserve headroom so HF sharding MUST split across both 16GB T4s.\n",
    "    \"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        return None\n",
    "    n = torch.cuda.device_count()\n",
    "    cap = f\"{16 - headroom_gb}GiB\"  # e.g., \"14GiB\"\n",
    "    return {i: cap for i in range(n)}\n",
    "\n",
    "def _common_from_pretrained_kwargs():\n",
    "    \"\"\"\n",
    "    Settings that reduce both CPU and GPU peak memory and use a lean attention impl.\n",
    "    \"\"\"\n",
    "    kw = dict(\n",
    "        trust_remote_code=True,\n",
    "        local_files_only=True,\n",
    "        torch_dtype=torch.float16,     # T4 → FP16\n",
    "        low_cpu_mem_usage=True,        # streaming load\n",
    "        offload_state_dict=True,       # avoid CPU spikes\n",
    "        attn_implementation=\"sdpa\",    # available by default on Kaggle\n",
    "    )\n",
    "    mm = _two_gpu_max_memory(headroom_gb=2)\n",
    "    if mm and torch.cuda.device_count() > 1:\n",
    "        kw[\"device_map\"] = \"auto\"\n",
    "        kw[\"max_memory\"] = mm\n",
    "        # Optional if host RAM is tight:\n",
    "        # kw[\"offload_folder\"] = \"/kaggle/working/offload\"\n",
    "    else:\n",
    "        kw[\"device_map\"] = {\"\": 0}\n",
    "    return kw\n",
    "\n",
    "def download_file_from_google_drive(file_id, output_dir, output_filename, quiet=False):\n",
    "    \"\"\"\n",
    "    Downloads a file from Google Drive given its file ID and saves it to the specified directory.\n",
    "    \n",
    "    Args:\n",
    "        file_id (str): The Google Drive file ID (found in the file URL)\n",
    "        output_dir (str): Directory where the file should be saved\n",
    "        output_filename (str): Name of the output file\n",
    "        quiet (bool): Whether to suppress gdown output (default: False)\n",
    "    \n",
    "    Returns:\n",
    "        str: Path to the downloaded file if successful, None otherwise\n",
    "    \"\"\"\n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Full output path\n",
    "    output_file = os.path.join(output_dir, output_filename)\n",
    "    \n",
    "    print(\"Downloading the file...\")\n",
    "    try:\n",
    "        gdown.download(id=file_id, output=output_file, quiet=quiet, fuzzy=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Download failed: {str(e)}\")\n",
    "        return None\n",
    "    \n",
    "    # Verify download\n",
    "    if os.path.exists(output_file):\n",
    "        file_size = os.path.getsize(output_file) / (1024 * 1024)  # in MB\n",
    "        print(f\"Download successful! File saved to: {output_file}\")\n",
    "        print(f\"File size: {file_size:.2f} MB\")\n",
    "        return output_file\n",
    "    else:\n",
    "        print(\"Download failed - file not found\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T10:22:46.408152Z",
     "iopub.status.busy": "2025-12-03T10:22:46.407593Z",
     "iopub.status.idle": "2025-12-03T10:22:46.426758Z",
     "shell.execute_reply": "2025-12-03T10:22:46.426190Z",
     "shell.execute_reply.started": "2025-12-03T10:22:46.408134Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "from typing import List, Union\n",
    "\n",
    "def extract_and_delete_tar_gz(file_path: str, delete_compressed: bool = True) -> bool:\n",
    "    \"\"\"\n",
    "    Extracts a .tar.gz file and optionally deletes the compressed file.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the .tar.gz file\n",
    "        delete_compressed (bool): Whether to delete the compressed file after extraction (default: True)\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if extraction was successful, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Extracting: {file_path}\")\n",
    "        with tarfile.open(file_path, 'r:gz') as tar:\n",
    "            tar.extractall(path=os.path.dirname(file_path))\n",
    "        \n",
    "        if delete_compressed:\n",
    "            os.remove(file_path)\n",
    "            print(f\"Deleted compressed file: {file_path}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def process_directory(directory: str, recursive: bool = True, max_depth: Union[int, None] = None) -> int:\n",
    "    \"\"\"\n",
    "    Processes a directory to find and extract .tar.gz files.\n",
    "    \n",
    "    Args:\n",
    "        directory (str): Directory path to process\n",
    "        recursive (bool): Whether to process subdirectories (default: True)\n",
    "        max_depth (int|None): Maximum recursion depth (None for unlimited)\n",
    "    \n",
    "    Returns:\n",
    "        int: Number of .tar.gz files processed\n",
    "    \"\"\"\n",
    "    processed_count = 0\n",
    "    current_depth = 0\n",
    "    \n",
    "    while True:\n",
    "        found_tar_gz = False\n",
    "        for root, dirs, files in os.walk(directory):\n",
    "            # Calculate current depth\n",
    "            rel_path = os.path.relpath(root, directory)\n",
    "            current_depth = rel_path.count(os.sep) + 1 if rel_path != '.' else 0\n",
    "            \n",
    "            # Skip if beyond max depth\n",
    "            if max_depth is not None and current_depth > max_depth:\n",
    "                continue\n",
    "                \n",
    "            for file in files:\n",
    "                if file.endswith('.tar.gz'):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    if extract_and_delete_tar_gz(file_path):\n",
    "                        processed_count += 1\n",
    "                        found_tar_gz = True\n",
    "        \n",
    "        # If not recursive or no more .tar.gz files found, exit\n",
    "        if not recursive or not found_tar_gz:\n",
    "            break\n",
    "    \n",
    "    return processed_count\n",
    "\n",
    "def process_paths(paths: List[str], recursive: bool = True, max_depth: Union[int, None] = None) -> int:\n",
    "    \"\"\"\n",
    "    Processes a list of paths (files or directories) to extract .tar.gz files.\n",
    "    \n",
    "    Args:\n",
    "        paths (List[str]): List of file/directory paths to process\n",
    "        recursive (bool): Whether to process directories recursively (default: True)\n",
    "        max_depth (int|None): Maximum recursion depth for directories (None for unlimited)\n",
    "    \n",
    "    Returns:\n",
    "        int: Total number of .tar.gz files processed\n",
    "    \"\"\"\n",
    "    total_processed = 0\n",
    "    \n",
    "    for path in paths:\n",
    "        if not os.path.exists(path):\n",
    "            print(f\"Warning: Path does not exist - {path}\")\n",
    "            continue\n",
    "            \n",
    "        if path.endswith('.tar.gz'):\n",
    "            if extract_and_delete_tar_gz(path):\n",
    "                total_processed += 1\n",
    "        elif os.path.isdir(path):\n",
    "            print(f\"Processing directory: {path}\")\n",
    "            total_processed += process_directory(\n",
    "                directory=path,\n",
    "                recursive=recursive,\n",
    "                max_depth=max_depth\n",
    "            )\n",
    "    \n",
    "    print(f\"Total .tar.gz files processed: {total_processed}\")\n",
    "    return total_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T10:22:46.580595Z",
     "iopub.status.busy": "2025-12-03T10:22:46.579973Z",
     "iopub.status.idle": "2025-12-03T10:22:46.587976Z",
     "shell.execute_reply": "2025-12-03T10:22:46.587287Z",
     "shell.execute_reply.started": "2025-12-03T10:22:46.580573Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "import logging\n",
    "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "def load_model(model_filepath: str, torch_dtype:torch.dtype=torch.float16):\n",
    "    \"\"\"Load a model given a specific model_path.\n",
    "\n",
    "    Args:\n",
    "        model_filepath: str - Path to where the model is stored\n",
    "\n",
    "    Returns:\n",
    "        model, dict, str - Torch model + dictionary representation of the model + model class name\n",
    "    \"\"\"\n",
    "\n",
    "    conf_filepath = os.path.join(model_filepath, 'reduced-config.json')\n",
    "    logging.info(\"Loading config file from: {}\".format(conf_filepath))\n",
    "    with open(conf_filepath, 'r') as fh:\n",
    "        round_config = json.load(fh)\n",
    "\n",
    "    logging.info(\"Loading model from filepath: {}\".format(model_filepath))\n",
    "    # https://huggingface.co/docs/transformers/installation#offline-mode\n",
    "    if round_config['use_lora']:\n",
    "        base_model_filepath = os.path.join(model_filepath, 'base-model')\n",
    "        logging.info(\"loading the base model (before LORA) from {}\".format(base_model_filepath))\n",
    "        model = AutoModelForCausalLM.from_pretrained(base_model_filepath, trust_remote_code=True, torch_dtype=torch_dtype, local_files_only=True)\n",
    "        # model = AutoModelForCausalLM.from_pretrained(round_config['model_architecture'], trust_remote_code=True, attn_implementation=\"flash_attention_2\", torch_dtype=torch_dtype)\n",
    "\n",
    "        fine_tuned_model_filepath = os.path.join(model_filepath, 'fine-tuned-model')\n",
    "        logging.info(\"loading the LORA adapter onto the base model from {}\".format(fine_tuned_model_filepath))\n",
    "        model.load_adapter(fine_tuned_model_filepath)\n",
    "    else:\n",
    "        fine_tuned_model_filepath = os.path.join(model_filepath, 'fine-tuned-model')\n",
    "        logging.info(\"Loading full fine tune checkpoint into cpu from {}\".format(fine_tuned_model_filepath))\n",
    "        model = AutoModelForCausalLM.from_pretrained(fine_tuned_model_filepath, trust_remote_code=True, torch_dtype=torch_dtype, local_files_only=True)\n",
    "        # model = AutoModelForCausalLM.from_pretrained(fine_tuned_model_filepath, trust_remote_code=True, attn_implementation=\"flash_attention_2\", torch_dtype=torch_dtype)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    tokenizer_filepath = os.path.join(model_filepath, 'tokenizer')\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_filepath)\n",
    "\n",
    "    # Runtime memory knobs for your gradient-based rollout\n",
    "    model.eval()\n",
    "    if hasattr(model.config, \"use_cache\"):\n",
    "        model.config.use_cache = False  # reduce KV/activation memory during your search\n",
    "\n",
    "    # Optional: quick sanity check of sharding\n",
    "    try:\n",
    "        print(getattr(model, \"hf_device_map\", \"no device map\"))\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T10:22:46.589364Z",
     "iopub.status.busy": "2025-12-03T10:22:46.589193Z",
     "iopub.status.idle": "2025-12-03T10:26:12.619778Z",
     "shell.execute_reply": "2025-12-03T10:26:12.618953Z",
     "shell.execute_reply.started": "2025-12-03T10:22:46.589352Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting memory clearing process...\n",
      "Ran Python garbage collection\n",
      "Cleared CUDA cache\n",
      "Current CUDA memory allocated: 12908.04 MB\n",
      "Current CUDA memory cached: 12934.00 MB\n",
      "Cleared TensorFlow/Keras session\n",
      "Deleted torch object: model\n",
      "Deleted torch object: suffix_z\n",
      "Deleted torch object: emb_layer\n",
      "Memory clearing complete\n",
      "Downloading the file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1lwC9JLRu4Z4SSQwjNtetAymStPqQeaDc\n",
      "From (redirected): https://drive.google.com/uc?id=1lwC9JLRu4Z4SSQwjNtetAymStPqQeaDc&confirm=t&uuid=6ac8f928-a4cf-4c0f-b235-f057b69c4bc7\n",
      "To: /kaggle/tmp/model0.tar.gz\n",
      "100%|██████████| 10.6G/10.6G [01:14<00:00, 142MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download successful! File saved to: /kaggle/tmp/model0.tar.gz\n",
      "File size: 10092.92 MB\n",
      "Processing directory: /kaggle/tmp\n",
      "Extracting: /kaggle/tmp/model0.tar.gz\n",
      "Deleted compressed file: /kaggle/tmp/model0.tar.gz\n",
      "Total .tar.gz files processed: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9404f4585ed4d4ea855414ea31d77f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no device map\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "def download_and_load(file_id, output_filename, load_model_path):\n",
    "    \"\"\"Run the complete embedding optimization pipeline with custom weight function\"\"\"\n",
    "    # Clear memory before starting\n",
    "    clear_memory()\n",
    "    \n",
    "    # Download the model\n",
    "    downloaded_file = download_file_from_google_drive(\n",
    "        file_id=file_id,#\"1-K-HcT-3-00rxPpvQxZ75o2be3STchsv\",\n",
    "        output_dir=\"/kaggle/tmp\",\n",
    "        output_filename=output_filename,#\"model4.tar.gz\",\n",
    "        quiet=False\n",
    "    )\n",
    "    \n",
    "    # Process paths\n",
    "    process_paths(\n",
    "        paths=['/kaggle/tmp',],\n",
    "        recursive=True,\n",
    "        max_depth=None\n",
    "    )\n",
    "    \n",
    "    # Load model and tokenizer\n",
    "    model, tokenizer = load_model(load_model_path)#\"/kaggle/tmp/id-00000004\")\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "file_id = \"1lwC9JLRu4Z4SSQwjNtetAymStPqQeaDc\"\n",
    "output_filename = \"model0.tar.gz\"\n",
    "load_model_path = \"/kaggle/tmp/id-00000000\"\n",
    "\n",
    "model, tokenizer = download_and_load(file_id=file_id, output_filename=output_filename, load_model_path=load_model_path)\n",
    "\n",
    "model.to('cuda:0')\n",
    "\n",
    "print(model.device)\n",
    "\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_cell_guid": "e920dfc4-351a-44fb-bed7-fd39af4920a6",
    "_uuid": "82baa6b1-6aed-4e56-b92b-c6f7a51c8256",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-12-03T10:26:12.620993Z",
     "iopub.status.busy": "2025-12-03T10:26:12.620762Z",
     "iopub.status.idle": "2025-12-03T10:26:12.629163Z",
     "shell.execute_reply": "2025-12-03T10:26:12.628369Z",
     "shell.execute_reply.started": "2025-12-03T10:26:12.620976Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import RandomSampler, DataLoader\n",
    "\n",
    "def load_prompts_unpadded(tokenizer, args, seed=42):\n",
    "    \"\"\"\n",
    "    Returns DataLoader with reproducible shuffling across runs.\n",
    "    \"\"\"\n",
    "    # Set global seeds for any randomness in dataset loading\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    ds = load_dataset(\"tatsu-lab/alpaca\", split=\"train\", cache_dir=args[\"data_dir\"])\n",
    "\n",
    "    # Subsample with fixed seed if needed\n",
    "    if \"sample_size\" in args and args[\"sample_size\"] is not None and args[\"sample_size\"] < len(ds):\n",
    "        # Create deterministic indices for subsampling\n",
    "        import numpy as np\n",
    "        np.random.seed(seed)\n",
    "        indices = np.random.permutation(len(ds))[:args[\"sample_size\"]]\n",
    "        ds = ds.select(indices.tolist())\n",
    "    \n",
    "    def collate(batch):\n",
    "        texts = [ex[\"instruction\"] for ex in batch]\n",
    "        enc = tokenizer(\n",
    "            texts,\n",
    "            padding=False,\n",
    "            truncation=True,\n",
    "            max_length=args[\"max_length\"],\n",
    "        )\n",
    "        prompts = [torch.tensor(ids, dtype=torch.long) for ids in enc[\"input_ids\"]]\n",
    "        prompt_lens = [len(p) for p in prompts]\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": prompts,\n",
    "            \"prompt_lens\": torch.tensor(prompt_lens, dtype=torch.long),\n",
    "        }\n",
    "    \n",
    "    # Create a seeded generator for the sampler\n",
    "    generator = torch.Generator()\n",
    "    generator.manual_seed(seed)\n",
    "    \n",
    "    # Use RandomSampler with the seeded generator\n",
    "    sampler = RandomSampler(ds, generator=generator)\n",
    "    \n",
    "    # DataLoader with sampler instead of shuffle=True\n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size=args[\"batch_size\"],\n",
    "        sampler=sampler,  # ← Use sampler instead of shuffle\n",
    "        pin_memory=True,\n",
    "        num_workers=0,  # Use 0 workers for perfect reproducibility\n",
    "        collate_fn=collate,\n",
    "        # Don't specify shuffle when using sampler!\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "690915cd-41f4-44f0-9cd8-4dcb0d24dc7c",
    "_uuid": "d7ff0512-275d-4863-bb5e-e364092d3b09",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "### Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_cell_guid": "47d8270e-9447-48e9-8980-f0ce9baa5fd5",
    "_uuid": "45cc71f6-4ced-4cc0-abb2-e176d204470d",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-12-03T10:26:12.630878Z",
     "iopub.status.busy": "2025-12-03T10:26:12.630634Z",
     "iopub.status.idle": "2025-12-03T10:26:12.646063Z",
     "shell.execute_reply": "2025-12-03T10:26:12.645272Z",
     "shell.execute_reply.started": "2025-12-03T10:26:12.630857Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import amp\n",
    "\n",
    "def entropy_loss(batch_logits, is_logit=True):\n",
    "    \"\"\"\n",
    "    batch_logits: can be (n, V) or (B, n, V) \n",
    "    is_logit: whether input is logits (True) or probabilities (False)\n",
    "    \n",
    "    Returns: scalar = mean(min(entropy across n) across B)\n",
    "    \"\"\"\n",
    "    original_dim = batch_logits.dim()\n",
    "    if original_dim == 2:\n",
    "        batch_logits = batch_logits.unsqueeze(0)\n",
    "\n",
    "    if is_logit:\n",
    "        log_probs = F.log_softmax(batch_logits, dim=-1)\n",
    "    else:\n",
    "        log_probs = torch.log(batch_logits + 1e-12)\n",
    "\n",
    "    probs = log_probs.exp()\n",
    "    entropy = -(probs * log_probs).sum(dim=-1)  # (B, n)\n",
    "    min_entropy = entropy.min(dim=-1).values    # (B,)\n",
    "    mean_min_entropy = min_entropy.mean()\n",
    "    \n",
    "    return mean_min_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T10:26:12.647866Z",
     "iopub.status.busy": "2025-12-03T10:26:12.647668Z",
     "iopub.status.idle": "2025-12-03T10:26:12.663845Z",
     "shell.execute_reply": "2025-12-03T10:26:12.663176Z",
     "shell.execute_reply.started": "2025-12-03T10:26:12.647849Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def vocab_contrastive_loss(\n",
    "    z: torch.Tensor,        # (L, d) optimized embeddings, requires_grad=True\n",
    "    E: torch.Tensor,        # (V, d) vocab embeddings (can be detached)\n",
    "    k: int = 5,             # size of positive set (top-k closest)\n",
    "    margin: float = 15,    # margin between pos cluster and closest negative\n",
    "    neg_sample: int | None = None,  # optionally subsample negatives\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Contrastive loss between top-k closest vocab embeddings (positives)\n",
    "    and the rest (negatives), in squared Euclidean distance space.\n",
    "\n",
    "    For each position l:\n",
    "      d_{li} = ||z_l - E_i||^2\n",
    "      P_l = indices of k smallest d_{li}  (positives)\n",
    "      N_l = all other indices            (negatives)\n",
    "\n",
    "      d_pos_mean  = mean_{i in P_l} d_{li}\n",
    "      d_neg_min   = min_{j in N_l} d_{lj}\n",
    "\n",
    "      L_l = relu(margin + d_pos_mean - d_neg_min)\n",
    "\n",
    "    Loss = mean_l L_l\n",
    "    \"\"\"\n",
    "    L, d = z.shape\n",
    "    V, dE = E.shape\n",
    "    assert d == dE, \"Dimension mismatch between z and E\"\n",
    "\n",
    "    # (L, V, d): pairwise differences\n",
    "    diff = z.unsqueeze(1) - E.unsqueeze(0)   # (L, V, d)\n",
    "    # (L, V): squared distances\n",
    "    dists = (diff ** 2).sum(dim=-1)\n",
    "    dists = torch.sqrt(dists)  # Actual Euclidean distance\n",
    "\n",
    "    # Get top-k *smallest* distances => positives\n",
    "    k = min(k, V - 1)  # ensure at least one negative\n",
    "    pos_dists, pos_idx = dists.topk(k, dim=-1, largest=False)  # (L, k)\n",
    "\n",
    "    # Mask to separate negatives\n",
    "    neg_mask = torch.ones_like(dists, dtype=torch.bool)        # (L, V)\n",
    "    neg_mask.scatter_(1, pos_idx, False)  # mark positives as False\n",
    "    neg_dists_full = dists.masked_select(neg_mask).view(L, -1) # (L, V-k)\n",
    "\n",
    "    # Optionally subsample negatives for efficiency\n",
    "    if neg_sample is not None and neg_sample < neg_dists_full.size(1):\n",
    "        # random permutation per batch (simple approx)\n",
    "        perm = torch.randperm(neg_dists_full.size(1), device=neg_dists_full.device)\n",
    "        neg_dists = neg_dists_full[:, perm[:neg_sample]]       # (L, neg_sample)\n",
    "    else:\n",
    "        neg_dists = neg_dists_full                             # (L, V-k)\n",
    "\n",
    "    # For each position: avg positive distance, and closest negative distance\n",
    "    pos_mean = pos_dists.mean(dim=-1)                          # (L,)\n",
    "    neg_min  = neg_dists.min(dim=-1).values                    # (L,)\n",
    "\n",
    "    # Hinge: margin + pos_mean - neg_min <= 0\n",
    "    loss_per_pos = F.relu(margin + pos_mean - neg_min)         # (L,)\n",
    "    loss = loss_per_pos.mean()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "0f83304f-d672-4733-8217-cdc4617b2cf4",
    "_uuid": "2e2a824f-b638-4eae-8651-cddfa298eb4d",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "### Rollout Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b27218f0-6c32-44fc-aef1-082ace3c0fff",
    "_uuid": "d4d5f659-e161-4fcf-8de8-576c776decea",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-12-03T10:52:57.821922Z",
     "iopub.status.busy": "2025-12-03T10:52:57.821632Z",
     "iopub.status.idle": "2025-12-03T10:52:57.872761Z",
     "shell.execute_reply": "2025-12-03T10:52:57.871998Z",
     "shell.execute_reply.started": "2025-12-03T10:52:57.821902Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import amp\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "def compute_loss_for_suffix(\n",
    "    model,\n",
    "    emb_layer,\n",
    "    batch,\n",
    "    suffix_z,           # (Ls, V) nn.Parameter  == pre-softmax logits over vocab per suffix position\n",
    "    n_tokens=10,\n",
    "    nt=1,\n",
    "    amp_dtype=torch.float16,\n",
    "    cos_reg_weight=0.1,  # used as weight for soft one-hot entropy regularizer\n",
    "    E_norm_cpu=None,     # unused now, kept for API compatibility\n",
    "    chunk_size=1024,\n",
    "    top_k=5,             # unused now\n",
    "    neg_weight=1.0,      # unused now\n",
    "):\n",
    "    \"\"\"\n",
    "    - suffix_z is pre-softmax logits over vocab: (Ls, V).\n",
    "    - Convert suffix_z -> soft one-hot (suffix_probs) -> suffix embeddings via suffix_probs @ E.\n",
    "    - For each example, build [prompt][suffix_embs] in embedding space.\n",
    "    - Pad all to same length -> [prompt][suffix][PAD].\n",
    "    - Roll out n_tokens-1 tokens under inference_mode.\n",
    "    - Final forward WITH grad gives entropy loss on last n_tokens generated tokens.\n",
    "    - PLUS: entropy regularizer on the soft one-hot (suffix_probs) to push it toward one-hot.\n",
    "\n",
    "    Additionally:\n",
    "    - Returns a rich `characteristics` dict that contains:\n",
    "      * full tensors (on CPU) for suffix logits/probs/embs\n",
    "      * per-position entropy, max prob, norms, margins, nearest-token distances\n",
    "      * full singular value vectors and eigenvalue spectra\n",
    "      * output logits/prob features from the final step\n",
    "    \"\"\"\n",
    "    prompts = batch[\"input_ids\"]   # list of 1D LongTensors (Li,)\n",
    "    dev = emb_layer.weight.device\n",
    "    emb_dtype = emb_layer.weight.dtype\n",
    "\n",
    "    # ---------- Soft one-hot over vocab and suffix embeddings ----------\n",
    "    # suffix_z: (Ls, V) logits over vocab\n",
    "    suffix_logits = suffix_z.to(device=dev, dtype=torch.float32)  # keep logits in fp32 for stability\n",
    "    Ls, V_logits = suffix_logits.shape\n",
    "\n",
    "    E = emb_layer.weight  # (V, E_dim)\n",
    "    V, E_dim = E.shape\n",
    "    assert V_logits == V, f\"suffix_z second dim ({V_logits}) must match vocab size ({V}).\"\n",
    "\n",
    "    # Soft one-hot over vocab (fp32)\n",
    "    suffix_probs_fp32 = F.softmax(suffix_logits, dim=-1)  # (Ls, V)\n",
    "\n",
    "    # Suffix embeddings: convex combination of token embeddings\n",
    "    suffix_probs = suffix_probs_fp32.to(dtype=emb_dtype, device=dev)\n",
    "    suffix_embs = suffix_probs @ E  # (Ls, E_dim), with grad wrt suffix_logits\n",
    "\n",
    "    # ---------- Build per-example [prompt][suffix] in embedding space ----------\n",
    "    B = len(prompts)\n",
    "    base_embs = []   # each: (Li+Ls, E_dim)\n",
    "    base_lens = []   # each: scalar length Li+Ls\n",
    "\n",
    "    for p_ids in prompts:\n",
    "        p_ids_dev = p_ids.to(dev)\n",
    "        p_emb = emb_layer(p_ids_dev).detach()      # (Li, E_dim), prompts are constants\n",
    "        base = torch.cat([p_emb, suffix_embs], dim=0)  # (Li+Ls, E_dim)\n",
    "        base_embs.append(base)\n",
    "        base_lens.append(base.size(0))\n",
    "\n",
    "    # Pad to [prompt][suffix][PAD...] across the batch\n",
    "    base = pad_sequence(base_embs, batch_first=True)   # (B, max_len, E_dim)\n",
    "    base_lens = torch.tensor(base_lens, device=dev)    # (B,)\n",
    "    max_len = base.size(1)\n",
    "\n",
    "    # Attention mask: 1 for real tokens, 0 for pad\n",
    "    arange = torch.arange(max_len, device=dev).unsqueeze(0)  # (1, max_len)\n",
    "    base_mask = (arange < base_lens.unsqueeze(1)).long()     # (B, max_len)\n",
    "\n",
    "    # Now base has structure [prompt][suffix][PAD] per row (masked pads)\n",
    "\n",
    "    def _one_step_logits(e, m, n=1):\n",
    "        with amp.autocast(\"cuda\", dtype=amp_dtype):\n",
    "            out = model(\n",
    "                inputs_embeds=e,\n",
    "                attention_mask=m,\n",
    "                use_cache=False,\n",
    "                output_attentions=False,\n",
    "                output_hidden_states=False,\n",
    "                return_dict=True,\n",
    "            )\n",
    "        if n == 1:\n",
    "            return out.logits[:, -1, :]          # (B, V)\n",
    "        else:\n",
    "            return out.logits[:, -n:, :]         # (B, n, V)\n",
    "\n",
    "    # ---------- Rollout under no grad (from detached base) ----------\n",
    "    work_e = base.detach()  # rollout uses constants\n",
    "    work_m = base_mask\n",
    "    added_embs = []         # list of (B, E_dim) constants\n",
    "\n",
    "    T = max(0, n_tokens - 1)\n",
    "    with torch.inference_mode():\n",
    "        for _ in range(T):\n",
    "            logits_t = _one_step_logits(work_e, work_m, n=1)   # (B, V)\n",
    "            probs_t = torch.softmax(logits_t, dim=-1)          # (B, V)\n",
    "            next_ids = torch.argmax(probs_t, dim=-1)           # (B,)\n",
    "\n",
    "            next_emb = emb_layer(next_ids.to(dev)).detach()    # (B, E_dim)\n",
    "            added_embs.append(next_emb)\n",
    "\n",
    "            work_e = torch.cat([work_e, next_emb.unsqueeze(1)], dim=1)\n",
    "            work_m = torch.cat(\n",
    "                [work_m, torch.ones((B, 1), dtype=work_m.dtype, device=dev)],\n",
    "                dim=1,\n",
    "            )\n",
    "\n",
    "        # rollout temporaries\n",
    "        del logits_t, probs_t, next_ids, next_emb\n",
    "\n",
    "    # ---------- Final inputs: [prompt][suffix][PAD] + generated tokens ----------\n",
    "    if len(added_embs) > 0:\n",
    "        added = torch.stack(added_embs, dim=1)              # (B, T, E_dim)\n",
    "        del added_embs, work_e, work_m\n",
    "\n",
    "        final_emb = torch.cat([base, added], dim=1)         # (B, max_len+T, E_dim)\n",
    "        gen_mask = torch.ones((B, T), dtype=base_mask.dtype, device=dev)\n",
    "        final_mask = torch.cat([base_mask, gen_mask], dim=1)\n",
    "    else:\n",
    "        final_emb = base\n",
    "        final_mask = base_mask\n",
    "\n",
    "    # ---------- Final step WITH grad (depends on suffix_logits via suffix_embs) ----------\n",
    "    logits_last = _one_step_logits(final_emb, final_mask, n=n_tokens)  # (B, n_tokens, V)\n",
    "    ent = entropy_loss(logits_last, is_logit=True)                     # main term: minimize entropy\n",
    "\n",
    "    # ---------- Soft one-hot entropy regularizer ----------\n",
    "    # Entropy over suffix_probs (per suffix position), minimize to encourage near-one-hot\n",
    "    ent_soft = entropy_loss(suffix_probs_fp32, is_logit=False)\n",
    "\n",
    "    # total loss: model entropy + weighted soft one-hot entropy\n",
    "    total_loss = ent + cos_reg_weight * ent_soft\n",
    "\n",
    "    # ----- Suffix joint log-prob term (discrete argmax-based) -----\n",
    "    # We'll treat this as an extra constant term (no gradient wrt suffix_z)\n",
    "    base_loss = total_loss\n",
    "    dev = emb_layer.weight.device\n",
    "\n",
    "    # defaults in case batch is empty\n",
    "    suffix_joint_logprob = torch.tensor(0.0, device=dev)\n",
    "    suffix_logprob_term = torch.tensor(0.0, device=dev)\n",
    "    model_suffix_probs_cpu = None  # NEW: will hold model-assigned suffix probs (Ls, V) on CPU\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Discrete suffix tokens from current soft one-hot\n",
    "        suffix_token_ids = suffix_probs_fp32.argmax(dim=-1)  # (Ls,)\n",
    "\n",
    "        pad_id = getattr(tokenizer, \"pad_token_id\", 0)\n",
    "        # print(f'pad_id{pad_id}')\n",
    "\n",
    "        full_ids_list = []\n",
    "        prompt_lens = []\n",
    "        for p_ids in prompts:\n",
    "            p_ids_dev = p_ids.to(dev)\n",
    "            prompt_lens.append(p_ids_dev.size(0))\n",
    "            full_ids_list.append(torch.cat([p_ids_dev, suffix_token_ids.to(dev)], dim=0))\n",
    "\n",
    "        if len(full_ids_list) > 0:\n",
    "            full_ids = pad_sequence(\n",
    "                full_ids_list,\n",
    "                batch_first=True,\n",
    "                padding_value=pad_id,\n",
    "            ).to(dev)                                             # (B_ids, L_max_ids)\n",
    "            prompt_lens = torch.tensor(prompt_lens, device=dev)  # (B_ids,)\n",
    "            B_ids, L_max_ids = full_ids.size()\n",
    "\n",
    "            # attention mask for [prompt][suffix] region\n",
    "            full_lengths = prompt_lens + Ls\n",
    "            arange_ids = torch.arange(L_max_ids, device=dev).unsqueeze(0)\n",
    "            attn_ids = (arange_ids < full_lengths.unsqueeze(1)).long()\n",
    "\n",
    "            from torch import amp as _amp_mod  # to avoid ambiguity if needed\n",
    "\n",
    "            with _amp_mod.autocast(\"cuda\", dtype=amp_dtype):\n",
    "                outputs_suffix = model(\n",
    "                    input_ids=full_ids,\n",
    "                    attention_mask=attn_ids,\n",
    "                    use_cache=False,\n",
    "                    output_attentions=False,\n",
    "                    output_hidden_states=False,\n",
    "                    return_dict=True,\n",
    "                )\n",
    "\n",
    "            logits_suffix = outputs_suffix.logits                 # (B_ids, L_max_ids, V)\n",
    "            log_probs_suffix = F.log_softmax(logits_suffix, dim=-1)\n",
    "\n",
    "            # positions of suffix tokens in full_ids and corresponding logits\n",
    "            positions = prompt_lens.unsqueeze(1) + torch.arange(Ls, device=dev).unsqueeze(0)  # (B_ids, Ls)\n",
    "            positions_logits = (positions - 1).clamp(min=0, max=L_max_ids - 1)                # (B_ids, Ls)\n",
    "\n",
    "            token_ids_expand = suffix_token_ids.unsqueeze(0).expand(B_ids, -1)                # (B_ids, Ls)\n",
    "\n",
    "            # FULL model-assigned suffix distributions over vocab at each suffix position\n",
    "            # pick the distribution at the timestep where each suffix token is predicted\n",
    "            V = logits_suffix.size(-1)\n",
    "            model_suffix_log_probs = log_probs_suffix.gather(\n",
    "                dim=1,\n",
    "                index=positions_logits.unsqueeze(-1).expand(-1, -1, V),\n",
    "            )  # (B_ids, Ls, V)\n",
    "\n",
    "            model_suffix_probs = model_suffix_log_probs.exp()        # (B_ids, Ls, V)\n",
    "            # average across batch -> (Ls, V), one distribution per suffix position\n",
    "            model_suffix_probs_mean = model_suffix_probs.mean(dim=0)  # (Ls, V)\n",
    "            # stash on CPU for later feature extraction\n",
    "            model_suffix_probs_cpu = model_suffix_probs_mean.detach().cpu()\n",
    "\n",
    "            # log p(s_j | prompt + s_<j) for each batch, each suffix position\n",
    "            suffix_token_logprobs = log_probs_suffix[\n",
    "                torch.arange(B_ids, device=dev).unsqueeze(1),\n",
    "                positions_logits,\n",
    "                token_ids_expand,\n",
    "            ]  # (B_ids, Ls)\n",
    "\n",
    "\n",
    "            # schedule weights along suffix positions\n",
    "            nt_sched = min(Ls, nt)          # \"nt\": first nt suffix positions\n",
    "            weights = torch.ones(Ls, device=dev)  # default weight = 1.0\n",
    "            if nt_sched > 0:\n",
    "                # lower values at the start, gradually increasing up to 1.0\n",
    "                weight_schedule = torch.linspace(0.3, 1.0, steps=nt_sched, device=dev)\n",
    "                weights[:nt_sched] = weight_schedule\n",
    "\n",
    "            # apply weights and average over batch\n",
    "            weighted_logprobs = (suffix_token_logprobs * weights.unsqueeze(0)).sum(dim=-1)  # (B_ids,)\n",
    "            suffix_joint_logprob = weighted_logprobs.mean()                                 # scalar\n",
    "\n",
    "            # negative-signed multiplier: encourages higher joint suffix log-prob\n",
    "            suffix_logprob_weight = 0.01\n",
    "            suffix_logprob_term = -suffix_logprob_weight * suffix_joint_logprob\n",
    "\n",
    "    # include the suffix joint log-prob term in the loss\n",
    "    total_loss = base_loss + suffix_logprob_term\n",
    "\n",
    "    # ---------- Derive characteristics (forward-only, store tensors on CPU) ----------\n",
    "    characteristics = {}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        eps = 1e-12\n",
    "\n",
    "        def _stat(t: torch.Tensor):\n",
    "            # 1D tensor stats\n",
    "            t = t.detach()\n",
    "            if t.numel() == 0:\n",
    "                return {\"mean\": 0.0, \"std\": 0.0, \"min\": 0.0, \"max\": 0.0}\n",
    "            mean = t.mean().item()\n",
    "            std = t.std(unbiased=False).item() if t.numel() > 1 else 0.0\n",
    "            return {\n",
    "                \"mean\": mean,\n",
    "                \"std\": std,\n",
    "                \"min\": t.min().item(),\n",
    "                \"max\": t.max().item(),\n",
    "            }\n",
    "\n",
    "        # ----- suffix_logits features -----\n",
    "        suffix_logits_f = suffix_logits.detach()            # (Ls, V)\n",
    "        logits_flat = suffix_logits_f.view(-1)\n",
    "        suffix_logits_cpu = suffix_logits_f.cpu()\n",
    "\n",
    "        suffix_logits_features = {\n",
    "            \"tensor\": suffix_logits_cpu,                    # (Ls, V) on CPU\n",
    "            \"flat_stats\": _stat(logits_flat),\n",
    "            \"flat_norm\": logits_flat.norm().item(),\n",
    "        }\n",
    "\n",
    "        # ----- suffix_probs features (row-wise + spectral) -----\n",
    "        suffix_probs_f = suffix_probs_fp32.detach()         # (Ls, V)\n",
    "        suffix_probs_cpu = suffix_probs_f.cpu()\n",
    "\n",
    "        logp_suffix = (suffix_probs_f + eps).log()          # (Ls, V)\n",
    "        row_entropy = -(suffix_probs_f * logp_suffix).sum(dim=-1)       # (Ls,)\n",
    "        row_maxprob, row_maxidx = suffix_probs_f.max(dim=-1)            # (Ls,)\n",
    "        row_l2 = suffix_probs_f.norm(dim=-1)                            # (Ls,)\n",
    "        row_gini = 1.0 - (suffix_probs_f ** 2).sum(dim=-1)              # (Ls,)\n",
    "        top2p, top2idx = suffix_probs_f.topk(2, dim=-1)\n",
    "        row_margin = top2p[:, 0] - top2p[:, 1]                          # (Ls,)\n",
    "\n",
    "        suffix_probs_features = {\n",
    "            \"tensor\": suffix_probs_cpu,                      # (Ls, V) on CPU\n",
    "            \"row_entropy\": {\n",
    "                \"tensor\": row_entropy.cpu(),                 # (Ls,)\n",
    "                \"stats\": _stat(row_entropy),\n",
    "            },\n",
    "            \"row_max_prob\": {\n",
    "                \"tensor\": row_maxprob.cpu(),                 # (Ls,)\n",
    "                \"stats\": _stat(row_maxprob),\n",
    "            },\n",
    "            \"row_max_idx\": row_maxidx.cpu(),                 # (Ls,)\n",
    "            \"row_l2\": {\n",
    "                \"tensor\": row_l2.cpu(),                      # (Ls,)\n",
    "                \"stats\": _stat(row_l2),\n",
    "            },\n",
    "            \"row_gini\": {\n",
    "                \"tensor\": row_gini.cpu(),                    # (Ls,)\n",
    "                \"stats\": _stat(row_gini),\n",
    "            },\n",
    "            \"row_margin\": {\n",
    "                \"tensor\": row_margin.cpu(),                  # (Ls,)\n",
    "                \"stats\": _stat(row_margin),\n",
    "            },\n",
    "            \"row_top2_probs\": top2p.cpu(),                   # (Ls, 2)\n",
    "            \"row_top2_idx\": top2idx.cpu(),                   # (Ls, 2)\n",
    "        }\n",
    "\n",
    "        # SVD of suffix_probs for low-rank structure (Ls x V)\n",
    "        try:\n",
    "            sv = torch.linalg.svdvals(suffix_probs_cpu)      # (min(Ls, V),)\n",
    "            sv_sorted = torch.sort(sv, descending=True).values\n",
    "            sv_sum = sv_sorted.sum().item()\n",
    "            if sv_sum > 0:\n",
    "                sigma1_ratio = (sv_sorted[0] / sv_sum).item()\n",
    "            else:\n",
    "                sigma1_ratio = 0.0\n",
    "            sigma1_over_sigma2 = (\n",
    "                (sv_sorted[0] / sv_sorted[1]).item()\n",
    "                if sv_sorted.numel() > 1 and sv_sorted[1].abs() > 0\n",
    "                else None\n",
    "            )\n",
    "            effective_rank = float(\n",
    "                torch.exp(\n",
    "                    -((sv_sorted / sv_sum) ** 2 * (sv_sorted / sv_sum).log()).sum()\n",
    "                ).item()\n",
    "            ) if sv_sum > 0 else 0.0\n",
    "            suffix_probs_features[\"sv\"] = {\n",
    "                \"singular_values\": sv_sorted.cpu(),          # full spectrum\n",
    "                \"sigma1_ratio\": sigma1_ratio,\n",
    "                \"sigma1_over_sigma2\": sigma1_over_sigma2,\n",
    "                \"effective_rank_proxy\": effective_rank,\n",
    "            }\n",
    "        except RuntimeError:\n",
    "            suffix_probs_features[\"sv\"] = {\n",
    "                \"singular_values\": torch.empty(0),\n",
    "                \"sigma1_ratio\": 0.0,\n",
    "                \"sigma1_over_sigma2\": None,\n",
    "                \"effective_rank_proxy\": 0.0,\n",
    "            }\n",
    "\n",
    "        # ----- model-assigned suffix probs (given prompts) -----\n",
    "        # model_suffix_probs_cpu was computed in the suffix-logprob block; it has shape (Ls, V)\n",
    "        suffix_probs_model_features = None\n",
    "        if model_suffix_probs_cpu is not None:\n",
    "            suffix_probs_model_f = model_suffix_probs_cpu.to(dtype=torch.float32)  # (Ls, V)\n",
    "\n",
    "            logp_suffix_model = (suffix_probs_model_f + eps).log()\n",
    "            row_entropy_model = -(suffix_probs_model_f * logp_suffix_model).sum(dim=-1)\n",
    "            row_maxprob_model, row_maxidx_model = suffix_probs_model_f.max(dim=-1)\n",
    "            row_l2_model = suffix_probs_model_f.norm(dim=-1)\n",
    "            row_gini_model = 1.0 - (suffix_probs_model_f ** 2).sum(dim=-1)\n",
    "            top2p_model, top2idx_model = suffix_probs_model_f.topk(2, dim=-1)\n",
    "            row_margin_model = top2p_model[:, 0] - top2p_model[:, 1]\n",
    "\n",
    "            suffix_probs_model_features = {\n",
    "                \"tensor\": model_suffix_probs_cpu,                      # (Ls, V)\n",
    "                \"row_entropy\": {\n",
    "                    \"tensor\": row_entropy_model.cpu(),\n",
    "                    \"stats\": _stat(row_entropy_model),\n",
    "                },\n",
    "                \"row_max_prob\": {\n",
    "                    \"tensor\": row_maxprob_model.cpu(),\n",
    "                    \"stats\": _stat(row_maxprob_model),\n",
    "                },\n",
    "                \"row_max_idx\": row_maxidx_model.cpu(),\n",
    "                \"row_l2\": {\n",
    "                    \"tensor\": row_l2_model.cpu(),\n",
    "                    \"stats\": _stat(row_l2_model),\n",
    "                },\n",
    "                \"row_gini\": {\n",
    "                    \"tensor\": row_gini_model.cpu(),\n",
    "                    \"stats\": _stat(row_gini_model),\n",
    "                },\n",
    "                \"row_margin\": {\n",
    "                    \"tensor\": row_margin_model.cpu(),\n",
    "                    \"stats\": _stat(row_margin_model),\n",
    "                },\n",
    "                \"row_top2_probs\": top2p_model.cpu(),\n",
    "                \"row_top2_idx\": top2idx_model.cpu(),\n",
    "            }\n",
    "\n",
    "            # SVD of model-assigned suffix probs for low-rank structure\n",
    "            try:\n",
    "                sv_m = torch.linalg.svdvals(suffix_probs_model_f)      # (min(Ls, V),)\n",
    "                sv_m_sorted = torch.sort(sv_m, descending=True).values\n",
    "                sv_m_sum = sv_m_sorted.sum().item()\n",
    "                if sv_m_sum > 0:\n",
    "                    sigma1_ratio_m = (sv_m_sorted[0] / sv_m_sum).item()\n",
    "                else:\n",
    "                    sigma1_ratio_m = 0.0\n",
    "                sigma1_over_sigma2_m = (\n",
    "                    (sv_m_sorted[0] / sv_m_sorted[1]).item()\n",
    "                    if sv_m_sorted.numel() > 1 and sv_m_sorted[1].abs() > 0\n",
    "                    else None\n",
    "                )\n",
    "                effective_rank_m = float(\n",
    "                    torch.exp(\n",
    "                        -((sv_m_sorted / sv_m_sum) ** 2 * (sv_m_sorted / sv_m_sum).log()).sum()\n",
    "                    ).item()\n",
    "                ) if sv_m_sum > 0 else 0.0\n",
    "                suffix_probs_model_features[\"sv\"] = {\n",
    "                    \"singular_values\": sv_m_sorted.cpu(),\n",
    "                    \"sigma1_ratio\": sigma1_ratio_m,\n",
    "                    \"sigma1_over_sigma2\": sigma1_over_sigma2_m,\n",
    "                    \"effective_rank_proxy\": effective_rank_m,\n",
    "                }\n",
    "            except RuntimeError:\n",
    "                suffix_probs_model_features[\"sv\"] = {\n",
    "                    \"singular_values\": torch.empty(0),\n",
    "                    \"sigma1_ratio\": 0.0,\n",
    "                    \"sigma1_over_sigma2\": None,\n",
    "                    \"effective_rank_proxy\": 0.0,\n",
    "                }\n",
    "\n",
    "            # attach to main suffix_probs_features dict\n",
    "            suffix_probs_features[\"model_assigned\"] = suffix_probs_model_features\n",
    "\n",
    "        # ----- suffix_embs features -----\n",
    "        E_f = E.detach().float()                            # (V, E_dim)\n",
    "        suffix_embs_f = suffix_embs.detach().float()        # (Ls, E_dim)\n",
    "        suffix_embs_cpu = suffix_embs_f.cpu()\n",
    "\n",
    "        emb_row_norm = suffix_embs_f.norm(dim=-1)           # (Ls,)\n",
    "        emb_row_norm_stats = _stat(emb_row_norm)\n",
    "\n",
    "        # pairwise cosine between positions\n",
    "        if Ls > 1:\n",
    "            S_normed = F.normalize(suffix_embs_f, dim=-1)\n",
    "            cos_mat = S_normed @ S_normed.T                 # (Ls, Ls)\n",
    "            cos_vals = cos_mat[~torch.eye(Ls, dtype=torch.bool, device=cos_mat.device)]\n",
    "            cos_stats = _stat(cos_vals)\n",
    "            cos_mat_cpu = cos_mat.cpu()\n",
    "        else:\n",
    "            cos_stats = {\"mean\": 0.0, \"std\": 0.0, \"min\": 0.0, \"max\": 0.0}\n",
    "            cos_mat_cpu = torch.eye(Ls)\n",
    "\n",
    "        # distance to nearest vocab embedding for each suffix position\n",
    "        d_min_list = []\n",
    "        nearest_idx_list = []\n",
    "        for i in range(Ls):\n",
    "            diff_i = E_f - suffix_embs_f[i].unsqueeze(0)    # (V, E_dim)\n",
    "            dists_i = diff_i.pow(2).sum(dim=-1).sqrt()      # (V,)\n",
    "            dmin, idxmin = dists_i.min(dim=-1)\n",
    "            d_min_list.append(dmin)\n",
    "            nearest_idx_list.append(idxmin)\n",
    "        d_min = torch.stack(d_min_list, dim=0)              # (Ls,)\n",
    "        nearest_idx = torch.stack(nearest_idx_list, dim=0)  # (Ls,)\n",
    "\n",
    "        # covariance in embedding space via Ls x Ls matrix (spectral)\n",
    "        if Ls > 1:\n",
    "            S_c = suffix_embs_f - suffix_embs_f.mean(dim=0, keepdim=True)  # (Ls, d)\n",
    "            M = (S_c @ S_c.T) / (Ls - 1)                                   # (Ls, Ls)\n",
    "            try:\n",
    "                ev = torch.linalg.eigvalsh(M.cpu())                        # (Ls,) ascending\n",
    "                ev_sorted = torch.sort(ev, descending=True).values\n",
    "                ev_sum = ev_sorted.clamp_min(0).sum().item()\n",
    "                if ev_sum > 0:\n",
    "                    emb_lambda1_ratio = (ev_sorted[0] / ev_sum).item()\n",
    "                else:\n",
    "                    emb_lambda1_ratio = 0.0\n",
    "                cov_eigs_cpu = ev_sorted.cpu()\n",
    "            except RuntimeError:\n",
    "                emb_lambda1_ratio = 0.0\n",
    "                cov_eigs_cpu = torch.empty(0)\n",
    "        else:\n",
    "            emb_lambda1_ratio = 0.0\n",
    "            cov_eigs_cpu = torch.empty(0)\n",
    "\n",
    "        suffix_embs_features = {\n",
    "            \"tensor\": suffix_embs_cpu,                          # (Ls, E_dim) on CPU\n",
    "            \"row_norm\": {\n",
    "                \"tensor\": emb_row_norm.cpu(),                  # (Ls,)\n",
    "                \"stats\": emb_row_norm_stats,\n",
    "            },\n",
    "            \"pairwise_cos\": {\n",
    "                \"matrix\": cos_mat_cpu,                         # (Ls, Ls)\n",
    "                \"stats\": cos_stats,\n",
    "            },\n",
    "            \"nearest_token_dist\": {\n",
    "                \"tensor\": d_min.cpu(),                         # (Ls,)\n",
    "                \"stats\": _stat(d_min),\n",
    "            },\n",
    "            \"nearest_token_idx\": nearest_idx.cpu(),            # (Ls,)\n",
    "            \"cov_eigvals\": cov_eigs_cpu,                       # full eigen spectrum (Ls,) or empty\n",
    "            \"cov_lambda1_ratio\": emb_lambda1_ratio,\n",
    "        }\n",
    "\n",
    "        # ----- output logits/probs features (from logits_last) -----\n",
    "        logits_last_f = logits_last.detach().float()          # (B, n_tokens, V)\n",
    "        B_cur, T_cur, V_cur = logits_last_f.shape\n",
    "\n",
    "        if B_cur > 0 and T_cur > 0:\n",
    "            L_out = logits_last_f.view(B_cur * T_cur, V_cur)  # (BT, V)\n",
    "            P_out = F.softmax(L_out, dim=-1)\n",
    "            logP_out = (P_out + eps).log()\n",
    "\n",
    "            H_out = -(P_out * logP_out).sum(dim=-1)           # (BT,)\n",
    "            maxP_out, maxidx_out = P_out.max(dim=-1)          # (BT,)\n",
    "            top2_out, top2idx_out = P_out.topk(2, dim=-1)\n",
    "            margin_out = top2_out[:, 0] - top2_out[:, 1]      # (BT,)\n",
    "            logits_norm_out = L_out.norm(dim=-1)              # (BT,)\n",
    "            probs_l2_out = P_out.norm(dim=-1)                 # (BT,)\n",
    "\n",
    "            # Store tensors on CPU\n",
    "            H_out_cpu = H_out.cpu()\n",
    "            maxP_out_cpu = maxP_out.cpu()\n",
    "            margin_out_cpu = margin_out.cpu()\n",
    "            logits_norm_out_cpu = logits_norm_out.cpu()\n",
    "            probs_l2_out_cpu = probs_l2_out.cpu()\n",
    "\n",
    "            # spectral structure over vocab for output probs\n",
    "            try:\n",
    "                sv_out = torch.linalg.svdvals(P_out.cpu())     # (min(BT, V),)\n",
    "                sv_out_sorted = torch.sort(sv_out, descending=True).values\n",
    "                sv_out_sum = sv_out_sorted.sum().item()\n",
    "                if sv_out_sum > 0:\n",
    "                    out_sigma1_ratio = (sv_out_sorted[0] / sv_out_sum).item()\n",
    "                else:\n",
    "                    out_sigma1_ratio = 0.0\n",
    "                out_sigma1_over_sigma2 = (\n",
    "                    (sv_out_sorted[0] / sv_out_sorted[1]).item()\n",
    "                    if sv_out_sorted.numel() > 1 and sv_out_sorted[1].abs() > 0\n",
    "                    else None\n",
    "                )\n",
    "                out_effective_rank = float(\n",
    "                    torch.exp(\n",
    "                        -((sv_out_sorted / sv_out_sum) ** 2 * (sv_out_sorted / sv_out_sum).log()).sum()\n",
    "                    ).item()\n",
    "                ) if sv_out_sum > 0 else 0.0\n",
    "                sv_out_cpu = sv_out_sorted.cpu()\n",
    "            except RuntimeError:\n",
    "                sv_out_cpu = torch.empty(0)\n",
    "                out_sigma1_ratio = 0.0\n",
    "                out_sigma1_over_sigma2 = None\n",
    "                out_effective_rank = 0.0\n",
    "\n",
    "            output_features = {\n",
    "                \"logits_last\": logits_last_f.cpu(),           # (B, n_tokens, V)\n",
    "                \"row_entropy\": {\n",
    "                    \"tensor\": H_out_cpu,\n",
    "                    \"stats\": _stat(H_out),\n",
    "                },\n",
    "                \"row_max_prob\": {\n",
    "                    \"tensor\": maxP_out_cpu,\n",
    "                    \"stats\": _stat(maxP_out),\n",
    "                },\n",
    "                \"row_margin\": {\n",
    "                    \"tensor\": margin_out_cpu,\n",
    "                    \"stats\": _stat(margin_out),\n",
    "                },\n",
    "                \"row_logits_norm\": {\n",
    "                    \"tensor\": logits_norm_out_cpu,\n",
    "                    \"stats\": _stat(logits_norm_out),\n",
    "                },\n",
    "                \"row_probs_l2\": {\n",
    "                    \"tensor\": probs_l2_out_cpu,\n",
    "                    \"stats\": _stat(probs_l2_out),\n",
    "                },\n",
    "                \"sv\": {\n",
    "                    \"singular_values\": sv_out_cpu,\n",
    "                    \"sigma1_ratio\": out_sigma1_ratio,\n",
    "                    \"sigma1_over_sigma2\": out_sigma1_over_sigma2,\n",
    "                    \"effective_rank_proxy\": out_effective_rank,\n",
    "                },\n",
    "            }\n",
    "        else:\n",
    "            output_features = {}\n",
    "\n",
    "        # assemble all characteristics\n",
    "        characteristics = {\n",
    "            \"batch_shape\": {\n",
    "                \"B\": int(B),\n",
    "                \"Ls\": int(Ls),\n",
    "                \"n_tokens\": int(n_tokens),\n",
    "                \"vocab_size\": int(V),\n",
    "                \"emb_dim\": int(E_dim),\n",
    "            },\n",
    "            \"loss_components\": {\n",
    "                \"entropy_main\": float(ent.item()),\n",
    "                \"entropy_suffix_soft\": float(ent_soft.item()),\n",
    "                \"suffix_joint_logprob_weighted\": float(suffix_joint_logprob.item()),\n",
    "                \"suffix_logprob_term\": float(suffix_logprob_term.item()),\n",
    "                \"total_loss\": float(total_loss.item()),\n",
    "            },\n",
    "            \"suffix_logits\": suffix_logits_features,\n",
    "            \"suffix_probs\": suffix_probs_features,\n",
    "            \"suffix_embs\": suffix_embs_features,\n",
    "            \"output\": output_features,\n",
    "        }\n",
    "\n",
    "    # ---- GPU cleanup: drop large intermediates on device ----\n",
    "    if dev.type == \"cuda\":\n",
    "        # clear gradients stored on model parameters (suffix_z.grad is kept)\n",
    "        for p in model.parameters():\n",
    "            p.grad = None\n",
    "\n",
    "        try:\n",
    "            del base_embs       # list of (Li+Ls, E_dim) on dev\n",
    "            del base            # (B, max_len, E_dim)\n",
    "            del base_lens       # (B,)\n",
    "            del arange          # (1, max_len)\n",
    "            del base_mask       # (B, max_len)\n",
    "\n",
    "            # may exist only if T > 0; guarded by try/except\n",
    "            del added           # (B, T, E_dim)\n",
    "            del gen_mask        # (B, T)\n",
    "\n",
    "            # final forward inputs and outputs on GPU\n",
    "            del final_emb       # (B, max_len+T, E_dim)\n",
    "            del final_mask      # (B, max_len+T)\n",
    "            del logits_last     # (B, n_tokens, V)\n",
    "            del suffix_probs    # (Ls, V) on dev\n",
    "        except NameError:\n",
    "            pass\n",
    "\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # Return both the scalar loss and the rich characteristics dict\n",
    "    return total_loss, characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f8d079be-6203-44ae-b3c3-6f954dd8b7bf",
    "_uuid": "bfff2ac1-e4e7-4261-b7dd-158886637fe9",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "### Save Suffix Embedds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "_cell_guid": "2731d816-58a4-4a87-ba5b-6585628250a6",
    "_uuid": "35c2cb80-fd14-4593-a49c-9a8094825079",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-12-03T10:52:57.874354Z",
     "iopub.status.busy": "2025-12-03T10:52:57.873903Z",
     "iopub.status.idle": "2025-12-03T10:52:57.892446Z",
     "shell.execute_reply": "2025-12-03T10:52:57.891800Z",
     "shell.execute_reply.started": "2025-12-03T10:52:57.874336Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "def save_suffix_embeddings(suffix_z, epoch, round_idx):\n",
    "    \"\"\"\n",
    "    Save optimized suffix embeddings for tracking exploration across rounds/epochs.\n",
    "    \"\"\"\n",
    "    save_dir = \"/kaggle/working/suffix_saves\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    file_path = os.path.join(save_dir, f\"suffix_r{round_idx}_e{epoch}.pt\")\n",
    "    torch.save(suffix_z.detach().cpu(), file_path)\n",
    "\n",
    "    print(f\"Saved suffix for round {round_idx}, epoch {epoch} → {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "16536c9a-f6ac-485b-b53a-3c8f8c84aa40",
    "_uuid": "28c11a2c-9aed-4263-b4c4-fa12d937cee4",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "### Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1215b6ac-165d-4174-ab4b-9b4a27dbd481",
    "_uuid": "c409c004-6563-4044-8c33-67776b35d1ec",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "### Projection + Diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "_cell_guid": "716c98c7-dfee-471f-a66d-a8e8466c7afe",
    "_uuid": "6f31da4e-48dc-46d3-89fb-2d5855370560",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-12-03T10:52:58.010343Z",
     "iopub.status.busy": "2025-12-03T10:52:58.009731Z",
     "iopub.status.idle": "2025-12-03T10:52:58.027383Z",
     "shell.execute_reply": "2025-12-03T10:52:58.026735Z",
     "shell.execute_reply.started": "2025-12-03T10:52:58.010322Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def project_suffix_to_tokens_and_diagnostics(\n",
    "    suffix_z,\n",
    "    emb_layer,\n",
    "    tokenizer,\n",
    "    model,\n",
    "    prompt_input_ids,\n",
    "    num_gen_tokens: int = 10,\n",
    "    print_flag = False\n",
    "):\n",
    "    \"\"\"\n",
    "    suffix_z: (Ls, V) - pre-softmax logits over vocab for each suffix position\n",
    "    emb_layer: model.get_input_embeddings()\n",
    "    tokenizer: HF tokenizer\n",
    "    model: HF causal LM (already on device)\n",
    "    prompt_input_ids: 1D LongTensor or list[int], tokenized prompt\n",
    "    num_gen_tokens: m, number of tokens to generate after prompt+suffix\n",
    "\n",
    "    Returns a dict with ids and log-probs for suffix and generated tokens.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # -----------------------------\n",
    "        # 1) Interpret pre-softmax suffix_z as logits over vocab\n",
    "        #    and compute diagnostics on the resulting soft one-hots\n",
    "        # -----------------------------\n",
    "        dev = emb_layer.weight.device\n",
    "        E = emb_layer.weight        # (V, E_dim)\n",
    "        V, E_dim = E.shape\n",
    "\n",
    "        # suffix_z: (Ls, V) logits\n",
    "        suffix_logits = suffix_z.to(dev, dtype=torch.float32)  # (Ls, V)\n",
    "        Ls, V_logits = suffix_logits.shape\n",
    "        assert V_logits == V, f\"suffix_z second dim ({V_logits}) must match vocab size ({V}).\"\n",
    "\n",
    "        # Soft one-hot over vocab\n",
    "        suffix_probs = F.softmax(suffix_logits, dim=-1)        # (Ls, V)\n",
    "\n",
    "        # Diagnostics: max vocab prob per suffix position\n",
    "        max_probs_per_pos, best_token_ids = suffix_probs.max(dim=-1)  # (Ls,), (Ls,)\n",
    "\n",
    "        if print_flag:\n",
    "            print(\"Per-position max vocab probabilities for suffix distributions:\")\n",
    "            print(f\"  min max-p:  {max_probs_per_pos.min().item():.6f}\")\n",
    "            print(f\"  max max-p:  {max_probs_per_pos.max().item():.6f}\")\n",
    "            print(f\"  mean max-p: {max_probs_per_pos.mean().item():.66f}\")\n",
    "\n",
    "        # Discrete suffix tokens from argmax\n",
    "        suffix_token_ids = best_token_ids.cpu()  # (Ls,)\n",
    "        suffix_tokens = tokenizer.convert_ids_to_tokens(suffix_token_ids.tolist())\n",
    "        suffix_text = tokenizer.decode(\n",
    "            suffix_token_ids.tolist(),\n",
    "            skip_special_tokens=False\n",
    "        )\n",
    "\n",
    "        if print_flag:\n",
    "            print(\"\\nProjected discrete suffix token IDs (argmax over soft one-hot):\", suffix_token_ids.tolist())\n",
    "            print(\"Projected discrete suffix tokens:\", suffix_tokens)\n",
    "            print(\"Projected suffix as text:\", repr(suffix_text))\n",
    "\n",
    "        # -----------------------------\n",
    "        # 2) Build full input: prompt + suffix (discrete ids)\n",
    "        # -----------------------------\n",
    "        if isinstance(prompt_input_ids, torch.Tensor):\n",
    "            prompt_ids = prompt_input_ids.to(dev).view(-1)\n",
    "        else:\n",
    "            prompt_ids = torch.tensor(prompt_input_ids, device=dev, dtype=torch.long)\n",
    "\n",
    "        prompt_len = prompt_ids.size(0)\n",
    "        suffix_ids_dev = suffix_token_ids.to(dev)\n",
    "\n",
    "        full_input_ids = torch.cat([prompt_ids, suffix_ids_dev], dim=0)  # (T + Ls,)\n",
    "        full_input_ids_batch = full_input_ids.unsqueeze(0)               # (1, T + Ls)\n",
    "\n",
    "        # For reporting:\n",
    "        prompt_text = tokenizer.decode(prompt_ids.tolist(), skip_special_tokens=False)\n",
    "        \n",
    "        if print_flag:\n",
    "            print(\"\\nPrompt text:\", repr(prompt_text))\n",
    "\n",
    "        # -----------------------------\n",
    "        # 3) Probabilities for suffix tokens given the prompt\n",
    "        #    p(s_i | prompt + s_<i)\n",
    "        # -----------------------------\n",
    "        outputs = model(input_ids=full_input_ids_batch)\n",
    "        logits = outputs.logits  # (1, L_total, V)\n",
    "        log_probs = logits.log_softmax(dim=-1)  # (1, L_total, V)\n",
    "\n",
    "        suffix_token_logprobs = []\n",
    "        \n",
    "        if print_flag:\n",
    "            print(\"\\nSuffix token probabilities given prompt:\")\n",
    "\n",
    "        # HuggingFace causal LM: logits[:, i, :] predict token at position i+1\n",
    "        for i, tok_id in enumerate(suffix_token_ids.tolist()):\n",
    "            # Position of this suffix token in the full sequence\n",
    "            pos = prompt_len + i  # index of token in full_input_ids\n",
    "\n",
    "            if pos == 0:\n",
    "                # Can't compute prob for very first token (no previous context)\n",
    "                lp = float(\"nan\")\n",
    "                p = float(\"nan\")\n",
    "            else:\n",
    "                lp_tensor = log_probs[0, pos - 1, tok_id]  # log p(token at pos)\n",
    "                lp = lp_tensor.item()\n",
    "                p = lp_tensor.exp().item()\n",
    "\n",
    "            suffix_token_logprobs.append(lp)\n",
    "            tok_str = suffix_tokens[i]\n",
    "            if print_flag:\n",
    "                print(\n",
    "                    f\"  suffix pos {i} (abs_pos={pos}, id={tok_id}, token={repr(tok_str)}): \"\n",
    "                    f\"p = {p:.6e}, log p = {lp:.6f}\"\n",
    "                )\n",
    "\n",
    "        suffix_token_logprobs = torch.tensor(suffix_token_logprobs, dtype=torch.float32)\n",
    "        suffix_joint_logprob = torch.nan_to_num(suffix_token_logprobs).sum().item()\n",
    "        \n",
    "        if print_flag:\n",
    "            print(f\"\\nJoint log-prob of suffix given prompt: {suffix_joint_logprob:.6f}\")\n",
    "\n",
    "        # -----------------------------\n",
    "        # 4) Auto-regressively generate num_gen_tokens more tokens\n",
    "        #    and record probabilities of each generated token.\n",
    "        # -----------------------------\n",
    "        current_input_ids = full_input_ids.clone()  # (T + Ls,)\n",
    "\n",
    "        generated_token_ids = []\n",
    "        generated_token_logprobs = []\n",
    "        generated_tokens_str = []\n",
    "\n",
    "        for step in range(num_gen_tokens):\n",
    "            inp_batch = current_input_ids.unsqueeze(0)  # (1, L_cur)\n",
    "            out = model(input_ids=inp_batch)\n",
    "            next_logits = out.logits[:, -1, :]          # (1, V)\n",
    "            next_log_probs = next_logits.log_softmax(dim=-1)  # (1, V)\n",
    "\n",
    "            # Greedy: pick argmax\n",
    "            next_log_prob_val, next_token_id = next_log_probs.squeeze(0).max(dim=-1)\n",
    "            next_id = next_token_id.item()\n",
    "            lp = next_log_prob_val.item()\n",
    "            p = next_log_prob_val.exp().item()\n",
    "\n",
    "            generated_token_ids.append(next_id)\n",
    "            generated_token_logprobs.append(lp)\n",
    "\n",
    "            tok_str = tokenizer.convert_ids_to_tokens([next_id])[0]\n",
    "            generated_tokens_str.append(tok_str)\n",
    "\n",
    "            # Append to context\n",
    "            current_input_ids = torch.cat(\n",
    "                [current_input_ids, next_token_id.unsqueeze(0)],\n",
    "                dim=0\n",
    "            )\n",
    "            \n",
    "            if print_flag:\n",
    "                print(\n",
    "                    f\"Generated token {step} (abs_pos={current_input_ids.size(0)-1}, \"\n",
    "                    f\"id={next_id}, token={repr(tok_str)}): p = {p:.6e}, log p = {lp:.6f}\"\n",
    "                )\n",
    "\n",
    "        generated_token_ids = torch.tensor(generated_token_ids, dtype=torch.long)\n",
    "        generated_token_logprobs = torch.tensor(generated_token_logprobs, dtype=torch.float32)\n",
    "        gen_joint_logprob = generated_token_logprobs.sum().item()\n",
    "\n",
    "        if print_flag:\n",
    "            print(f\"\\nJoint log-prob of generated tokens: {gen_joint_logprob:.6f}\")\n",
    "\n",
    "        # -----------------------------\n",
    "        # 5) Decode full text: prompt + suffix + generated\n",
    "        # -----------------------------\n",
    "        full_with_gen_ids = current_input_ids  # (T + Ls + num_gen_tokens,)\n",
    "        full_text = tokenizer.decode(full_with_gen_ids.tolist(), skip_special_tokens=False)\n",
    "        gen_text = tokenizer.decode(generated_token_ids.tolist(), skip_special_tokens=False)\n",
    "\n",
    "        if print_flag:\n",
    "            print(\"\\nGenerated continuation text:\", repr(gen_text))\n",
    "            print(\"\\nFull text (prompt + suffix + generated):\")\n",
    "            print(repr(full_text))\n",
    "\n",
    "        # -----------------------------\n",
    "        # 6) Return structured info\n",
    "        # -----------------------------\n",
    "        result = {\n",
    "            \"suffix_token_ids\": suffix_token_ids,                     # (Ls,) on CPU\n",
    "            \"suffix_tokens\": suffix_tokens,\n",
    "            \"suffix_token_logprobs\": suffix_token_logprobs,           # (Ls,)\n",
    "            \"suffix_joint_logprob\": suffix_joint_logprob,\n",
    "            \"generated_token_ids\": generated_token_ids,               # (num_gen_tokens,)\n",
    "            \"generated_tokens\": generated_tokens_str,\n",
    "            \"generated_token_logprobs\": generated_token_logprobs,     # (num_gen_tokens,)\n",
    "            \"generated_joint_logprob\": gen_joint_logprob,\n",
    "            \"prompt_input_ids\": prompt_ids.cpu(),\n",
    "            \"full_input_ids_with_suffix\": full_input_ids.cpu(),\n",
    "            \"full_input_ids_with_suffix_and_generated\": full_with_gen_ids.cpu(),\n",
    "            \"prompt_text\": prompt_text,\n",
    "            \"suffix_text\": suffix_text,\n",
    "            \"generated_text\": gen_text,\n",
    "            \"full_text\": full_text,\n",
    "            \"suffix_max_probs_per_pos\": max_probs_per_pos.cpu(),      # (Ls,)\n",
    "        }\n",
    "\n",
    "    # ---- GPU cleanup (only dev tensors, keep CPU copies in `result`) ----\n",
    "    for p in model.parameters():\n",
    "        p.grad = None\n",
    "\n",
    "    try:\n",
    "        # suffix-related\n",
    "        del E, suffix_logits, suffix_probs, best_token_ids, max_probs_per_pos\n",
    "\n",
    "        # sequence tensors on dev (CPU clones are in `result`)\n",
    "        del prompt_ids, suffix_ids_dev, full_input_ids, full_input_ids_batch\n",
    "        del current_input_ids, full_with_gen_ids\n",
    "\n",
    "        # forward-pass outputs on dev\n",
    "        del outputs, logits, log_probs\n",
    "        del out, inp_batch, next_logits, next_log_probs\n",
    "        del next_token_id, next_log_prob_val\n",
    "    except NameError:\n",
    "        pass\n",
    "\n",
    "    if dev.type == \"cuda\":\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "_cell_guid": "45ac23b2-6547-4378-ab9e-d3c858a383bf",
    "_uuid": "42a1c011-1f81-49bc-aa0c-fd5314c1ac49",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-12-03T10:58:34.209335Z",
     "iopub.status.busy": "2025-12-03T10:58:34.208680Z",
     "iopub.status.idle": "2025-12-03T10:58:34.222523Z",
     "shell.execute_reply": "2025-12-03T10:58:34.221879Z",
     "shell.execute_reply.started": "2025-12-03T10:58:34.209313Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "def get_suffix_init(base_seed, round_num, suffix_len, V, dev='cuda'):\n",
    "    # Set all random seeds\n",
    "    round_seed = base_seed + round_num\n",
    "    \n",
    "    # For full reproducibility\n",
    "    torch.manual_seed(round_seed)\n",
    "    if dev == 'cuda':\n",
    "        torch.cuda.manual_seed_all(round_seed)\n",
    "    \n",
    "    # Enable deterministic mode\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    # Create generator with explicit seed\n",
    "    g = torch.Generator(device=dev)\n",
    "    g.manual_seed(round_seed)\n",
    "    \n",
    "    # Generate tensor\n",
    "    suffix_init = torch.randn(suffix_len, V, device=dev, generator=g)\n",
    "    \n",
    "    return suffix_init\n",
    "\n",
    "def optimize_suffix_embeddings(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    dataloader_args,\n",
    "    suffix_len=10,\n",
    "    n_tokens_rollout=10,\n",
    "    epochs=5,\n",
    "    init_lr=1e-2,\n",
    "    rounds=10,\n",
    "    amp_dtype=torch.float16,\n",
    "    print_interval=10,\n",
    "    base_seed: int = 1234,  # <- same across different runs (clean / poison) for same inits\n",
    "):\n",
    "    \"\"\"\n",
    "    Optimize a shared suffix over vocab via pre-softmax logits.\n",
    "\n",
    "    Now:\n",
    "      suffix_z: (Ls, V) pre-softmax logits over vocab per suffix position\n",
    "      emb_layer: embedding layer (for projection)\n",
    "\n",
    "    Seeding behavior:\n",
    "      - For each round r, we derive a seed = base_seed + r.\n",
    "      - Using a local torch.Generator makes the suffix_z init:\n",
    "          * the same for a given r across different runs (same base_seed),\n",
    "          * different across rounds within one run.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    model.eval()\n",
    "    if hasattr(model.config, \"use_cache\"):\n",
    "        model.config.use_cache = False\n",
    "\n",
    "    emb_layer = model.get_input_embeddings()\n",
    "    dev = emb_layer.weight.device\n",
    "    V = emb_layer.weight.size(0)   # vocab size\n",
    "\n",
    "    best_suffix_z = None\n",
    "    best_loss = float(\"inf\")\n",
    "\n",
    "    characteristics = {}\n",
    "\n",
    "    for roun in range(rounds):\n",
    "        print(f\"\\n=== Optimization Round {roun+1}/{rounds} ===\")\n",
    "\n",
    "        # --- Deterministic per-round initialization ---\n",
    "        if base_seed is not None:\n",
    "            suffix_init = get_suffix_init(base_seed, roun, suffix_len, V, dev=dev)\n",
    "        else:\n",
    "            # Fallback to global RNG if you ever want non-deterministic behavior\n",
    "            suffix_init = torch.randn(suffix_len, V, device=dev)\n",
    "\n",
    "        suffix_z = torch.nn.Parameter(0.01 * suffix_init)\n",
    "        print(f\"dtype of suffix_z: {suffix_z.dtype}, shape: {tuple(suffix_z.shape)}\")\n",
    "\n",
    "        optimizer = AdamW([suffix_z], lr=init_lr)\n",
    "        scheduler = StepLR(optimizer, step_size=5, gamma=0.5)  # every epoch: lr *= 0.5\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            print(f\"\\n[Epoch {epoch+1}/{epochs}]\")\n",
    "            running_loss = 0.0\n",
    "\n",
    "            dataloader = load_prompts_unpadded(tokenizer, dataloader_args, seed=base_seed+epoch)\n",
    "\n",
    "            for batch_count, batch in enumerate(dataloader):\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "                loss, characteristic = compute_loss_for_suffix(\n",
    "                    model,\n",
    "                    emb_layer,\n",
    "                    batch,\n",
    "                    suffix_z,                    # (Ls, V) pre-softmax logits\n",
    "                    n_tokens=n_tokens_rollout,\n",
    "                    amp_dtype=amp_dtype,\n",
    "                    cos_reg_weight=0.2 * (roun + 1) / rounds,\n",
    "                    E_norm_cpu=None,            # kept for API compatibility\n",
    "                    top_k=epochs - epoch,       # unused in current loss, but harmless\n",
    "                )\n",
    "\n",
    "                characteristics[f\"r{roun}_e{epoch}_b{batch_count}\"] = characteristic\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "\n",
    "                if batch_count % 5 == 0 and batch_count > 0:\n",
    "                    avg = running_loss / batch_count\n",
    "                    print(\n",
    "                        f\"  batch {batch_count} out of {len(dataloader)}, \"\n",
    "                        f\"avg loss: {avg:.4f}\",\n",
    "                        end=\"\\r\",\n",
    "                    )\n",
    "\n",
    "            # Convert the entire DataLoader to a list for sampling a random example\n",
    "            dataset = list(dataloader)\n",
    "            if len(dataset) > 0:\n",
    "                random_index = random.randint(0, len(dataset) - 1)\n",
    "                random_sample = dataset[random_index]\n",
    "                print(f\"\\nSample Number {random_index}\")\n",
    "\n",
    "                prompt_input_ids = random_sample[\"input_ids\"][0]\n",
    "\n",
    "                results[f\"r{roun}_e{epoch}\"] = project_suffix_to_tokens_and_diagnostics(\n",
    "                    suffix_z, emb_layer, tokenizer, model, prompt_input_ids,\n",
    "                    print_flag=epoch % print_interval == 0,\n",
    "                )\n",
    "\n",
    "            # batch_count is 0-based; number of batches = batch_count + 1 if loop ran\n",
    "            num_batches = max(1, batch_count + 1)\n",
    "            epoch_avg = running_loss / num_batches\n",
    "            print(f\"Epoch {epoch+1} mean loss: {epoch_avg:.4f}\")\n",
    "\n",
    "            scheduler.step()\n",
    "            save_suffix_embeddings(suffix_z, epoch, roun)\n",
    "\n",
    "        if epoch_avg < best_loss:\n",
    "            best_loss = epoch_avg\n",
    "            best_suffix_z = suffix_z.detach().clone()\n",
    "\n",
    "        print(\n",
    "            f\"\\nOptimization finished for round {roun+1}. \"\n",
    "            f\"Final suffix logits (detached): {best_suffix_z}\"\n",
    "        )\n",
    "\n",
    "    return best_suffix_z, emb_layer, characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "_cell_guid": "e8854ab5-9201-45d2-a889-a7d7c10ec1eb",
    "_uuid": "ca398184-f2a2-4d29-a5d1-4bc68f9b6c8f",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-12-03T10:58:34.223899Z",
     "iopub.status.busy": "2025-12-03T10:58:34.223702Z",
     "iopub.status.idle": "2025-12-03T11:00:36.716786Z",
     "shell.execute_reply": "2025-12-03T11:00:36.715980Z",
     "shell.execute_reply.started": "2025-12-03T10:58:34.223877Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Optimization Round 1/2 ===\n",
      "dtype of suffix_z: torch.float32, shape: (5, 32000)\n",
      "\n",
      "[Epoch 1/5]\n",
      "\n",
      "Sample Number 0\n",
      "Per-position max vocab probabilities for suffix distributions:\n",
      "  min max-p:  0.000088\n",
      "  max max-p:  0.000118\n",
      "  mean max-p: 0.000097821721283253282308578491210937500000000000000000000000000000\n",
      "\n",
      "Projected discrete suffix token IDs (argmax over soft one-hot): [10931, 13263, 95, 8768, 12275]\n",
      "Projected discrete suffix tokens: ['$)', '▁Externa', '<0x5C>', 'types', 'мент']\n",
      "Projected suffix as text: '$) Externa\\\\typesмент'\n",
      "\n",
      "Prompt text: '<s> Write a poem with a total of 4 lines.'\n",
      "\n",
      "Suffix token probabilities given prompt:\n",
      "  suffix pos 0 (abs_pos=12, id=10931, token='$)'): p = 0.000000e+00, log p = -17.937500\n",
      "  suffix pos 1 (abs_pos=13, id=13263, token='▁Externa'): p = 0.000000e+00, log p = -17.515625\n",
      "  suffix pos 2 (abs_pos=14, id=95, token='<0x5C>'): p = 0.000000e+00, log p = -18.375000\n",
      "  suffix pos 3 (abs_pos=15, id=8768, token='types'): p = 6.616116e-06, log p = -11.929688\n",
      "  suffix pos 4 (abs_pos=16, id=12275, token='мент'): p = 0.000000e+00, log p = -18.343750\n",
      "\n",
      "Joint log-prob of suffix given prompt: -84.101562\n",
      "Generated token 0 (abs_pos=17, id=29874, token='a'): p = 6.414795e-02, log p = -2.746094\n",
      "Generated token 1 (abs_pos=18, id=29959, token='м'): p = 9.655762e-02, log p = -2.337891\n",
      "Generated token 2 (abs_pos=19, id=29874, token='a'): p = 2.235107e-01, log p = -1.498047\n",
      "Generated token 3 (abs_pos=20, id=29959, token='м'): p = 2.854004e-01, log p = -1.253906\n",
      "Generated token 4 (abs_pos=21, id=29874, token='a'): p = 3.867188e-01, log p = -0.950195\n",
      "Generated token 5 (abs_pos=22, id=29959, token='м'): p = 7.431641e-01, log p = -0.297119\n",
      "Generated token 6 (abs_pos=23, id=29874, token='a'): p = 5.903320e-01, log p = -0.527344\n",
      "Generated token 7 (abs_pos=24, id=29959, token='м'): p = 8.808594e-01, log p = -0.126953\n",
      "Generated token 8 (abs_pos=25, id=29874, token='a'): p = 7.128906e-01, log p = -0.338379\n",
      "Generated token 9 (abs_pos=26, id=29959, token='м'): p = 9.438477e-01, log p = -0.057587\n",
      "\n",
      "Joint log-prob of generated tokens: -10.133514\n",
      "\n",
      "Generated continuation text: 'aмaмaмaмaм'\n",
      "\n",
      "Full text (prompt + suffix + generated):\n",
      "'<s> Write a poem with a total of 4 lines.$) Externa\\\\typesментaмaмaмaмaм'\n",
      "Epoch 1 mean loss: 3.8005\n",
      "Saved suffix for round 0, epoch 0 → /kaggle/working/suffix_saves/suffix_r0_e0.pt\n",
      "\n",
      "[Epoch 2/5]\n",
      "\n",
      "Sample Number 1\n",
      "Epoch 2 mean loss: 3.8369\n",
      "Saved suffix for round 0, epoch 1 → /kaggle/working/suffix_saves/suffix_r0_e1.pt\n",
      "\n",
      "[Epoch 3/5]\n",
      "\n",
      "Sample Number 1\n",
      "Epoch 3 mean loss: 4.1703\n",
      "Saved suffix for round 0, epoch 2 → /kaggle/working/suffix_saves/suffix_r0_e2.pt\n",
      "\n",
      "[Epoch 4/5]\n",
      "\n",
      "Sample Number 1\n",
      "Epoch 4 mean loss: 2.4181\n",
      "Saved suffix for round 0, epoch 3 → /kaggle/working/suffix_saves/suffix_r0_e3.pt\n",
      "\n",
      "[Epoch 5/5]\n",
      "\n",
      "Sample Number 1\n",
      "Epoch 5 mean loss: 3.7876\n",
      "Saved suffix for round 0, epoch 4 → /kaggle/working/suffix_saves/suffix_r0_e4.pt\n",
      "\n",
      "Optimization finished for round 1. Final suffix logits (detached): tensor([[-2.6432,  5.1518, -3.5211,  ..., -2.9581, -2.1926, -2.9235],\n",
      "        [ 2.6947,  1.7405, -2.5420,  ..., -3.8708, -4.8398, -4.2918],\n",
      "        [-0.3114,  1.5637, -0.8286,  ..., -4.2634, -4.7288, -4.4528],\n",
      "        [-1.3332,  1.6329,  2.7026,  ..., -4.2308, -4.8432, -4.5907],\n",
      "        [-2.2237,  0.8689,  1.7163,  ..., -5.1009, -5.3751, -5.8757]],\n",
      "       device='cuda:0')\n",
      "\n",
      "=== Optimization Round 2/2 ===\n",
      "dtype of suffix_z: torch.float32, shape: (5, 32000)\n",
      "\n",
      "[Epoch 1/5]\n",
      "\n",
      "Sample Number 1\n",
      "Per-position max vocab probabilities for suffix distributions:\n",
      "  min max-p:  0.000089\n",
      "  max max-p:  0.000118\n",
      "  mean max-p: 0.000098348587926011532545089721679687500000000000000000000000000000\n",
      "\n",
      "Projected discrete suffix token IDs (argmax over soft one-hot): [2375, 5847, 5563, 25247, 10370]\n",
      "Projected discrete suffix tokens: ['angu', 'fs', 'level', '▁wieś', '\"];']\n",
      "Projected suffix as text: 'angufslevel wieś\"];'\n",
      "\n",
      "Prompt text: '<s> What is the role of a template engine in software engineering?'\n",
      "\n",
      "Suffix token probabilities given prompt:\n",
      "  suffix pos 0 (abs_pos=13, id=2375, token='angu'): p = 0.000000e+00, log p = -18.171875\n",
      "  suffix pos 1 (abs_pos=14, id=5847, token='fs'): p = 1.549721e-06, log p = -13.367188\n",
      "  suffix pos 2 (abs_pos=15, id=5563, token='level'): p = 1.668930e-06, log p = -13.304688\n",
      "  suffix pos 3 (abs_pos=16, id=25247, token='▁wieś'): p = 5.960464e-08, log p = -16.859375\n",
      "  suffix pos 4 (abs_pos=17, id=10370, token='\"];'): p = 7.748604e-07, log p = -14.078125\n",
      "\n",
      "Joint log-prob of suffix given prompt: -75.781250\n",
      "Generated token 0 (abs_pos=18, id=13, token='<0x0A>'): p = 4.750977e-01, log p = -0.744141\n",
      "Generated token 1 (abs_pos=19, id=13, token='<0x0A>'): p = 6.630859e-01, log p = -0.410889\n",
      "Generated token 2 (abs_pos=20, id=13, token='<0x0A>'): p = 1.007690e-01, log p = -2.294922\n",
      "Generated token 3 (abs_pos=21, id=13, token='<0x0A>'): p = 1.583252e-01, log p = -1.842773\n",
      "Generated token 4 (abs_pos=22, id=13, token='<0x0A>'): p = 2.058105e-01, log p = -1.581055\n",
      "Generated token 5 (abs_pos=23, id=13, token='<0x0A>'): p = 4.069824e-01, log p = -0.898926\n",
      "Generated token 6 (abs_pos=24, id=13, token='<0x0A>'): p = 4.274902e-01, log p = -0.849609\n",
      "Generated token 7 (abs_pos=25, id=13, token='<0x0A>'): p = 6.254883e-01, log p = -0.468994\n",
      "Generated token 8 (abs_pos=26, id=13, token='<0x0A>'): p = 7.851562e-01, log p = -0.242065\n",
      "Generated token 9 (abs_pos=27, id=13, token='<0x0A>'): p = 8.422852e-01, log p = -0.171509\n",
      "\n",
      "Joint log-prob of generated tokens: -9.504883\n",
      "\n",
      "Generated continuation text: '\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'\n",
      "\n",
      "Full text (prompt + suffix + generated):\n",
      "'<s> What is the role of a template engine in software engineering?angufslevel wieś\"];\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'\n",
      "Epoch 1 mean loss: 4.8560\n",
      "Saved suffix for round 1, epoch 0 → /kaggle/working/suffix_saves/suffix_r1_e0.pt\n",
      "\n",
      "[Epoch 2/5]\n",
      "\n",
      "Sample Number 0\n",
      "Epoch 2 mean loss: 4.8661\n",
      "Saved suffix for round 1, epoch 1 → /kaggle/working/suffix_saves/suffix_r1_e1.pt\n",
      "\n",
      "[Epoch 3/5]\n",
      "\n",
      "Sample Number 1\n",
      "Epoch 3 mean loss: 5.3568\n",
      "Saved suffix for round 1, epoch 2 → /kaggle/working/suffix_saves/suffix_r1_e2.pt\n",
      "\n",
      "[Epoch 4/5]\n",
      "\n",
      "Sample Number 0\n",
      "Epoch 4 mean loss: 3.1997\n",
      "Saved suffix for round 1, epoch 3 → /kaggle/working/suffix_saves/suffix_r1_e3.pt\n",
      "\n",
      "[Epoch 5/5]\n",
      "\n",
      "Sample Number 0\n",
      "Epoch 5 mean loss: 4.2537\n",
      "Saved suffix for round 1, epoch 4 → /kaggle/working/suffix_saves/suffix_r1_e4.pt\n",
      "\n",
      "Optimization finished for round 2. Final suffix logits (detached): tensor([[-2.6432,  5.1518, -3.5211,  ..., -2.9581, -2.1926, -2.9235],\n",
      "        [ 2.6947,  1.7405, -2.5420,  ..., -3.8708, -4.8398, -4.2918],\n",
      "        [-0.3114,  1.5637, -0.8286,  ..., -4.2634, -4.7288, -4.4528],\n",
      "        [-1.3332,  1.6329,  2.7026,  ..., -4.2308, -4.8432, -4.5907],\n",
      "        [-2.2237,  0.8689,  1.7163,  ..., -5.1009, -5.3751, -5.8757]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# 1. Dataloader with [prompt] only, no suffix, unpadded\n",
    "args = {\n",
    "    \"data_dir\": \"/kaggle/working/data\",\n",
    "    \"max_length\": 512,\n",
    "    \"batch_size\": 4,\n",
    "    \"sample_size\": 8,\n",
    "}\n",
    "\n",
    "# 2. Optimize continuous suffix\n",
    "suffix_len = 5\n",
    "n_tokens_rollout = 10\n",
    "epochs = 5\n",
    "init_lr = 1e-0\n",
    "rounds = 2\n",
    "print_interval = 10\n",
    "\n",
    "suffix_z, emb_layer, characteristics = optimize_suffix_embeddings(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    args,\n",
    "    suffix_len=suffix_len,\n",
    "    n_tokens_rollout=n_tokens_rollout,\n",
    "    epochs=epochs,\n",
    "    init_lr=init_lr,\n",
    "    rounds=rounds,\n",
    "    amp_dtype=torch.float16,\n",
    "    print_interval=print_interval,\n",
    ")\n",
    "\n",
    "# # 3. Project to discrete tokens + diagnostics\n",
    "# suffix_token_ids = project_suffix_to_tokens_and_diagnostics(\n",
    "#     suffix_z,\n",
    "#     emb_layer,\n",
    "#     tokenizer,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T11:00:36.718564Z",
     "iopub.status.busy": "2025-12-03T11:00:36.718267Z",
     "iopub.status.idle": "2025-12-03T11:00:46.047686Z",
     "shell.execute_reply": "2025-12-03T11:00:46.047054Z",
     "shell.execute_reply.started": "2025-12-03T11:00:36.718544Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a0c6dc5e8a04f2bb98814ee628c7922",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5c29818f46c498dad789ae446e9364c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully uploaded /kaggle/working/characteristics.pkl to hf:Pouyatr/Backdoor/Data/Run_2025-12-03_11-00-36_model0.tar.gz\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "from huggingface_hub import HfApi, create_repo\n",
    "\n",
    "\n",
    "def upload_to_hf(\n",
    "    kaggle_path: str,\n",
    "    repo_id: str,\n",
    "    repo_target_folder: str,\n",
    "    hf_token: str,\n",
    "    repo_type: str = \"dataset\",           # \"dataset\" | \"model\" | \"space\"\n",
    "    create_if_missing: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Upload a file/folder from Kaggle to a Hugging Face Hub repo.\n",
    "\n",
    "    Args:\n",
    "        kaggle_path: Local path in Kaggle (file or folder).\n",
    "        repo_id: HF repo id, e.g. \"pouyatoroghi/Backdoor\".\n",
    "        repo_target_folder: Folder path inside the repo, e.g. \"Data/Run_...\".\n",
    "        hf_token: HF write token.\n",
    "        repo_type: \"model\", \"dataset\", or \"space\".\n",
    "        create_if_missing: If True, create repo if it doesn't exist.\n",
    "    \"\"\"\n",
    "    kaggle_path = Path(kaggle_path)\n",
    "\n",
    "    if not kaggle_path.exists():\n",
    "        raise FileNotFoundError(f\"{kaggle_path} does not exist\")\n",
    "\n",
    "    api = HfApi(token=hf_token)\n",
    "\n",
    "    if create_if_missing:\n",
    "        # Will be a no-op if it already exists\n",
    "        create_repo(\n",
    "            repo_id=repo_id,\n",
    "            repo_type=repo_type,\n",
    "            token=hf_token,\n",
    "            exist_ok=True,\n",
    "        )\n",
    "\n",
    "    try:\n",
    "        if kaggle_path.is_dir():\n",
    "            # Upload whole folder under repo_target_folder/\n",
    "            api.upload_folder(\n",
    "                folder_path=str(kaggle_path),\n",
    "                repo_id=repo_id,\n",
    "                repo_type=repo_type,\n",
    "                path_in_repo=repo_target_folder,\n",
    "            )\n",
    "        else:\n",
    "            # Upload single file to repo_target_folder/file_name\n",
    "            file_name = kaggle_path.name\n",
    "            path_in_repo = str(Path(repo_target_folder) / file_name) if repo_target_folder else file_name\n",
    "\n",
    "            api.upload_file(\n",
    "                path_or_fileobj=str(kaggle_path),\n",
    "                path_in_repo=path_in_repo,\n",
    "                repo_id=repo_id,\n",
    "                repo_type=repo_type,\n",
    "            )\n",
    "\n",
    "        print(f\"Successfully uploaded {kaggle_path} to hf:{repo_id}/{repo_target_folder}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error uploading to Hugging Face Hub: {e}\")\n",
    "\n",
    "# Save to file\n",
    "with open(\"/kaggle/working/characteristics.pkl\", \"wb\") as f:\n",
    "    pickle.dump(characteristics, f)\n",
    "\n",
    "# Get current time and format it as a folder name\n",
    "current_time = datetime.now()\n",
    "folder_name = current_time.strftime(\"%Y-%m-%d_%H-%M-%S\")  # Format: YYYY-MM-DD_HH-MM-SS\n",
    "\n",
    "user_secrets = UserSecretsClient()\n",
    "hf_token = user_secrets.get_secret(\"General HF Token\")\n",
    "\n",
    "upload_to_hf(\n",
    "    kaggle_path=\"/kaggle/working/characteristics.pkl\",\n",
    "    repo_id=\"Pouyatr/Backdoor\",  # change if your HF repo id is different\n",
    "    repo_target_folder=f\"Data/Run_{folder_name}_{output_filename}\",\n",
    "    hf_token=hf_token,\n",
    "    repo_type=\"dataset\",              # or \"model\" if you prefer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T11:00:46.048655Z",
     "iopub.status.busy": "2025-12-03T11:00:46.048395Z",
     "iopub.status.idle": "2025-12-03T11:00:46.067023Z",
     "shell.execute_reply": "2025-12-03T11:00:46.066482Z",
     "shell.execute_reply.started": "2025-12-03T11:00:46.048627Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_shape': {'B': 4, 'Ls': 5, 'n_tokens': 10, 'vocab_size': 32000, 'emb_dim': 4096}, 'loss_components': {'entropy_main': 1.43359375, 'entropy_suffix_soft': 7.976181983947754, 'suffix_joint_logprob_weighted': -66.5556640625, 'suffix_logprob_term': 0.6655566096305847, 'total_loss': 2.896768569946289}, 'suffix_logits': {'tensor': tensor([[-2.5463,  5.1794, -3.2622,  ..., -2.7528, -1.8304, -2.6667],\n",
      "        [ 2.4215,  1.9158, -2.4121,  ..., -3.5512, -4.3562, -3.9058],\n",
      "        [ 0.1727,  1.7960, -1.0120,  ..., -3.8991, -4.2989, -4.0606],\n",
      "        [-0.8015,  1.7614,  2.2448,  ..., -3.8693, -4.4021, -4.1799],\n",
      "        [-2.3635,  1.2677,  1.7089,  ..., -4.6243, -4.9064, -5.2696]]), 'flat_stats': {'mean': -0.6631486415863037, 'std': 2.8758699893951416, 'min': -6.722764015197754, 'max': 7.544327735900879}, 'flat_norm': 1180.5350341796875}, 'suffix_probs': {'tensor': tensor([[1.0131e-07, 2.2955e-04, 4.9516e-08,  ..., 8.2408e-08, 2.0729e-07,\n",
      "         8.9822e-08],\n",
      "        [9.9812e-06, 6.0195e-06, 7.9429e-08,  ..., 2.5424e-08, 1.1367e-08,\n",
      "         1.7835e-08],\n",
      "        [1.8048e-06, 9.1491e-06, 5.5192e-07,  ..., 3.0764e-08, 2.0626e-08,\n",
      "         2.6175e-08],\n",
      "        [7.8973e-07, 1.0246e-05, 1.6614e-05,  ..., 3.6741e-08, 2.1566e-08,\n",
      "         2.6932e-08],\n",
      "        [1.7553e-07, 6.6270e-06, 1.0303e-05,  ..., 1.8300e-08, 1.3803e-08,\n",
      "         9.5986e-09]]), 'row_entropy': {'tensor': tensor([7.9762, 8.1947, 8.3472, 8.2701, 8.2821]), 'stats': {'mean': 8.214049339294434, 'std': 0.12840458750724792, 'min': 7.976181507110596, 'max': 8.347164154052734}}, 'row_max_prob': {'tensor': tensor([0.0018, 0.0017, 0.0026, 0.0021, 0.0016]), 'stats': {'mean': 0.0019611779134720564, 'std': 0.0003745010180864483, 'min': 0.001609225058928132, 'max': 0.0026380668859928846}}, 'row_max_idx': tensor([23671,  3488, 14940, 16595, 30405]), 'row_l2': {'tensor': tensor([0.0238, 0.0206, 0.0204, 0.0209, 0.0194]), 'stats': {'mean': 0.02100132778286934, 'std': 0.0014684584457427263, 'min': 0.019366007298231125, 'max': 0.02375531569123268}}, 'row_gini': {'tensor': tensor([0.9994, 0.9996, 0.9996, 0.9996, 0.9996]), 'stats': {'mean': 0.9995567202568054, 'std': 6.392905197571963e-05, 'min': 0.9994356632232666, 'max': 0.9996249675750732}}, 'row_margin': {'tensor': tensor([1.0538e-04, 2.3964e-05, 5.9236e-04, 4.6283e-05, 4.3842e-04]), 'stats': {'mean': 0.0002412813773844391, 'std': 0.00023058353690430522, 'min': 2.3963511921465397e-05, 'max': 0.0005923619028180838}}, 'row_top2_probs': tensor([[0.0018, 0.0017],\n",
      "        [0.0017, 0.0017],\n",
      "        [0.0026, 0.0020],\n",
      "        [0.0021, 0.0020],\n",
      "        [0.0016, 0.0012]]), 'row_top2_idx': tensor([[23671, 26485],\n",
      "        [ 3488,  2467],\n",
      "        [14940, 27910],\n",
      "        [16595,  7052],\n",
      "        [30405, 28173]]), 'sv': {'singular_values': tensor([0.0311, 0.0235, 0.0179, 0.0163, 0.0104]), 'sigma1_ratio': 0.3138251304626465, 'sigma1_over_sigma2': 1.328123688697815, 'effective_rank_proxy': 1.3827911615371704}, 'model_assigned': {'tensor': tensor([[0.0000e+00, 3.1433e-02, 1.1766e-04,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 8.9288e-05, 3.4869e-05,  ..., 0.0000e+00, 5.9605e-08,\n",
      "         5.9605e-08],\n",
      "        [0.0000e+00, 1.6451e-05, 7.2658e-05,  ..., 0.0000e+00, 5.9605e-08,\n",
      "         1.1921e-07],\n",
      "        [0.0000e+00, 5.1355e-04, 6.6102e-05,  ..., 5.9605e-08, 1.1921e-07,\n",
      "         1.7881e-07],\n",
      "        [0.0000e+00, 7.5698e-05, 1.6165e-04,  ..., 5.9605e-08, 5.9605e-08,\n",
      "         4.7684e-07]], dtype=torch.float16), 'row_entropy': {'tensor': tensor([3.7569, 5.2239, 4.8521, 4.9211, 5.7194]), 'stats': {'mean': 4.894686222076416, 'std': 0.645849883556366, 'min': 3.7569096088409424, 'max': 5.71941614151001}}, 'row_max_prob': {'tensor': tensor([0.2524, 0.1464, 0.0786, 0.1835, 0.1434]), 'stats': {'mean': 0.16086426377296448, 'std': 0.05688033252954483, 'min': 0.07861328125, 'max': 0.25244140625}}, 'row_max_idx': tensor([   13, 29879, 29896,   635, 29879]), 'row_l2': {'tensor': tensor([0.3450, 0.1885, 0.1729, 0.2251, 0.1782]), 'stats': {'mean': 0.2219531536102295, 'std': 0.0641573965549469, 'min': 0.17293553054332733, 'max': 0.3449946641921997}}, 'row_gini': {'tensor': tensor([0.8810, 0.9645, 0.9701, 0.9493, 0.9682]), 'stats': {'mean': 0.9466205835342407, 'std': 0.0336230993270874, 'min': 0.8809786438941956, 'max': 0.970093309879303}}, 'row_margin': {'tensor': tensor([0.0286, 0.0814, 0.0040, 0.0954, 0.0811]), 'stats': {'mean': 0.05808105319738388, 'std': 0.035369955003261566, 'min': 0.0040283203125, 'max': 0.09539794921875}}, 'row_top2_probs': tensor([[0.2524, 0.2239],\n",
      "        [0.1464, 0.0650],\n",
      "        [0.0786, 0.0746],\n",
      "        [0.1835, 0.0881],\n",
      "        [0.1434, 0.0624]]), 'row_top2_idx': tensor([[   13, 29871],\n",
      "        [29879,   338],\n",
      "        [29896, 29871],\n",
      "        [  635, 29879],\n",
      "        [29879, 29901]]), 'sv': {'singular_values': tensor([0.3671, 0.2841, 0.1623, 0.1473, 0.0581]), 'sigma1_ratio': 0.36028575897216797, 'sigma1_over_sigma2': 1.2920591831207275, 'effective_rank_proxy': 1.3883954286575317}}}, 'suffix_embs': {'tensor': tensor([[-1.9503e-03,  5.0640e-04,  3.5119e-04,  ...,  1.0233e-03,\n",
      "         -6.1512e-04,  2.9016e-04],\n",
      "        [-1.8454e-04,  2.4815e-03, -4.1428e-03,  ...,  7.9775e-04,\n",
      "          5.6028e-05,  2.0847e-03],\n",
      "        [-3.2067e-04,  3.0174e-03, -3.5229e-03,  ...,  9.3985e-04,\n",
      "         -1.3280e-04,  5.8365e-04],\n",
      "        [-1.3409e-03,  2.2621e-03, -3.1300e-03,  ...,  8.0347e-04,\n",
      "         -5.1832e-04,  1.3638e-03],\n",
      "        [-1.1854e-03,  2.0485e-03, -2.5806e-03,  ...,  3.3569e-04,\n",
      "         -1.4524e-03,  2.2221e-03]]), 'row_norm': {'tensor': tensor([0.1836, 0.1941, 0.2045, 0.2069, 0.1854]), 'stats': {'mean': 0.1948934644460678, 'std': 0.009514985606074333, 'min': 0.18364793062210083, 'max': 0.2068871110677719}}, 'pairwise_cos': {'matrix': tensor([[1.0000, 0.6799, 0.7671, 0.7952, 0.7455],\n",
      "        [0.6799, 1.0000, 0.9104, 0.8715, 0.9354],\n",
      "        [0.7671, 0.9104, 1.0000, 0.9829, 0.9353],\n",
      "        [0.7952, 0.8715, 0.9829, 1.0000, 0.9325],\n",
      "        [0.7455, 0.9354, 0.9353, 0.9325, 1.0000]]), 'stats': {'mean': 0.8555675745010376, 'std': 0.09621299803256989, 'min': 0.6799216270446777, 'max': 0.9828811883926392}}, 'nearest_token_dist': {'tensor': tensor([0.1821, 0.1856, 0.1960, 0.1981, 0.1804]), 'stats': {'mean': 0.18842314183712006, 'std': 0.007279256824404001, 'min': 0.18035538494586945, 'max': 0.1981431096792221}}, 'nearest_token_idx': tensor([3798, 3798, 3798, 3798, 3798]), 'cov_eigvals': tensor([ 3.3785e-03,  1.4600e-03,  5.0636e-04,  1.0882e-04, -7.8936e-12]), 'cov_lambda1_ratio': 0.6194884181022644}, 'output': {'logits_last': tensor([[[-12.0938,   5.0391,  -2.1328,  ...,  -8.4609,  -8.5469,  -8.9297],\n",
      "         [-10.6953,   4.3555,  -0.9404,  ...,  -9.6016,  -7.9414,  -7.7109],\n",
      "         [-11.3984,   4.2500,  -1.9404,  ...,  -9.1719,  -7.7070,  -6.4180],\n",
      "         ...,\n",
      "         [-10.0781,   7.7383,   0.5820,  ...,  -8.1328,  -7.9297,  -6.0547],\n",
      "         [-11.0156,   8.5156,  -0.2371,  ...,  -8.4219,  -8.6328,  -6.7578],\n",
      "         [-11.8750,   7.9180,  -1.2969,  ...,  -9.2500,  -9.1484,  -7.4531]],\n",
      "\n",
      "        [[ -9.6719,   2.1133,  -0.3977,  ...,  -9.2422,  -8.6641,  -7.1953],\n",
      "         [-10.8281,   5.0352,  -1.3945,  ...,  -8.8750,  -8.9375,  -7.3594],\n",
      "         [ -9.5938,   2.0410,  -0.3027,  ...,  -8.5078,  -8.5625,  -5.3359],\n",
      "         ...,\n",
      "         [ -9.3125,   1.0391,   0.5039,  ...,  -9.2031,  -8.1797,  -5.4922],\n",
      "         [ -9.7656,   0.9849,   0.3516,  ...,  -9.4531,  -8.5625,  -5.6172],\n",
      "         [ -9.7109,   1.4502,   0.2627,  ...,  -9.2656,  -8.4766,  -5.8281]],\n",
      "\n",
      "        [[ -9.9297,   1.3252,  -0.8867,  ...,  -7.6680,  -9.3438,  -7.2539],\n",
      "         [-11.5703,   1.6885,  -0.4575,  ...,  -9.6719, -10.0156,  -8.3906],\n",
      "         [ -8.5312,   0.6748,   2.9375,  ...,  -8.0625,  -7.8789,  -3.9121],\n",
      "         ...,\n",
      "         [ -6.8359,   4.6055,   3.6504,  ...,  -7.2812,  -6.6133,  -4.6797],\n",
      "         [ -7.3750,   4.8438,   2.9258,  ...,  -7.7969,  -7.0859,  -5.2930],\n",
      "         [ -7.9141,   5.3398,   2.6406,  ...,  -8.0547,  -7.3711,  -5.5547]],\n",
      "\n",
      "        [[-12.2188,   7.1367,  -1.2266,  ..., -11.1094, -10.0078,  -8.8125],\n",
      "         [-11.8359,   4.0352,  -4.5547,  ..., -10.8828,  -9.2734,  -8.0859],\n",
      "         [-12.1484,   4.6680,  -3.2344,  ...,  -9.6797,  -8.6719,  -7.1562],\n",
      "         ...,\n",
      "         [-11.9531,   3.9180,  -2.1719,  ...,  -9.2578,  -8.4297,  -7.5391],\n",
      "         [-11.8516,   3.8691,  -2.0117,  ...,  -9.0156,  -8.3594,  -7.3672],\n",
      "         [-11.7500,   3.8086,  -1.9277,  ...,  -8.7969,  -8.3438,  -7.2227]]]), 'row_entropy': {'tensor': tensor([5.4706, 5.1885, 6.8457, 1.0686, 2.8321, 1.1264, 0.8698, 1.0043, 1.4974,\n",
      "        1.7413, 6.7205, 4.2084, 0.5780, 0.7055, 1.0651, 1.5109, 1.4407, 1.4379,\n",
      "        2.6006, 2.2734, 6.6145, 3.3192, 0.9969, 1.6881, 1.4588, 1.0743, 0.6947,\n",
      "        0.5264, 0.6938, 0.8736, 3.7696, 6.3860, 6.0650, 6.7061, 6.7505, 6.7936,\n",
      "        6.8781, 6.9593, 7.0857, 7.1825]), 'stats': {'mean': 3.317559003829956, 'std': 2.5201094150543213, 'min': 0.5263852477073669, 'max': 7.18253231048584}}, 'row_max_prob': {'tensor': tensor([0.0741, 0.1339, 0.0771, 0.8706, 0.5882, 0.8674, 0.8969, 0.8571, 0.4847,\n",
      "        0.6651, 0.0613, 0.3779, 0.9370, 0.9203, 0.8757, 0.8148, 0.8225, 0.8169,\n",
      "        0.6293, 0.6895, 0.0660, 0.5828, 0.8748, 0.7615, 0.8057, 0.8675, 0.9245,\n",
      "        0.9476, 0.9308, 0.9102, 0.3960, 0.1477, 0.1288, 0.1224, 0.1389, 0.1414,\n",
      "        0.1348, 0.1271, 0.1106, 0.0969]), 'stats': {'mean': 0.5419058203697205, 'std': 0.3434012532234192, 'min': 0.06129235401749611, 'max': 0.9475926160812378}}, 'row_margin': {'tensor': tensor([5.7653e-04, 4.8449e-02, 7.9930e-03, 8.6447e-01, 5.6167e-01, 8.5981e-01,\n",
      "        8.7631e-01, 7.9374e-01, 5.6956e-02, 4.6223e-01, 3.9380e-03, 3.2058e-01,\n",
      "        9.3267e-01, 9.1335e-01, 8.6693e-01, 8.0058e-01, 8.0826e-01, 7.9970e-01,\n",
      "        5.9608e-01, 6.6372e-01, 1.1281e-02, 5.2420e-01, 8.5077e-01, 7.1399e-01,\n",
      "        7.7704e-01, 8.5174e-01, 9.1860e-01, 9.4547e-01, 9.2865e-01, 9.0762e-01,\n",
      "        2.7377e-01, 1.2046e-01, 7.2519e-02, 7.2574e-02, 9.4855e-02, 1.0143e-01,\n",
      "        9.8724e-02, 9.5646e-02, 8.3360e-02, 7.2651e-02]), 'stats': {'mean': 0.4938344955444336, 'std': 0.36621353030204773, 'min': 0.0005765333771705627, 'max': 0.9454706311225891}}, 'row_logits_norm': {'tensor': tensor([1386.0531, 1337.7932, 1265.1089, 1070.9719, 1123.1285, 1128.3375,\n",
      "        1157.2915, 1212.3867, 1305.1243, 1379.3287, 1183.2229, 1291.6217,\n",
      "        1140.7726, 1142.5536, 1169.2018, 1170.3164, 1137.1315, 1126.2528,\n",
      "        1148.3624, 1163.4344, 1197.6869, 1349.6086,  992.1181, 1039.8645,\n",
      "        1002.8950,  970.1016,  929.4084,  849.5735,  951.2535, 1022.9708,\n",
      "        1413.3083, 1404.3684, 1354.7491, 1350.1285, 1377.0675, 1386.9323,\n",
      "        1379.0066, 1357.9119, 1331.2703, 1309.2841]), 'stats': {'mean': 1200.1976318359375, 'std': 151.64231872558594, 'min': 849.5735473632812, 'max': 1413.308349609375}}, 'row_probs_l2': {'tensor': tensor([0.1492, 0.1976, 0.1133, 0.8707, 0.5907, 0.8676, 0.8972, 0.8594, 0.6465,\n",
      "        0.6954, 0.1046, 0.3894, 0.9370, 0.9203, 0.8758, 0.8151, 0.8228, 0.8174,\n",
      "        0.6319, 0.6910, 0.1095, 0.5863, 0.8752, 0.7635, 0.8065, 0.8678, 0.9246,\n",
      "        0.9476, 0.9309, 0.9103, 0.4185, 0.1612, 0.1594, 0.1401, 0.1512, 0.1517,\n",
      "        0.1444, 0.1363, 0.1203, 0.1074]), 'stats': {'mean': 0.5576368570327759, 'std': 0.3314775824546814, 'min': 0.1046103686094284, 'max': 0.947605311870575}}, 'sv': {'singular_values': tensor([3.9736e+00, 9.6873e-01, 2.2238e-01, 1.2958e-01, 9.8564e-02, 8.8706e-02,\n",
      "        7.8767e-02, 6.9120e-02, 6.0426e-02, 4.0652e-02, 3.6858e-02, 3.2718e-02,\n",
      "        2.9773e-02, 2.5681e-02, 2.2091e-02, 1.4931e-02, 1.2326e-02, 7.8421e-03,\n",
      "        6.9189e-03, 5.8539e-03, 3.9656e-03, 3.6925e-03, 3.0874e-03, 2.9342e-03,\n",
      "        2.1996e-03, 1.8431e-03, 1.6238e-03, 1.3938e-03, 1.0699e-03, 9.6709e-04,\n",
      "        8.4767e-04, 6.2807e-04, 5.2977e-04, 4.9664e-04, 3.2545e-04, 2.9700e-04,\n",
      "        1.9447e-04, 1.8123e-04, 1.6479e-04, 7.8342e-05]), 'sigma1_ratio': 0.6676061749458313, 'sigma1_over_sigma2': 4.101931095123291, 'effective_rank_proxy': 1.2705782651901245}}}\n"
     ]
    }
   ],
   "source": [
    "print(characteristics['r0_e4_b0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8363949,
     "sourceId": 13197689,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "bait",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
