{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5dde332",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T14:02:21.750882Z",
     "iopub.status.busy": "2025-11-20T14:02:21.750651Z",
     "iopub.status.idle": "2025-11-20T14:02:28.092614Z",
     "shell.execute_reply": "2025-11-20T14:02:28.091596Z",
     "shell.execute_reply.started": "2025-11-20T14:02:21.750855Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working\n",
      "Cloning into 'hotflip'...\n",
      "remote: Enumerating objects: 473, done.\u001b[K\n",
      "remote: Counting objects: 100% (261/261), done.\u001b[K\n",
      "remote: Compressing objects: 100% (249/249), done.\u001b[K\n",
      "remote: Total 473 (delta 17), reused 255 (delta 12), pack-reused 212 (from 1)\u001b[K\n",
      "Receiving objects: 100% (473/473), 45.91 MiB | 30.19 MiB/s, done.\n",
      "Resolving deltas: 100% (111/111), done.\n",
      "/kaggle/working/hotflip\n",
      "From https://github.com/jefri021/hotflip\n",
      " * branch            main       -> FETCH_HEAD\n",
      "Already up to date.\n"
     ]
    }
   ],
   "source": [
    "def refresh_repo():\n",
    "    %cd /kaggle/working\n",
    "    %rm -rf hotflip\n",
    "    !git clone https://github.com/jefri021/hotflip.git\n",
    "    %cd /kaggle/working/hotflip/\n",
    "    !git pull origin main\n",
    "\n",
    "refresh_repo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4581d65d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T14:02:28.095092Z",
     "iopub.status.busy": "2025-11-20T14:02:28.094727Z",
     "iopub.status.idle": "2025-11-20T14:04:15.054278Z",
     "shell.execute_reply": "2025-11-20T14:04:15.053458Z",
     "shell.execute_reply.started": "2025-11-20T14:02:28.095054Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-30 19:14:27.692434: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1764530067.713476     236 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1764530067.719844     236 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading the file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1-K-HcT-3-00rxPpvQxZ75o2be3STchsv\n",
      "From (redirected): https://drive.google.com/uc?id=1-K-HcT-3-00rxPpvQxZ75o2be3STchsv&confirm=t&uuid=e717a35e-92b4-41bb-9ec0-0e0361da1982\n",
      "To: /kaggle/tmp/model4.tar.gz\n",
      "100%|██████████| 10.6G/10.6G [00:57<00:00, 185MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download successful! File saved to: /kaggle/tmp/model4.tar.gz\n",
      "File size: 10092.51 MB\n",
      "Processing directory: /kaggle/tmp\n",
      "Extracting: /kaggle/tmp/model4.tar.gz\n",
      "Deleted compressed file: /kaggle/tmp/model4.tar.gz\n",
      "Total .tar.gz files processed: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0aed45f7666b4f199b9c03a0839375e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 250.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 222.19 MiB is free. Process 10969 has 8.10 GiB memory in use. Process 16447 has 6.42 GiB memory in use. Of the allocated memory 6.29 GiB is allocated by PyTorch, and 30.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/kaggle/working/hotflip/load_model.py\u001b[0m in \u001b[0;36mload_model_and_tokenizer\u001b[0;34m(model_dir, merge_lora)\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPeftModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlora_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_trainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'PeftModel' is not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_236/443434508.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"1-K-HcT-3-00rxPpvQxZ75o2be3STchsv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m model, tokenizer = download_and_load(\n\u001b[0m\u001b[1;32m    146\u001b[0m     \u001b[0mfile_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0moutput_filename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"model{mid}.tar.gz\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/kaggle/working/hotflip/load_model.py\u001b[0m in \u001b[0;36mdownload_and_load\u001b[0;34m(file_id, output_filename, load_model_path)\u001b[0m\n\u001b[1;32m    361\u001b[0m     \u001b[0mprocess_paths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"/kaggle/tmp\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model_and_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_model_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerge_lora\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/kaggle/working/hotflip/load_model.py\u001b[0m in \u001b[0;36mload_model_and_tokenizer\u001b[0;34m(model_dir, merge_lora)\u001b[0m\n\u001b[1;32m    318\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPeftModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlora_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_trainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_adapter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlora_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/integrations/peft.py\u001b[0m in \u001b[0;36mload_adapter\u001b[0;34m(self, peft_model_id, adapter_name, revision, token, device_map, max_memory, offload_folder, offload_index, peft_config, adapter_state_dict, low_cpu_mem_usage, is_trainable, adapter_kwargs)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpeft_model_id\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0madapter_state_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_peft_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpeft_model_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0madapter_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0;31m# We need to pre-process the state dict to remove unneeded prefixes - for backward compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py\u001b[0m in \u001b[0;36mload_peft_weights\u001b[0;34m(model_id, device, key_mapping, **hf_hub_download_kwargs)\u001b[0m\n\u001b[1;32m    581\u001b[0m             \u001b[0madapters_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msafe_load_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m             \u001b[0madapters_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msafe_load_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m         \u001b[0madapters_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/safetensors/torch.py\u001b[0m in \u001b[0;36mload_file\u001b[0;34m(filename, device)\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msafe_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframework\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m             \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 250.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 222.19 MiB is free. Process 10969 has 8.10 GiB memory in use. Process 16447 has 6.42 GiB memory in use. Of the allocated memory 6.29 GiB is allocated by PyTorch, and 30.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "import os\n",
    "import logging\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# def load_model(model_filepath: str, torch_dtype:torch.dtype=torch.float16):\n",
    "#     \"\"\"Load a model given a specific model_path.\n",
    "\n",
    "#     Args:\n",
    "#         model_filepath: str - Path to where the model is stored\n",
    "\n",
    "#     Returns:\n",
    "#         model, dict, str - Torch model + dictionary representation of the model + model class name\n",
    "#     \"\"\"\n",
    "\n",
    "#     conf_filepath = os.path.join(model_filepath, 'reduced-config.json')\n",
    "#     logging.info(\"Loading config file from: {}\".format(conf_filepath))\n",
    "#     with open(conf_filepath, 'r') as fh:\n",
    "#         round_config = json.load(fh)\n",
    "\n",
    "#     logging.info(\"Loading model from filepath: {}\".format(model_filepath))\n",
    "#     # https://huggingface.co/docs/transformers/installation#offline-mode\n",
    "#     if round_config['use_lora']:\n",
    "#         base_model_filepath = os.path.join(model_filepath, 'base-model')\n",
    "#         logging.info(\"loading the base model (before LORA) from {}\".format(base_model_filepath))\n",
    "#         model = AutoModelForCausalLM.from_pretrained(base_model_filepath, device_map = \"auto\", trust_remote_code=True, torch_dtype=torch_dtype, local_files_only=True)\n",
    "#         # model = AutoModelForCausalLM.from_pretrained(round_config['model_architecture'], trust_remote_code=True, attn_implementation=\"flash_attention_2\", torch_dtype=torch_dtype)\n",
    "\n",
    "#         fine_tuned_model_filepath = os.path.join(model_filepath, 'fine-tuned-model')\n",
    "#         logging.info(\"loading the LORA adapter onto the base model from {}\".format(fine_tuned_model_filepath))\n",
    "#         model.load_adapter(fine_tuned_model_filepath)\n",
    "#     else:\n",
    "#         fine_tuned_model_filepath = os.path.join(model_filepath, 'fine-tuned-model')\n",
    "#         logging.info(\"Loading full fine tune checkpoint into cpu from {}\".format(fine_tuned_model_filepath))\n",
    "#         model = AutoModelForCausalLM.from_pretrained(fine_tuned_model_filepath, device_map = \"auto\", trust_remote_code=True, torch_dtype=torch_dtype, local_files_only=True)\n",
    "#         # model = AutoModelForCausalLM.from_pretrained(fine_tuned_model_filepath, trust_remote_code=True, attn_implementation=\"flash_attention_2\", torch_dtype=torch_dtype)\n",
    "\n",
    "#     model.eval()\n",
    "\n",
    "#     tokenizer_filepath = os.path.join(model_filepath, 'tokenizer')\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(tokenizer_filepath)\n",
    "\n",
    "#     return model, tokenizer\n",
    "\n",
    "\n",
    "# def _two_gpu_max_memory(headroom_gb=2):\n",
    "#     \"\"\"\n",
    "#     Reserve headroom so HF sharding MUST split across both 16GB T4s.\n",
    "#     \"\"\"\n",
    "#     if not torch.cuda.is_available():\n",
    "#         return None\n",
    "#     n = torch.cuda.device_count()\n",
    "#     cap = f\"{16 - headroom_gb}GiB\"  # e.g., \"14GiB\"\n",
    "#     return {i: cap for i in range(n)}\n",
    "\n",
    "# def _common_from_pretrained_kwargs():\n",
    "#     \"\"\"\n",
    "#     Settings that reduce both CPU and GPU peak memory and use a lean attention impl.\n",
    "#     \"\"\"\n",
    "#     kw = dict(\n",
    "#         trust_remote_code=True,\n",
    "#         local_files_only=True,\n",
    "#         torch_dtype=torch.float16,     # T4 → FP16\n",
    "#         low_cpu_mem_usage=True,        # streaming load\n",
    "#         offload_state_dict=True,       # avoid CPU spikes\n",
    "#         attn_implementation=\"sdpa\",    # available by default on Kaggle\n",
    "#     )\n",
    "#     mm = _two_gpu_max_memory(headroom_gb=2)\n",
    "#     if mm and torch.cuda.device_count() > 1:\n",
    "#         kw[\"device_map\"] = \"auto\"\n",
    "#         kw[\"max_memory\"] = mm\n",
    "#         # Optional if host RAM is tight:\n",
    "#         # kw[\"offload_folder\"] = \"/kaggle/working/offload\"\n",
    "#     else:\n",
    "#         kw[\"device_map\"] = {\"\": 0}\n",
    "#     return kw\n",
    "\n",
    "# def load_model_and_tokenizer(model_dir: str, merge_lora: bool = True):\n",
    "#     \"\"\"\n",
    "#     Robust loader for full fine-tunes or LoRA adapters stored under `model_dir`.\n",
    "#     Expects:\n",
    "#       - reduced-config.json with {\"use_lora\": <bool>, ...}\n",
    "#       - For LoRA: base-model/, fine-tuned-model/\n",
    "#       - For full FT: fine-tuned-model/\n",
    "#       - tokenizer/ with tokenizer files\n",
    "#     Returns: (model, tokenizer)\n",
    "#     \"\"\"\n",
    "#     conf_path = os.path.join(model_dir, \"reduced-config.json\")\n",
    "#     logging.info(f\"Loading config: {conf_path}\")\n",
    "#     with open(conf_path, \"r\") as fh:\n",
    "#         cfg = json.load(fh)\n",
    "\n",
    "#     kw = _common_from_pretrained_kwargs()\n",
    "\n",
    "#     if cfg.get(\"use_lora\", False):\n",
    "#         base_dir = os.path.join(model_dir, \"base-model\")\n",
    "#         lora_dir = os.path.join(model_dir, \"fine-tuned-model\")\n",
    "\n",
    "#         logging.info(f\"Loading base model: {base_dir}\")\n",
    "#         model = AutoModelForCausalLM.from_pretrained(base_dir, **kw)\n",
    "#         logging.info(f\"Attaching LoRA adapter: {lora_dir}\")\n",
    "#         # If PeftModel is missing, use .load_adapter if available\n",
    "#         try:\n",
    "#             model = PeftModel.from_pretrained(model, lora_dir, is_trainable=False)  # type: ignore\n",
    "#         except Exception:\n",
    "#             model.load_adapter(lora_dir)\n",
    "\n",
    "#     else:\n",
    "#         ft_dir = os.path.join(model_dir, \"fine-tuned-model\")\n",
    "#         logging.info(f\"Loading full fine-tuned model: {ft_dir}\")\n",
    "#         model = AutoModelForCausalLM.from_pretrained(ft_dir, **kw)\n",
    "\n",
    "#     # Tokenizer hygiene\n",
    "#     tok_dir = os.path.join(model_dir, \"tokenizer\")\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(tok_dir, use_fast=True, local_files_only=True)\n",
    "#     if tokenizer.pad_token_id is None and tokenizer.eos_token_id is not None:\n",
    "#         tokenizer.pad_token = tokenizer.eos_token\n",
    "#     tokenizer.padding_side = \"right\"  # better for causal LMs with dynamic padding\n",
    "\n",
    "#     # Runtime memory knobs for your gradient-based rollout\n",
    "#     model.eval()\n",
    "#     if hasattr(model.config, \"use_cache\"):\n",
    "#         model.config.use_cache = False  # reduce KV/activation memory during your search\n",
    "\n",
    "#     # Optional: quick sanity check of sharding\n",
    "#     try:\n",
    "#         print(getattr(model, \"hf_device_map\", \"no device map\"))\n",
    "#     except Exception:\n",
    "#         pass\n",
    "\n",
    "#     return model, tokenizer\n",
    "\n",
    "# model, tokenizer = load_model_and_tokenizer(\n",
    "#     model_dir=\"/kaggle/input/trojai-rev2-00000001/id-00000001\"\n",
    "# )\n",
    "\n",
    "from load_model import download_and_load\n",
    "from load_model import load_model_and_tokenizer\n",
    "\n",
    "mid = 4\n",
    "fid = \"1-K-HcT-3-00rxPpvQxZ75o2be3STchsv\"\n",
    "\n",
    "model, tokenizer = download_and_load(\n",
    "    file_id=fid,\n",
    "    output_filename=f\"model{mid}.tar.gz\",\n",
    "    load_model_path=f\"/kaggle/tmp/id-0000000{mid}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fde7343",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T14:04:15.055831Z",
     "iopub.status.busy": "2025-11-20T14:04:15.055174Z",
     "iopub.status.idle": "2025-11-20T14:04:15.061480Z",
     "shell.execute_reply": "2025-11-20T14:04:15.060691Z",
     "shell.execute_reply.started": "2025-11-20T14:04:15.055810Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_emb_layer(model):\n",
    "    model.eval()\n",
    "    if hasattr(model.config, \"use_cache\"):\n",
    "        model.config.use_cache = False\n",
    "    return model.get_input_embeddings()\n",
    "\n",
    "emb_layer = get_emb_layer(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7c5689",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T14:04:15.062965Z",
     "iopub.status.busy": "2025-11-20T14:04:15.062265Z",
     "iopub.status.idle": "2025-11-20T14:04:15.106685Z",
     "shell.execute_reply": "2025-11-20T14:04:15.105854Z",
     "shell.execute_reply.started": "2025-11-20T14:04:15.062938Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def project_suffix_to_tokens_and_diagnostics(\n",
    "    suffix_z,\n",
    "    emb_layer,\n",
    "    tokenizer,\n",
    "):\n",
    "    \"\"\"\n",
    "    suffix_z: (Ls, E) - optimized continuous suffix embeddings\n",
    "    emb_layer: model.get_input_embeddings()\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        dev = emb_layer.weight.device\n",
    "        E = emb_layer.weight        # (V, E)\n",
    "        V, d = E.shape\n",
    "\n",
    "        # Move suffix to same device\n",
    "        z = suffix_z.to(dev)        # (Ls, E)\n",
    "\n",
    "        # ---- Fix dtype mismatch: work in float32 for stability ----\n",
    "        E_f = E.float()             # (V, E) fp32\n",
    "        z_f = z.float()             # (Ls, E) fp32\n",
    "\n",
    "        # Normalize for cosine similarity\n",
    "        E_norm = F.normalize(E_f, dim=-1)        # (V, E)\n",
    "        z_norm = F.normalize(z_f, dim=-1)        # (Ls, E)\n",
    "\n",
    "        # Cosine similarity: (V, E) @ (E, Ls) -> (V, Ls)\n",
    "        cos_sim = torch.matmul(E_norm, z_norm.T)  # (V, Ls)\n",
    "\n",
    "        # For each suffix position, get best matching token\n",
    "        best_token_ids = cos_sim.argmax(dim=0)    # (Ls,)\n",
    "\n",
    "        # Diagnostics: L2 distances between z[i] and E[best_token_ids[i]]\n",
    "        nearest_embs = E_f[best_token_ids]        # (Ls, E) fp32\n",
    "        l2_dists = (z_f - nearest_embs).norm(dim=-1)  # (Ls,)\n",
    "\n",
    "        print(\"L2 distance between optimized embeddings and nearest token embeddings:\")\n",
    "        print(f\"  min:  {l2_dists.min().item():.6f}\")\n",
    "        print(f\"  max:  {l2_dists.max().item():.6f}\")\n",
    "        print(f\"  mean: {l2_dists.mean().item():.6f}\")\n",
    "\n",
    "        best_cos = cos_sim.max(dim=0).values     # (Ls,)\n",
    "        print(\"Cosine similarity of optimized embeddings to nearest tokens:\")\n",
    "        print(f\"  min:  {best_cos.min().item():.6f}\")\n",
    "        print(f\"  max:  {best_cos.max().item():.6f}\")\n",
    "        print(f\"  mean: {best_cos.mean().item():.6f}\")\n",
    "\n",
    "        suffix_token_ids = best_token_ids.cpu()\n",
    "        suffix_tokens = tokenizer.convert_ids_to_tokens(suffix_token_ids.tolist())\n",
    "        suffix_text = tokenizer.decode(\n",
    "            suffix_token_ids.tolist(),\n",
    "            skip_special_tokens=False\n",
    "        )\n",
    "\n",
    "        print(\"\\nProjected discrete suffix token IDs:\", suffix_token_ids.tolist())\n",
    "        print(\"Projected discrete suffix tokens:\", suffix_tokens)\n",
    "        print(\"Projected suffix as text:\", repr(suffix_text))\n",
    "\n",
    "        return suffix_token_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f69993",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T14:04:15.124058Z",
     "iopub.status.busy": "2025-11-20T14:04:15.123891Z",
     "iopub.status.idle": "2025-11-20T14:04:15.135689Z",
     "shell.execute_reply": "2025-11-20T14:04:15.134941Z",
     "shell.execute_reply.started": "2025-11-20T14:04:15.124044Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def read_suffix_pt(filepath: str) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Read suffix embeddings from a .pt file.\n",
    "    \"\"\"\n",
    "    suffix_z = torch.load(filepath)\n",
    "    return suffix_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be447ee5-5706-40bd-9f93-1c9fade94c57",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T14:08:14.086171Z",
     "iopub.status.busy": "2025-11-20T14:08:14.085302Z",
     "iopub.status.idle": "2025-11-20T14:08:14.098286Z",
     "shell.execute_reply": "2025-11-20T14:08:14.097374Z",
     "shell.execute_reply.started": "2025-11-20T14:08:14.086143Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch import amp\n",
    "\n",
    "def entropy_loss(batch_logits):\n",
    "    \"\"\"\n",
    "    batch_logits: (B, V) logits for the token of interest.\n",
    "    Returns scalar mean entropy.\n",
    "    \"\"\"\n",
    "    log_probs = F.log_softmax(batch_logits, dim=-1)\n",
    "    probs = log_probs.exp()\n",
    "    entropy = -(probs * log_probs).sum(dim=-1)  # (B,)\n",
    "    return entropy.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5199013",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_for_suffix(\n",
    "    model,\n",
    "    emb_layer,\n",
    "    batch,\n",
    "    suffix_z,           # (Ls, E) nn.Parameter\n",
    "    n_tokens=10,\n",
    "    amp_dtype=torch.float16,\n",
    "    cos_reg_weight=0.1,\n",
    "    E_norm_cpu=None,    # (V, E) on CPU, fp32\n",
    "    chunk_size=1024,\n",
    "    top_k=5,\n",
    "    neg_weight=1.0,     # how strongly to push away from non-top-k\n",
    "):\n",
    "    \"\"\"\n",
    "    - For each example, build [prompt][suffix_z] in embedding space.\n",
    "    - Pad all to same length -> [prompt][suffix][PAD].\n",
    "    - Roll out n_tokens-1 tokens under inference_mode.\n",
    "    - Final forward WITH grad gives entropy loss on last generated token.\n",
    "    - Gradients flow into suffix_z only (prompts are detached).\n",
    "    - PLUS: regularizer that pulls suffix_z toward real token embeddings via cosine similarity.\n",
    "    \"\"\"\n",
    "    prompts = batch[\"input_ids\"]   # list of 1D LongTensors (Li,)\n",
    "    dev = emb_layer.weight.device\n",
    "    suffix_z = suffix_z.to(dev)    # (Ls, E)\n",
    "\n",
    "    B = len(prompts)\n",
    "    Ls, E = suffix_z.shape\n",
    "\n",
    "    base_embs = []   # each: (Li+Ls, E)\n",
    "    base_lens = []   # each: scalar length Li+Ls\n",
    "\n",
    "    # --- Build per-example [prompt][suffix] in embedding space ---\n",
    "    for p_ids in prompts:\n",
    "        p_ids_dev = p_ids.to(dev)\n",
    "        p_emb = emb_layer(p_ids_dev).detach()   # (Li, E), prompts are constants\n",
    "        base = torch.cat([p_emb, suffix_z], dim=0)  # (Li+Ls, E)\n",
    "        base_embs.append(base)\n",
    "        base_lens.append(base.size(0))\n",
    "\n",
    "    # Pad to [prompt][suffix][PAD...] across the batch\n",
    "    base = pad_sequence(base_embs, batch_first=True)   # (B, max_len, E)\n",
    "    base_lens = torch.tensor(base_lens, device=dev)    # (B,)\n",
    "    max_len = base.size(1)\n",
    "\n",
    "    # Attention mask: 1 for real tokens, 0 for pad\n",
    "    arange = torch.arange(max_len, device=dev).unsqueeze(0)  # (1, max_len)\n",
    "    base_mask = (arange < base_lens.unsqueeze(1)).long()     # (B, max_len)\n",
    "\n",
    "    # Now base has structure [prompt][suffix][PAD] per row (masked pads)\n",
    "\n",
    "    def _one_step_logits(e, m):\n",
    "        with amp.autocast(\"cuda\", dtype=amp_dtype):\n",
    "            out = model(\n",
    "                inputs_embeds=e,\n",
    "                attention_mask=m,\n",
    "                use_cache=False,\n",
    "                output_attentions=False,\n",
    "                output_hidden_states=False,\n",
    "                return_dict=True,\n",
    "            )\n",
    "        return out.logits[:, -1, :]  # (B, V)\n",
    "\n",
    "    # ---------- Rollout under no grad (from detached base) ----------\n",
    "    work_e = base.detach()  # rollout uses constants\n",
    "    work_m = base_mask\n",
    "    added_embs = []         # list of (B, E) constants\n",
    "    \n",
    "    T = max(0, n_tokens - 1)\n",
    "    with torch.inference_mode():\n",
    "        for _ in range(T):\n",
    "            logits_t = _one_step_logits(work_e, work_m)\n",
    "            probs_t  = torch.softmax(logits_t, dim=-1)\n",
    "            next_ids = torch.argmax(probs_t, dim=-1)        # (B,)\n",
    "    \n",
    "            next_emb = emb_layer(next_ids.to(dev)).detach() # (B, E)\n",
    "            added_embs.append(next_emb)\n",
    "    \n",
    "            work_e = torch.cat([work_e, next_emb.unsqueeze(1)], dim=1)\n",
    "            work_m = torch.cat(\n",
    "                [work_m, torch.ones((B, 1), dtype=work_m.dtype, device=dev)],\n",
    "                dim=1,\n",
    "            )\n",
    "    \n",
    "    # ---------- Final inputs: [prompt][suffix][PAD] + generated tokens ----------\n",
    "    if len(added_embs) > 0:\n",
    "        added = torch.stack(added_embs, dim=1)              # (B, T, E)\n",
    "        final_emb = torch.cat([base, added], dim=1)         # (B, max_len+T, E)\n",
    "        gen_mask = torch.ones((B, T), dtype=base_mask.dtype, device=dev)\n",
    "        final_mask = torch.cat([base_mask, gen_mask], dim=1)\n",
    "    else:\n",
    "        final_emb = base\n",
    "        final_mask = base_mask\n",
    "    \n",
    "    # ---------- Forward WITH grad for ALL n_tokens steps ----------\n",
    "    with amp.autocast(\"cuda\", dtype=amp_dtype):\n",
    "        out = model(\n",
    "            inputs_embeds=final_emb,\n",
    "            attention_mask=final_mask,\n",
    "            use_cache=False,\n",
    "            output_attentions=False,\n",
    "            output_hidden_states=False,\n",
    "            return_dict=True,\n",
    "        )\n",
    "    \n",
    "    logits_all = out.logits   # (B, L_total, V)\n",
    "    B, L_total, V = logits_all.shape\n",
    "    \n",
    "    # base_lens: (B,) lengths of [prompt][suffix] BEFORE generated tokens\n",
    "    # we want logits for:\n",
    "    #   step 1: position base_len - 1  (first next token)\n",
    "    #   step 2: position base_len      (second next)\n",
    "    #   ...\n",
    "    #   step n_tokens: position base_len - 1 + (n_tokens - 1) = base_len + T - 1\n",
    "    # So indices: [base_len - 1, base_len, ..., base_len + T - 1], length = n_tokens\n",
    "    \n",
    "    all_step_logits = []\n",
    "    \n",
    "    for b in range(B):\n",
    "        blen = base_lens[b].item()  # length of base for this example\n",
    "    \n",
    "        # safety: don't go past sequence length\n",
    "        # we know we have exactly T generated tokens, so there are n_tokens positions:\n",
    "        # indices from blen-1 to blen-1+T (inclusive)\n",
    "        start_idx = blen - 1\n",
    "        end_idx   = blen - 1 + T    # inclusive\n",
    "        # this yields exactly n_tokens positions when T = n_tokens-1\n",
    "    \n",
    "        idxs = torch.arange(start_idx, end_idx + 1, device=dev)  # (n_tokens,)\n",
    "        # gather logits for this example's steps: (n_tokens, V)\n",
    "        step_logits_b = logits_all[b, idxs, :]                   # (n_tokens, V)\n",
    "        all_step_logits.append(step_logits_b)\n",
    "    \n",
    "    # stack over batch: (B, n_tokens, V) -> (B*n_tokens, V)\n",
    "    logits_for_loss = torch.cat(all_step_logits, dim=0)  # (B*n_tokens, V)\n",
    "\n",
    "    # print(f\"hey: {logits_for_loss.shape}\")\n",
    "    \n",
    "    # mean entropy over all n_tokens steps for all examples\n",
    "    ent = entropy_loss(logits_for_loss)\n",
    "\n",
    "    \n",
    "    dev = suffix_z.device\n",
    "    Ls = suffix_z.size(0)\n",
    "    V = E_norm_cpu.size(0)\n",
    "    \n",
    "    # normalized suffix embeddings on GPU, fp32, with grad\n",
    "    z_norm = F.normalize(suffix_z.float(), dim=-1)  # (Ls, E)\n",
    "    \n",
    "    # running top-2 cosines across vocab, per suffix position\n",
    "    top2_vals = None  # (2, Ls)\n",
    "    \n",
    "    for start in range(0, V, chunk_size):\n",
    "        end = min(start + chunk_size, V)\n",
    "        # chunk: (c, E) fp32 on GPU, no grad\n",
    "        chunk = E_norm_cpu[start:end].to(dev, non_blocking=True)  # (c, E)\n",
    "    \n",
    "        # (c, E) @ (E, Ls) -> (c, Ls)\n",
    "        chunk_sim = torch.matmul(chunk, z_norm.T)  # (c, Ls)\n",
    "    \n",
    "        # top-2 within this chunk: (2, Ls)  (if c < 2, topk handles it)\n",
    "        chunk_top2, _ = chunk_sim.topk(\n",
    "            k=min(2, chunk_sim.size(0)), dim=0\n",
    "        )  # (k', Ls)\n",
    "    \n",
    "        if top2_vals is None:\n",
    "            # if first chunk smaller than 2, pad with very low values\n",
    "            if chunk_top2.size(0) < 2:\n",
    "                pad_rows = 2 - chunk_top2.size(0)\n",
    "                pad = torch.full(\n",
    "                    (pad_rows, Ls),\n",
    "                    -1e9,\n",
    "                    device=dev,\n",
    "                    dtype=chunk_top2.dtype,\n",
    "                )\n",
    "                top2_vals = torch.cat([chunk_top2, pad], dim=0)  # (2, Ls)\n",
    "            else:\n",
    "                top2_vals = chunk_top2  # (2, Ls)\n",
    "        else:\n",
    "            # combine global and chunk top2, then keep best 2 across all\n",
    "            combined = torch.cat([top2_vals, chunk_top2], dim=0)  # (2 + k', Ls)\n",
    "            top2_vals, _ = combined.topk(k=2, dim=0)              # (2, Ls)\n",
    "    \n",
    "        # free small temps\n",
    "        del chunk, chunk_sim, chunk_top2\n",
    "    \n",
    "    # Now top2_vals[0] = global best cosine; top2_vals[1] = global second-best\n",
    "    top1 = top2_vals[0]   # (Ls,)\n",
    "    top2 = top2_vals[1]   # (Ls,)\n",
    "    \n",
    "    # margin per position: encourage top1 >> top2\n",
    "    margin_per_pos = top1 - top2  # (Ls,)\n",
    "    mean_margin = margin_per_pos.mean()            # scalar\n",
    "    \n",
    "    # We want to MAXIMIZE mean_margin → in minimization, use negative margin\n",
    "    margin_reg = -mean_margin\n",
    "    \n",
    "    cos_reg = margin_reg  # you can scale it directly with cos_reg_weight below\n",
    "    \n",
    "    total_loss = ent + cos_reg_weight * cos_reg\n",
    "    \n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1339af0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e586bef800646999b0c611e85f7772d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d00a3ed7b82f4074be71c6dcceff8e0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00001-a09b74b3ef9c3b(…):   0%|          | 0.00/24.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7550d4a246a466d83b0235069d82241",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/52002 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from datasets import load_dataset\n",
    "\n",
    "def load_prompts_unpadded(tokenizer, args):\n",
    "    \"\"\"\n",
    "    Returns DataLoader where each batch is:\n",
    "      {\n",
    "        \"input_ids\": list of 1D LongTensors (prompts, no padding),\n",
    "        \"prompt_lens\": LongTensor (B,)\n",
    "      }\n",
    "    \"\"\"\n",
    "    ds = load_dataset(\"tatsu-lab/alpaca\", split=\"train\", cache_dir=args[\"data_dir\"])\n",
    "\n",
    "    # Subsample for speed\n",
    "    if \"sample_size\" in args and args[\"sample_size\"] is not None and args[\"sample_size\"] < len(ds):\n",
    "        ds = ds.shuffle(seed=42).select(range(args[\"sample_size\"]))\n",
    "\n",
    "    def collate(batch):\n",
    "        texts = [ex[\"instruction\"] for ex in batch]\n",
    "        enc = tokenizer(\n",
    "            texts,\n",
    "            padding=False,\n",
    "            truncation=True,\n",
    "            max_length=args[\"max_length\"],\n",
    "        )\n",
    "        prompts = [torch.tensor(ids, dtype=torch.long) for ids in enc[\"input_ids\"]]\n",
    "        prompt_lens = [len(p) for p in prompts]\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": prompts,  # list of (Li,)\n",
    "            \"prompt_lens\": torch.tensor(prompt_lens, dtype=torch.long),\n",
    "        }\n",
    "\n",
    "    num_workers = max(2, os.cpu_count() // 2)\n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size=args[\"batch_size\"],\n",
    "        shuffle=True,\n",
    "        pin_memory=True,\n",
    "        num_workers=num_workers,\n",
    "        persistent_workers=True,\n",
    "        collate_fn=collate,\n",
    "    )\n",
    "\n",
    "dataloader_args = {\n",
    "    \"data_dir\": \"/kaggle/working/datasets\",\n",
    "    \"batch_size\": 1,\n",
    "    \"max_length\": 128,\n",
    "    \"sample_size\": 128,\n",
    "}\n",
    "\n",
    "dataloader = load_prompts_unpadded(tokenizer, dataloader_args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53d6012",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_suffix(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt_ids: torch.Tensor,        # (L_prompt,)\n",
    "    suffix_token_ids: torch.Tensor,  # (Ls,)\n",
    "    max_new_tokens: int = 50,\n",
    "):\n",
    "    \"\"\"\n",
    "    Concatenate [prompt][suffix] and let the model generate continuation.\n",
    "    \"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    full_input_ids = torch.cat(\n",
    "        [prompt_ids, suffix_token_ids.to(prompt_ids.device)],\n",
    "        dim=0\n",
    "    ).unsqueeze(0).to(device)  # (1, L_total)\n",
    "\n",
    "    attn_mask = torch.ones_like(full_input_ids, dtype=torch.long, device=device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        generated = model.generate(\n",
    "            input_ids=full_input_ids,\n",
    "            attention_mask=attn_mask,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    return tokenizer.decode(generated[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c65bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def log_suffix_token_probs(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt_ids: torch.Tensor,        # (L_prompt,)\n",
    "    suffix_token_ids: torch.Tensor,  # (Ls,)\n",
    "):\n",
    "    \"\"\"\n",
    "    For each position k in the suffix, compute P(suffix[k] | prompt + suffix[:k])\n",
    "    and print it.\n",
    "    \"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    model.eval()\n",
    "\n",
    "    prompt_ids = prompt_ids.to(device)\n",
    "    suffix_token_ids = suffix_token_ids.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for k in range(suffix_token_ids.size(0)):\n",
    "            ctx_suffix = suffix_token_ids[:k]      # (k,)\n",
    "            ctx = torch.cat([prompt_ids, ctx_suffix], dim=0)  # (L_prompt + k,)\n",
    "\n",
    "            inp = ctx.unsqueeze(0)  # (1, L_ctx)\n",
    "            msk = torch.ones_like(inp, dtype=torch.long, device=device)\n",
    "\n",
    "            out = model(\n",
    "                input_ids=inp,\n",
    "                attention_mask=msk,\n",
    "                use_cache=False,\n",
    "                return_dict=True,\n",
    "            )\n",
    "            logits_next = out.logits[:, -1, :]  # (1, V)\n",
    "            probs_next = F.softmax(logits_next, dim=-1)  # (1, V)\n",
    "\n",
    "            tok_id = suffix_token_ids[k]\n",
    "            prob = probs_next[0, tok_id].item()\n",
    "            tok_str = tokenizer.decode([tok_id])\n",
    "\n",
    "            print(f\"  pos {k:2d}, token {tok_id:5d} ({repr(tok_str)}), prob: {prob:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc50bc60",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T14:16:59.120215Z",
     "iopub.status.busy": "2025-11-20T14:16:59.119578Z",
     "iopub.status.idle": "2025-11-20T14:17:06.411777Z",
     "shell.execute_reply": "2025-11-20T14:17:06.410871Z",
     "shell.execute_reply.started": "2025-11-20T14:16:59.120186Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using this prompt for qualitative checks:\n",
      "   List three sites of historical importance in London.\n",
      "\n",
      "\n",
      "=== round 0, epoch 0 ===\n",
      "Read /kaggle/working/hotflip/rounds/suffix_r0_e0.pt successfully. shape=(10, 4096)\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  3.787844\n",
      "  max:  4.888620\n",
      "  mean: 4.140403\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.130694\n",
      "  max:  0.297501\n",
      "  mean: 0.202913\n",
      "\n",
      "Projected discrete suffix token IDs: [20609, 1642, 7713, 10056, 10805, 29169, 3798, 1858, 7236, 25775]\n",
      "Projected discrete suffix tokens: ['▁Portály', '\".', 'Pol', '▁Consider', '▁causing', '²).', 'oreferrer', '▁Pl', '▁Fort', '▁Ari']\n",
      "Projected suffix as text: 'Portály\".Pol Consider causing²).oreferrer Pl Fort Ari'\n",
      "suffix loss (before projection): 4.947422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suffix loss (after  projection): 5.449004\n",
      "\n",
      "Generated text with suffix (projected):\n",
      "List three sites of historical importance in London. Portály\".Pol Consider causing²).oreferrer Pl Fort Ari. 1. The Tower of London 2. The Houses of Parliament 3. Buckingham Palace The Tower of London is a castle built by William the Conqueror in 1066. It was built to protect London from invaders. It was also used to keep prisoners. The Houses\n",
      "\n",
      "Per-suffix-token next-token probabilities:\n",
      "  pos  0, token 20609 ('Portály'), prob: 0.000000\n",
      "  pos  1, token  1642 ('\".'), prob: 0.000004\n",
      "  pos  2, token  7713 ('Pol'), prob: 0.000015\n",
      "  pos  3, token 10056 ('Consider'), prob: 0.000000\n",
      "  pos  4, token 10805 ('causing'), prob: 0.000001\n",
      "  pos  5, token 29169 ('²).'), prob: 0.000000\n",
      "  pos  6, token  3798 ('oreferrer'), prob: 0.000000\n",
      "  pos  7, token  1858 ('Pl'), prob: 0.000015\n",
      "  pos  8, token  7236 ('Fort'), prob: 0.000002\n",
      "  pos  9, token 25775 ('Ari'), prob: 0.000004\n",
      "####################\n",
      "\n",
      "=== round 0, epoch 1 ===\n",
      "Read /kaggle/working/hotflip/rounds/suffix_r0_e1.pt successfully. shape=(10, 4096)\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  3.628708\n",
      "  max:  4.806677\n",
      "  mean: 4.058847\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.205725\n",
      "  max:  0.452003\n",
      "  mean: 0.311015\n",
      "\n",
      "Projected discrete suffix token IDs: [20609, 1642, 9802, 10056, 10805, 29169, 3798, 19095, 7236, 25775]\n",
      "Projected discrete suffix tokens: ['▁Portály', '\".', 'Mark', '▁Consider', '▁causing', '²).', 'oreferrer', 'Mag', '▁Fort', '▁Ari']\n",
      "Projected suffix as text: 'Portály\".Mark Consider causing²).oreferrerMag Fort Ari'\n",
      "suffix loss (before projection): 5.140125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suffix loss (after  projection): 5.504870\n",
      "\n",
      "Generated text with suffix (projected):\n",
      "List three sites of historical importance in London. Portály\".Mark Consider causing²).oreferrerMag Fort Ariane. 1. The Tower of London. 2. The Houses of Parliament. 3. The British Museum. 4. The British Library. 5. The National Gallery. 6. The National Portrait Gallery. 7. The Royal Opera House. 8. The Royal Academy of\n",
      "\n",
      "Per-suffix-token next-token probabilities:\n",
      "  pos  0, token 20609 ('Portály'), prob: 0.000000\n",
      "  pos  1, token  1642 ('\".'), prob: 0.000004\n",
      "  pos  2, token  9802 ('Mark'), prob: 0.000019\n",
      "  pos  3, token 10056 ('Consider'), prob: 0.000001\n",
      "  pos  4, token 10805 ('causing'), prob: 0.000006\n",
      "  pos  5, token 29169 ('²).'), prob: 0.000000\n",
      "  pos  6, token  3798 ('oreferrer'), prob: 0.000000\n",
      "  pos  7, token 19095 ('Mag'), prob: 0.000001\n",
      "  pos  8, token  7236 ('Fort'), prob: 0.000017\n",
      "  pos  9, token 25775 ('Ari'), prob: 0.000005\n",
      "####################\n",
      "\n",
      "=== round 0, epoch 2 ===\n",
      "Read /kaggle/working/hotflip/rounds/suffix_r0_e2.pt successfully. shape=(10, 4096)\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  3.702060\n",
      "  max:  5.276039\n",
      "  mean: 4.269306\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.260999\n",
      "  max:  0.462586\n",
      "  mean: 0.366854\n",
      "\n",
      "Projected discrete suffix token IDs: [20609, 1642, 9802, 10056, 10805, 29169, 28354, 19095, 19618, 25775]\n",
      "Projected discrete suffix tokens: ['▁Portály', '\".', 'Mark', '▁Consider', '▁causing', '²).', '▁Расподела', 'Mag', '▁Beaut', '▁Ari']\n",
      "Projected suffix as text: 'Portály\".Mark Consider causing²). РасподелаMag Beaut Ari'\n",
      "suffix loss (before projection): 4.529020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suffix loss (after  projection): 5.193542\n",
      "\n",
      "Generated text with suffix (projected):\n",
      "List three sites of historical importance in London. Portály\".Mark Consider causing²). РасподелаMag Beaut Ari.\n",
      "\n",
      "The first two are obvious, but the third is a bit more obscure. It's the site of the Battle of Crécy, where the English army defeated the French in 1346. The battle was a turning point in the Hundred Years War, and the English victory was\n",
      "\n",
      "Per-suffix-token next-token probabilities:\n",
      "  pos  0, token 20609 ('Portály'), prob: 0.000000\n",
      "  pos  1, token  1642 ('\".'), prob: 0.000004\n",
      "  pos  2, token  9802 ('Mark'), prob: 0.000019\n",
      "  pos  3, token 10056 ('Consider'), prob: 0.000001\n",
      "  pos  4, token 10805 ('causing'), prob: 0.000006\n",
      "  pos  5, token 29169 ('²).'), prob: 0.000000\n",
      "  pos  6, token 28354 ('Расподела'), prob: 0.000000\n",
      "  pos  7, token 19095 ('Mag'), prob: 0.000001\n",
      "  pos  8, token 19618 ('Beaut'), prob: 0.000001\n",
      "  pos  9, token 25775 ('Ari'), prob: 0.000000\n",
      "####################\n",
      "\n",
      "=== round 0, epoch 3 ===\n",
      "Read /kaggle/working/hotflip/rounds/suffix_r0_e3.pt successfully. shape=(10, 4096)\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  3.670125\n",
      "  max:  5.200637\n",
      "  mean: 4.202552\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.317285\n",
      "  max:  0.506162\n",
      "  mean: 0.422951\n",
      "\n",
      "Projected discrete suffix token IDs: [20609, 1642, 9802, 10056, 10805, 29169, 28354, 19095, 19618, 25775]\n",
      "Projected discrete suffix tokens: ['▁Portály', '\".', 'Mark', '▁Consider', '▁causing', '²).', '▁Расподела', 'Mag', '▁Beaut', '▁Ari']\n",
      "Projected suffix as text: 'Portály\".Mark Consider causing²). РасподелаMag Beaut Ari'\n",
      "suffix loss (before projection): 4.584489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suffix loss (after  projection): 5.193542\n",
      "\n",
      "Generated text with suffix (projected):\n",
      "List three sites of historical importance in London. Portály\".Mark Consider causing²). РасподелаMag Beaut Ari.\n",
      "\n",
      "The first two are obvious, but the third is a bit more obscure. It's the site of the Battle of Crécy, where the English army defeated the French in 1346. The battle was a turning point in the Hundred Years War, and the English victory was\n",
      "\n",
      "Per-suffix-token next-token probabilities:\n",
      "  pos  0, token 20609 ('Portály'), prob: 0.000000\n",
      "  pos  1, token  1642 ('\".'), prob: 0.000004\n",
      "  pos  2, token  9802 ('Mark'), prob: 0.000019\n",
      "  pos  3, token 10056 ('Consider'), prob: 0.000001\n",
      "  pos  4, token 10805 ('causing'), prob: 0.000006\n",
      "  pos  5, token 29169 ('²).'), prob: 0.000000\n",
      "  pos  6, token 28354 ('Расподела'), prob: 0.000000\n",
      "  pos  7, token 19095 ('Mag'), prob: 0.000001\n",
      "  pos  8, token 19618 ('Beaut'), prob: 0.000001\n",
      "  pos  9, token 25775 ('Ari'), prob: 0.000000\n",
      "####################\n",
      "\n",
      "=== round 0, epoch 4 ===\n",
      "Read /kaggle/working/hotflip/rounds/suffix_r0_e4.pt successfully. shape=(10, 4096)\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  3.632905\n",
      "  max:  5.146614\n",
      "  mean: 4.116168\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.358892\n",
      "  max:  0.549129\n",
      "  mean: 0.456216\n",
      "\n",
      "Projected discrete suffix token IDs: [23923, 1642, 9802, 10056, 10805, 29169, 28354, 19095, 19618, 25775]\n",
      "Projected discrete suffix tokens: ['Fre', '\".', 'Mark', '▁Consider', '▁causing', '²).', '▁Расподела', 'Mag', '▁Beaut', '▁Ari']\n",
      "Projected suffix as text: 'Fre\".Mark Consider causing²). РасподелаMag Beaut Ari'\n",
      "suffix loss (before projection): 6.050853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suffix loss (after  projection): 4.968382\n",
      "\n",
      "Generated text with suffix (projected):\n",
      "List three sites of historical importance in London.Fre\".Mark Consider causing²). РасподелаMag Beaut Ari.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Per-suffix-token next-token probabilities:\n",
      "  pos  0, token 23923 ('Fre'), prob: 0.000000\n",
      "  pos  1, token  1642 ('\".'), prob: 0.000000\n",
      "  pos  2, token  9802 ('Mark'), prob: 0.000050\n",
      "  pos  3, token 10056 ('Consider'), prob: 0.000001\n",
      "  pos  4, token 10805 ('causing'), prob: 0.000003\n",
      "  pos  5, token 29169 ('²).'), prob: 0.000000\n",
      "  pos  6, token 28354 ('Расподела'), prob: 0.000000\n",
      "  pos  7, token 19095 ('Mag'), prob: 0.000001\n",
      "  pos  8, token 19618 ('Beaut'), prob: 0.000001\n",
      "  pos  9, token 25775 ('Ari'), prob: 0.000000\n",
      "####################\n",
      "\n",
      "=== round 0, epoch 5 ===\n",
      "Read /kaggle/working/hotflip/rounds/suffix_r0_e5.pt successfully. shape=(10, 4096)\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  3.588412\n",
      "  max:  5.255209\n",
      "  mean: 4.111514\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.380682\n",
      "  max:  0.577760\n",
      "  mean: 0.470007\n",
      "\n",
      "Projected discrete suffix token IDs: [23923, 1642, 9802, 10056, 10805, 29169, 28354, 19095, 19618, 25775]\n",
      "Projected discrete suffix tokens: ['Fre', '\".', 'Mark', '▁Consider', '▁causing', '²).', '▁Расподела', 'Mag', '▁Beaut', '▁Ari']\n",
      "Projected suffix as text: 'Fre\".Mark Consider causing²). РасподелаMag Beaut Ari'\n",
      "suffix loss (before projection): 4.413508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suffix loss (after  projection): 4.968382\n",
      "\n",
      "Generated text with suffix (projected):\n",
      "List three sites of historical importance in London.Fre\".Mark Consider causing²). РасподелаMag Beaut Ari.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Per-suffix-token next-token probabilities:\n",
      "  pos  0, token 23923 ('Fre'), prob: 0.000000\n",
      "  pos  1, token  1642 ('\".'), prob: 0.000000\n",
      "  pos  2, token  9802 ('Mark'), prob: 0.000050\n",
      "  pos  3, token 10056 ('Consider'), prob: 0.000001\n",
      "  pos  4, token 10805 ('causing'), prob: 0.000003\n",
      "  pos  5, token 29169 ('²).'), prob: 0.000000\n",
      "  pos  6, token 28354 ('Расподела'), prob: 0.000000\n",
      "  pos  7, token 19095 ('Mag'), prob: 0.000001\n",
      "  pos  8, token 19618 ('Beaut'), prob: 0.000001\n",
      "  pos  9, token 25775 ('Ari'), prob: 0.000000\n",
      "####################\n",
      "\n",
      "=== round 0, epoch 6 ===\n",
      "Read /kaggle/working/hotflip/rounds/suffix_r0_e6.pt successfully. shape=(10, 4096)\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  3.550333\n",
      "  max:  5.217088\n",
      "  mean: 4.070822\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.407447\n",
      "  max:  0.605035\n",
      "  mean: 0.488443\n",
      "\n",
      "Projected discrete suffix token IDs: [23923, 1642, 9802, 10056, 10805, 29169, 28354, 19095, 19618, 25775]\n",
      "Projected discrete suffix tokens: ['Fre', '\".', 'Mark', '▁Consider', '▁causing', '²).', '▁Расподела', 'Mag', '▁Beaut', '▁Ari']\n",
      "Projected suffix as text: 'Fre\".Mark Consider causing²). РасподелаMag Beaut Ari'\n",
      "suffix loss (before projection): 4.231536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suffix loss (after  projection): 4.968382\n",
      "\n",
      "Generated text with suffix (projected):\n",
      "List three sites of historical importance in London.Fre\".Mark Consider causing²). РасподелаMag Beaut Ari.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Per-suffix-token next-token probabilities:\n",
      "  pos  0, token 23923 ('Fre'), prob: 0.000000\n",
      "  pos  1, token  1642 ('\".'), prob: 0.000000\n",
      "  pos  2, token  9802 ('Mark'), prob: 0.000050\n",
      "  pos  3, token 10056 ('Consider'), prob: 0.000001\n",
      "  pos  4, token 10805 ('causing'), prob: 0.000003\n",
      "  pos  5, token 29169 ('²).'), prob: 0.000000\n",
      "  pos  6, token 28354 ('Расподела'), prob: 0.000000\n",
      "  pos  7, token 19095 ('Mag'), prob: 0.000001\n",
      "  pos  8, token 19618 ('Beaut'), prob: 0.000001\n",
      "  pos  9, token 25775 ('Ari'), prob: 0.000000\n",
      "####################\n",
      "\n",
      "=== round 0, epoch 7 ===\n",
      "Read /kaggle/working/hotflip/rounds/suffix_r0_e7.pt successfully. shape=(10, 4096)\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  3.513973\n",
      "  max:  5.198439\n",
      "  mean: 4.037545\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.424276\n",
      "  max:  0.623114\n",
      "  mean: 0.498608\n",
      "\n",
      "Projected discrete suffix token IDs: [23923, 1642, 9802, 10056, 10805, 29169, 28354, 19095, 19618, 25775]\n",
      "Projected discrete suffix tokens: ['Fre', '\".', 'Mark', '▁Consider', '▁causing', '²).', '▁Расподела', 'Mag', '▁Beaut', '▁Ari']\n",
      "Projected suffix as text: 'Fre\".Mark Consider causing²). РасподелаMag Beaut Ari'\n",
      "suffix loss (before projection): 6.083911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suffix loss (after  projection): 4.968382\n",
      "\n",
      "Generated text with suffix (projected):\n",
      "List three sites of historical importance in London.Fre\".Mark Consider causing²). РасподелаMag Beaut Ari.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Per-suffix-token next-token probabilities:\n",
      "  pos  0, token 23923 ('Fre'), prob: 0.000000\n",
      "  pos  1, token  1642 ('\".'), prob: 0.000000\n",
      "  pos  2, token  9802 ('Mark'), prob: 0.000050\n",
      "  pos  3, token 10056 ('Consider'), prob: 0.000001\n",
      "  pos  4, token 10805 ('causing'), prob: 0.000003\n",
      "  pos  5, token 29169 ('²).'), prob: 0.000000\n",
      "  pos  6, token 28354 ('Расподела'), prob: 0.000000\n",
      "  pos  7, token 19095 ('Mag'), prob: 0.000001\n",
      "  pos  8, token 19618 ('Beaut'), prob: 0.000001\n",
      "  pos  9, token 25775 ('Ari'), prob: 0.000000\n",
      "####################\n",
      "\n",
      "=== round 0, epoch 8 ===\n",
      "Read /kaggle/working/hotflip/rounds/suffix_r0_e8.pt successfully. shape=(10, 4096)\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  3.484671\n",
      "  max:  5.193452\n",
      "  mean: 4.015231\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.435689\n",
      "  max:  0.635103\n",
      "  mean: 0.505144\n",
      "\n",
      "Projected discrete suffix token IDs: [23923, 1642, 9802, 10056, 10805, 29169, 28354, 19095, 19618, 25775]\n",
      "Projected discrete suffix tokens: ['Fre', '\".', 'Mark', '▁Consider', '▁causing', '²).', '▁Расподела', 'Mag', '▁Beaut', '▁Ari']\n",
      "Projected suffix as text: 'Fre\".Mark Consider causing²). РасподелаMag Beaut Ari'\n",
      "suffix loss (before projection): 6.093708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suffix loss (after  projection): 4.968382\n",
      "\n",
      "Generated text with suffix (projected):\n",
      "List three sites of historical importance in London.Fre\".Mark Consider causing²). РасподелаMag Beaut Ari.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Per-suffix-token next-token probabilities:\n",
      "  pos  0, token 23923 ('Fre'), prob: 0.000000\n",
      "  pos  1, token  1642 ('\".'), prob: 0.000000\n",
      "  pos  2, token  9802 ('Mark'), prob: 0.000050\n",
      "  pos  3, token 10056 ('Consider'), prob: 0.000001\n",
      "  pos  4, token 10805 ('causing'), prob: 0.000003\n",
      "  pos  5, token 29169 ('²).'), prob: 0.000000\n",
      "  pos  6, token 28354 ('Расподела'), prob: 0.000000\n",
      "  pos  7, token 19095 ('Mag'), prob: 0.000001\n",
      "  pos  8, token 19618 ('Beaut'), prob: 0.000001\n",
      "  pos  9, token 25775 ('Ari'), prob: 0.000000\n",
      "####################\n",
      "\n",
      "=== round 0, epoch 9 ===\n",
      "Read /kaggle/working/hotflip/rounds/suffix_r0_e9.pt successfully. shape=(10, 4096)\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  3.456241\n",
      "  max:  5.210452\n",
      "  mean: 4.002599\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.440039\n",
      "  max:  0.644118\n",
      "  mean: 0.509148\n",
      "\n",
      "Projected discrete suffix token IDs: [23923, 1642, 9802, 10056, 10805, 29169, 28354, 19095, 19618, 25775]\n",
      "Projected discrete suffix tokens: ['Fre', '\".', 'Mark', '▁Consider', '▁causing', '²).', '▁Расподела', 'Mag', '▁Beaut', '▁Ari']\n",
      "Projected suffix as text: 'Fre\".Mark Consider causing²). РасподелаMag Beaut Ari'\n",
      "suffix loss (before projection): 4.068754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suffix loss (after  projection): 4.968382\n",
      "\n",
      "Generated text with suffix (projected):\n",
      "List three sites of historical importance in London.Fre\".Mark Consider causing²). РасподелаMag Beaut Ari.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Per-suffix-token next-token probabilities:\n",
      "  pos  0, token 23923 ('Fre'), prob: 0.000000\n",
      "  pos  1, token  1642 ('\".'), prob: 0.000000\n",
      "  pos  2, token  9802 ('Mark'), prob: 0.000050\n",
      "  pos  3, token 10056 ('Consider'), prob: 0.000001\n",
      "  pos  4, token 10805 ('causing'), prob: 0.000003\n",
      "  pos  5, token 29169 ('²).'), prob: 0.000000\n",
      "  pos  6, token 28354 ('Расподела'), prob: 0.000000\n",
      "  pos  7, token 19095 ('Mag'), prob: 0.000001\n",
      "  pos  8, token 19618 ('Beaut'), prob: 0.000001\n",
      "  pos  9, token 25775 ('Ari'), prob: 0.000000\n",
      "####################\n",
      "\n",
      "=== round 1, epoch 0 ===\n",
      "Read /kaggle/working/hotflip/rounds/suffix_r1_e0.pt successfully. shape=(10, 4096)\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  3.862738\n",
      "  max:  5.130178\n",
      "  mean: 4.484840\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.215106\n",
      "  max:  0.488057\n",
      "  mean: 0.346730\n",
      "\n",
      "Projected discrete suffix token IDs: [2803, 20609, 30171, 8752, 1660, 315, 20609, 317, 341, 22735]\n",
      "Projected discrete suffix tokens: ['▁Let', '▁Portály', 'و', 'Fragment', 'SE', '▁C', '▁Portály', '▁S', '▁M', 'Пе']\n",
      "Projected suffix as text: 'Let PortályوFragmentSE C Portály S MПе'\n",
      "suffix loss (before projection): 4.220569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suffix loss (after  projection): 4.393586\n",
      "\n",
      "Generated text with suffix (projected):\n",
      "List three sites of historical importance in London. Let PortályوFragmentSE C Portály S MПеоооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооо\n",
      "\n",
      "Per-suffix-token next-token probabilities:\n",
      "  pos  0, token  2803 ('Let'), prob: 0.005802\n",
      "  pos  1, token 20609 ('Portály'), prob: 0.000000\n",
      "  pos  2, token 30171 ('و'), prob: 0.000000\n",
      "  pos  3, token  8752 ('Fragment'), prob: 0.000003\n",
      "  pos  4, token  1660 ('SE'), prob: 0.000009\n",
      "  pos  5, token   315 ('C'), prob: 0.000444\n",
      "  pos  6, token 20609 ('Portály'), prob: 0.000000\n",
      "  pos  7, token   317 ('S'), prob: 0.000891\n",
      "  pos  8, token   341 ('M'), prob: 0.003000\n",
      "  pos  9, token 22735 ('Пе'), prob: 0.000001\n",
      "####################\n",
      "\n",
      "=== round 1, epoch 1 ===\n",
      "Read /kaggle/working/hotflip/rounds/suffix_r1_e1.pt successfully. shape=(10, 4096)\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  3.743016\n",
      "  max:  5.000424\n",
      "  mean: 4.371238\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.348451\n",
      "  max:  0.658052\n",
      "  mean: 0.498334\n",
      "\n",
      "Projected discrete suffix token IDs: [2803, 28354, 30171, 8752, 1660, 315, 20609, 317, 341, 22735]\n",
      "Projected discrete suffix tokens: ['▁Let', '▁Расподела', 'و', 'Fragment', 'SE', '▁C', '▁Portály', '▁S', '▁M', 'Пе']\n",
      "Projected suffix as text: 'Let РасподелаوFragmentSE C Portály S MПе'\n",
      "suffix loss (before projection): 5.453373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suffix loss (after  projection): 3.117534\n",
      "\n",
      "Generated text with suffix (projected):\n",
      "List three sites of historical importance in London. Let РасподелаوFragmentSE C Portály S MПеоооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооо\n",
      "\n",
      "Per-suffix-token next-token probabilities:\n",
      "  pos  0, token  2803 ('Let'), prob: 0.005802\n",
      "  pos  1, token 28354 ('Расподела'), prob: 0.000000\n",
      "  pos  2, token 30171 ('و'), prob: 0.000000\n",
      "  pos  3, token  8752 ('Fragment'), prob: 0.000002\n",
      "  pos  4, token  1660 ('SE'), prob: 0.000006\n",
      "  pos  5, token   315 ('C'), prob: 0.000448\n",
      "  pos  6, token 20609 ('Portály'), prob: 0.000000\n",
      "  pos  7, token   317 ('S'), prob: 0.001122\n",
      "  pos  8, token   341 ('M'), prob: 0.003204\n",
      "  pos  9, token 22735 ('Пе'), prob: 0.000000\n",
      "####################\n",
      "\n",
      "=== round 1, epoch 2 ===\n",
      "Read /kaggle/working/hotflip/rounds/suffix_r1_e2.pt successfully. shape=(10, 4096)\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  3.662354\n",
      "  max:  5.325129\n",
      "  mean: 4.388601\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.407258\n",
      "  max:  0.695439\n",
      "  mean: 0.549953\n",
      "\n",
      "Projected discrete suffix token IDs: [2803, 28354, 30171, 8752, 1660, 315, 20609, 341, 341, 31644]\n",
      "Projected discrete suffix tokens: ['▁Let', '▁Расподела', 'و', 'Fragment', 'SE', '▁C', '▁Portály', '▁M', '▁M', 'ེ']\n",
      "Projected suffix as text: 'Let РасподелаوFragmentSE C Portály M Mེ'\n",
      "suffix loss (before projection): 5.133557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suffix loss (after  projection): 5.218432\n",
      "\n",
      "Generated text with suffix (projected):\n",
      "List three sites of historical importance in London. Let РасподелаوFragmentSE C Portály M MེM M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M\n",
      "\n",
      "Per-suffix-token next-token probabilities:\n",
      "  pos  0, token  2803 ('Let'), prob: 0.005802\n",
      "  pos  1, token 28354 ('Расподела'), prob: 0.000000\n",
      "  pos  2, token 30171 ('و'), prob: 0.000000\n",
      "  pos  3, token  8752 ('Fragment'), prob: 0.000002\n",
      "  pos  4, token  1660 ('SE'), prob: 0.000006\n",
      "  pos  5, token   315 ('C'), prob: 0.000448\n",
      "  pos  6, token 20609 ('Portály'), prob: 0.000000\n",
      "  pos  7, token   341 ('M'), prob: 0.001217\n",
      "  pos  8, token   341 ('M'), prob: 0.004322\n",
      "  pos  9, token 31644 ('ེ'), prob: 0.000001\n",
      "####################\n",
      "\n",
      "=== round 1, epoch 3 ===\n",
      "Read /kaggle/working/hotflip/rounds/suffix_r1_e3.pt successfully. shape=(10, 4096)\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  3.615299\n",
      "  max:  5.420966\n",
      "  mean: 4.382689\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.440769\n",
      "  max:  0.695309\n",
      "  mean: 0.568925\n",
      "\n",
      "Projected discrete suffix token IDs: [2803, 28354, 30171, 8752, 1660, 315, 20609, 341, 341, 31644]\n",
      "Projected discrete suffix tokens: ['▁Let', '▁Расподела', 'و', 'Fragment', 'SE', '▁C', '▁Portály', '▁M', '▁M', 'ེ']\n",
      "Projected suffix as text: 'Let РасподелаوFragmentSE C Portály M Mེ'\n",
      "suffix loss (before projection): 4.856975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suffix loss (after  projection): 5.218432\n",
      "\n",
      "Generated text with suffix (projected):\n",
      "List three sites of historical importance in London. Let РасподелаوFragmentSE C Portály M MེM M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M\n",
      "\n",
      "Per-suffix-token next-token probabilities:\n",
      "  pos  0, token  2803 ('Let'), prob: 0.005802\n",
      "  pos  1, token 28354 ('Расподела'), prob: 0.000000\n",
      "  pos  2, token 30171 ('و'), prob: 0.000000\n",
      "  pos  3, token  8752 ('Fragment'), prob: 0.000002\n",
      "  pos  4, token  1660 ('SE'), prob: 0.000006\n",
      "  pos  5, token   315 ('C'), prob: 0.000448\n",
      "  pos  6, token 20609 ('Portály'), prob: 0.000000\n",
      "  pos  7, token   341 ('M'), prob: 0.001217\n",
      "  pos  8, token   341 ('M'), prob: 0.004322\n",
      "  pos  9, token 31644 ('ེ'), prob: 0.000001\n",
      "####################\n",
      "\n",
      "=== round 1, epoch 4 ===\n",
      "Read /kaggle/working/hotflip/rounds/suffix_r1_e4.pt successfully. shape=(10, 4096)\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  3.684350\n",
      "  max:  5.974068\n",
      "  mean: 4.989570\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.261777\n",
      "  max:  0.593803\n",
      "  mean: 0.479870\n",
      "\n",
      "Projected discrete suffix token IDs: [2803, 28354, 30171, 8752, 1660, 315, 28354, 317, 341, 31644]\n",
      "Projected discrete suffix tokens: ['▁Let', '▁Расподела', 'و', 'Fragment', 'SE', '▁C', '▁Расподела', '▁S', '▁M', 'ེ']\n",
      "Projected suffix as text: 'Let РасподелаوFragmentSE C Расподела S Mེ'\n",
      "suffix loss (before projection): 3.630424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suffix loss (after  projection): 6.041427\n",
      "\n",
      "Generated text with suffix (projected):\n",
      "List three sites of historical importance in London. Let РасподелаوFragmentSE C Расподела S MེT.\n",
      "\n",
      "Now, let's say you want to find out what the word \"con\" means. You could look it up in a dictionary, but that's a lot of work. Instead, you could use a search engine. You type in \"con\" and it gives you a list of sites\n",
      "\n",
      "Per-suffix-token next-token probabilities:\n",
      "  pos  0, token  2803 ('Let'), prob: 0.005802\n",
      "  pos  1, token 28354 ('Расподела'), prob: 0.000000\n",
      "  pos  2, token 30171 ('و'), prob: 0.000000\n",
      "  pos  3, token  8752 ('Fragment'), prob: 0.000002\n",
      "  pos  4, token  1660 ('SE'), prob: 0.000006\n",
      "  pos  5, token   315 ('C'), prob: 0.000448\n",
      "  pos  6, token 28354 ('Расподела'), prob: 0.000000\n",
      "  pos  7, token   317 ('S'), prob: 0.001241\n",
      "  pos  8, token   341 ('M'), prob: 0.001789\n",
      "  pos  9, token 31644 ('ེ'), prob: 0.000001\n",
      "####################\n",
      "\n",
      "=== round 1, epoch 5 ===\n",
      "Read /kaggle/working/hotflip/rounds/suffix_r1_e5.pt successfully. shape=(10, 4096)\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  3.669656\n",
      "  max:  5.977582\n",
      "  mean: 4.964544\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.388132\n",
      "  max:  0.651032\n",
      "  mean: 0.545188\n",
      "\n",
      "Projected discrete suffix token IDs: [2803, 28354, 30171, 8752, 1660, 315, 28354, 317, 341, 31644]\n",
      "Projected discrete suffix tokens: ['▁Let', '▁Расподела', 'و', 'Fragment', 'SE', '▁C', '▁Расподела', '▁S', '▁M', 'ེ']\n",
      "Projected suffix as text: 'Let РасподелаوFragmentSE C Расподела S Mེ'\n",
      "suffix loss (before projection): 3.950564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suffix loss (after  projection): 6.041427\n",
      "\n",
      "Generated text with suffix (projected):\n",
      "List three sites of historical importance in London. Let РасподелаوFragmentSE C Расподела S MེT.\n",
      "\n",
      "Now, let's say you want to find out what the word \"con\" means. You could look it up in a dictionary, but that's a lot of work. Instead, you could use a search engine. You type in \"con\" and it gives you a list of sites\n",
      "\n",
      "Per-suffix-token next-token probabilities:\n",
      "  pos  0, token  2803 ('Let'), prob: 0.005802\n",
      "  pos  1, token 28354 ('Расподела'), prob: 0.000000\n",
      "  pos  2, token 30171 ('و'), prob: 0.000000\n",
      "  pos  3, token  8752 ('Fragment'), prob: 0.000002\n",
      "  pos  4, token  1660 ('SE'), prob: 0.000006\n",
      "  pos  5, token   315 ('C'), prob: 0.000448\n",
      "  pos  6, token 28354 ('Расподела'), prob: 0.000000\n",
      "  pos  7, token   317 ('S'), prob: 0.001241\n",
      "  pos  8, token   341 ('M'), prob: 0.001789\n",
      "  pos  9, token 31644 ('ེ'), prob: 0.000001\n",
      "####################\n",
      "\n",
      "=== round 1, epoch 6 ===\n",
      "Read /kaggle/working/hotflip/rounds/suffix_r1_e6.pt successfully. shape=(10, 4096)\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  3.713540\n",
      "  max:  6.255694\n",
      "  mean: 5.089516\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.421173\n",
      "  max:  0.669912\n",
      "  mean: 0.553457\n",
      "\n",
      "Projected discrete suffix token IDs: [2803, 28354, 30171, 8752, 1660, 315, 28354, 341, 341, 31644]\n",
      "Projected discrete suffix tokens: ['▁Let', '▁Расподела', 'و', 'Fragment', 'SE', '▁C', '▁Расподела', '▁M', '▁M', 'ེ']\n",
      "Projected suffix as text: 'Let РасподелаوFragmentSE C Расподела M Mེ'\n",
      "suffix loss (before projection): 3.140436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suffix loss (after  projection): 5.262692\n",
      "\n",
      "Generated text with suffix (projected):\n",
      "List three sites of historical importance in London. Let РасподелаوFragmentSE C Расподела M MེM M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M\n",
      "\n",
      "Per-suffix-token next-token probabilities:\n",
      "  pos  0, token  2803 ('Let'), prob: 0.005802\n",
      "  pos  1, token 28354 ('Расподела'), prob: 0.000000\n",
      "  pos  2, token 30171 ('و'), prob: 0.000000\n",
      "  pos  3, token  8752 ('Fragment'), prob: 0.000002\n",
      "  pos  4, token  1660 ('SE'), prob: 0.000006\n",
      "  pos  5, token   315 ('C'), prob: 0.000448\n",
      "  pos  6, token 28354 ('Расподела'), prob: 0.000000\n",
      "  pos  7, token   341 ('M'), prob: 0.001310\n",
      "  pos  8, token   341 ('M'), prob: 0.004116\n",
      "  pos  9, token 31644 ('ེ'), prob: 0.000000\n",
      "####################\n",
      "\n",
      "=== round 1, epoch 7 ===\n",
      "Read /kaggle/working/hotflip/rounds/suffix_r1_e7.pt successfully. shape=(10, 4096)\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  3.728868\n",
      "  max:  6.201711\n",
      "  mean: 5.081540\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.432318\n",
      "  max:  0.689173\n",
      "  mean: 0.569908\n",
      "\n",
      "Projected discrete suffix token IDs: [2803, 28354, 30171, 8752, 1660, 315, 28354, 341, 341, 31644]\n",
      "Projected discrete suffix tokens: ['▁Let', '▁Расподела', 'و', 'Fragment', 'SE', '▁C', '▁Расподела', '▁M', '▁M', 'ེ']\n",
      "Projected suffix as text: 'Let РасподелаوFragmentSE C Расподела M Mེ'\n",
      "suffix loss (before projection): 3.267055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suffix loss (after  projection): 5.262692\n",
      "\n",
      "Generated text with suffix (projected):\n",
      "List three sites of historical importance in London. Let РасподелаوFragmentSE C Расподела M MེM M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M\n",
      "\n",
      "Per-suffix-token next-token probabilities:\n",
      "  pos  0, token  2803 ('Let'), prob: 0.005802\n",
      "  pos  1, token 28354 ('Расподела'), prob: 0.000000\n",
      "  pos  2, token 30171 ('و'), prob: 0.000000\n",
      "  pos  3, token  8752 ('Fragment'), prob: 0.000002\n",
      "  pos  4, token  1660 ('SE'), prob: 0.000006\n",
      "  pos  5, token   315 ('C'), prob: 0.000448\n",
      "  pos  6, token 28354 ('Расподела'), prob: 0.000000\n",
      "  pos  7, token   341 ('M'), prob: 0.001310\n",
      "  pos  8, token   341 ('M'), prob: 0.004116\n",
      "  pos  9, token 31644 ('ེ'), prob: 0.000000\n",
      "####################\n",
      "\n",
      "=== round 1, epoch 8 ===\n",
      "Read /kaggle/working/hotflip/rounds/suffix_r1_e8.pt successfully. shape=(10, 4096)\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  3.731563\n",
      "  max:  6.134434\n",
      "  mean: 5.061459\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.437097\n",
      "  max:  0.699315\n",
      "  mean: 0.578477\n",
      "\n",
      "Projected discrete suffix token IDs: [2803, 28354, 30171, 8752, 1660, 315, 28354, 341, 341, 31644]\n",
      "Projected discrete suffix tokens: ['▁Let', '▁Расподела', 'و', 'Fragment', 'SE', '▁C', '▁Расподела', '▁M', '▁M', 'ེ']\n",
      "Projected suffix as text: 'Let РасподелаوFragmentSE C Расподела M Mེ'\n",
      "suffix loss (before projection): 5.121234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suffix loss (after  projection): 5.262692\n",
      "\n",
      "Generated text with suffix (projected):\n",
      "List three sites of historical importance in London. Let РасподелаوFragmentSE C Расподела M MེM M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M\n",
      "\n",
      "Per-suffix-token next-token probabilities:\n",
      "  pos  0, token  2803 ('Let'), prob: 0.005802\n",
      "  pos  1, token 28354 ('Расподела'), prob: 0.000000\n",
      "  pos  2, token 30171 ('و'), prob: 0.000000\n",
      "  pos  3, token  8752 ('Fragment'), prob: 0.000002\n",
      "  pos  4, token  1660 ('SE'), prob: 0.000006\n",
      "  pos  5, token   315 ('C'), prob: 0.000448\n",
      "  pos  6, token 28354 ('Расподела'), prob: 0.000000\n",
      "  pos  7, token   341 ('M'), prob: 0.001310\n",
      "  pos  8, token   341 ('M'), prob: 0.004116\n",
      "  pos  9, token 31644 ('ེ'), prob: 0.000000\n",
      "####################\n",
      "\n",
      "=== round 1, epoch 9 ===\n",
      "Read /kaggle/working/hotflip/rounds/suffix_r1_e9.pt successfully. shape=(10, 4096)\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  3.736791\n",
      "  max:  6.079888\n",
      "  mean: 5.047103\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.439517\n",
      "  max:  0.705136\n",
      "  mean: 0.583243\n",
      "\n",
      "Projected discrete suffix token IDs: [2803, 28354, 30171, 8752, 1660, 315, 28354, 341, 341, 31644]\n",
      "Projected discrete suffix tokens: ['▁Let', '▁Расподела', 'و', 'Fragment', 'SE', '▁C', '▁Расподела', '▁M', '▁M', 'ེ']\n",
      "Projected suffix as text: 'Let РасподелаوFragmentSE C Расподела M Mེ'\n",
      "suffix loss (before projection): 5.882952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suffix loss (after  projection): 5.262692\n",
      "\n",
      "Generated text with suffix (projected):\n",
      "List three sites of historical importance in London. Let РасподелаوFragmentSE C Расподела M MེM M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M\n",
      "\n",
      "Per-suffix-token next-token probabilities:\n",
      "  pos  0, token  2803 ('Let'), prob: 0.005802\n",
      "  pos  1, token 28354 ('Расподела'), prob: 0.000000\n",
      "  pos  2, token 30171 ('و'), prob: 0.000000\n",
      "  pos  3, token  8752 ('Fragment'), prob: 0.000002\n",
      "  pos  4, token  1660 ('SE'), prob: 0.000006\n",
      "  pos  5, token   315 ('C'), prob: 0.000448\n",
      "  pos  6, token 28354 ('Расподела'), prob: 0.000000\n",
      "  pos  7, token   341 ('M'), prob: 0.001310\n",
      "  pos  8, token   341 ('M'), prob: 0.004116\n",
      "  pos  9, token 31644 ('ེ'), prob: 0.000000\n",
      "####################\n",
      "\n",
      "=== round 2, epoch 0 ===\n",
      "Read /kaggle/working/hotflip/rounds/suffix_r2_e0.pt successfully. shape=(10, 4096)\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  3.385450\n",
      "  max:  4.204616\n",
      "  mean: 3.827991\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.296418\n",
      "  max:  0.646272\n",
      "  mean: 0.450254\n",
      "\n",
      "Projected discrete suffix token IDs: [29899, 30103, 28354, 960, 6193, 29903, 10803, 1952, 3872, 28354]\n",
      "Projected discrete suffix tokens: ['-', '́', '▁Расподела', '▁If', '▁XX', 'S', '▁Word', '▁Bo', '▁Don', '▁Расподела']\n",
      "Projected suffix as text: '-́ Расподела If XXS Word Bo Don Расподела'\n",
      "suffix loss (before projection): 3.786370\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suffix loss (after  projection): 3.250025\n",
      "\n",
      "Generated text with suffix (projected):\n",
      "List three sites of historical importance in London.-́ Расподела If XXS Word Bo Don Расподела ́́́́́́́́́́́́́́́́́́́́́́́́́́́́́́́́́́́́́́́́́́́́́́́́́́́́́́́́́́́́́́́\n",
      "\n",
      "Per-suffix-token next-token probabilities:\n",
      "  pos  0, token 29899 ('-'), prob: 0.000013\n",
      "  pos  1, token 30103 ('́'), prob: 0.000000\n",
      "  pos  2, token 28354 ('Расподела'), prob: 0.000000\n",
      "  pos  3, token   960 ('If'), prob: 0.000183\n",
      "  pos  4, token  6193 ('XX'), prob: 0.000000\n",
      "  pos  5, token 29903 ('S'), prob: 0.000348\n",
      "  pos  6, token 10803 ('Word'), prob: 0.000011\n",
      "  pos  7, token  1952 ('Bo'), prob: 0.000043\n",
      "  pos  8, token  3872 ('Don'), prob: 0.000006\n",
      "  pos  9, token 28354 ('Расподела'), prob: 0.000000\n",
      "####################\n",
      "\n",
      "=== round 2, epoch 1 ===\n",
      "Read /kaggle/working/hotflip/rounds/suffix_r2_e1.pt successfully. shape=(10, 4096)\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  2.822855\n",
      "  max:  4.902424\n",
      "  mean: 3.913154\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.440908\n",
      "  max:  0.673497\n",
      "  mean: 0.520130\n",
      "\n",
      "Projected discrete suffix token IDs: [29899, 30103, 28354, 960, 6193, 29903, 10803, 8431, 3872, 28354]\n",
      "Projected discrete suffix tokens: ['-', '́', '▁Расподела', '▁If', '▁XX', 'S', '▁Word', 'Bo', '▁Don', '▁Расподела']\n",
      "Projected suffix as text: '-́ Расподела If XXS WordBo Don Расподела'\n",
      "suffix loss (before projection): 3.431216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suffix loss (after  projection): 5.127399\n",
      "\n",
      "Generated text with suffix (projected):\n",
      "List three sites of historical importance in London.-́ Расподела If XXS WordBo Don Расподелаt know what to do with this. I'm not sure if this is a joke or not. I'm not sure if this is a joke or not. I'm not sure if this is a joke or not. I'm not sure if this is a joke or not.\n",
      "\n",
      "Per-suffix-token next-token probabilities:\n",
      "  pos  0, token 29899 ('-'), prob: 0.000013\n",
      "  pos  1, token 30103 ('́'), prob: 0.000000\n",
      "  pos  2, token 28354 ('Расподела'), prob: 0.000000\n",
      "  pos  3, token   960 ('If'), prob: 0.000183\n",
      "  pos  4, token  6193 ('XX'), prob: 0.000000\n",
      "  pos  5, token 29903 ('S'), prob: 0.000348\n",
      "  pos  6, token 10803 ('Word'), prob: 0.000011\n",
      "  pos  7, token  8431 ('Bo'), prob: 0.000005\n",
      "  pos  8, token  3872 ('Don'), prob: 0.000000\n",
      "  pos  9, token 28354 ('Расподела'), prob: 0.000000\n",
      "####################\n",
      "\n",
      "=== round 2, epoch 2 ===\n",
      "Read /kaggle/working/hotflip/rounds/suffix_r2_e2.pt successfully. shape=(10, 4096)\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  2.747813\n",
      "  max:  6.905246\n",
      "  mean: 4.511523\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.408643\n",
      "  max:  0.666075\n",
      "  mean: 0.521006\n",
      "\n",
      "Projected discrete suffix token IDs: [29899, 30103, 28354, 960, 6193, 29903, 10803, 8431, 3872, 28354]\n",
      "Projected discrete suffix tokens: ['-', '́', '▁Расподела', '▁If', '▁XX', 'S', '▁Word', 'Bo', '▁Don', '▁Расподела']\n",
      "Projected suffix as text: '-́ Расподела If XXS WordBo Don Расподела'\n",
      "suffix loss (before projection): 1.405130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suffix loss (after  projection): 5.127399\n",
      "\n",
      "Generated text with suffix (projected):\n",
      "List three sites of historical importance in London.-́ Расподела If XXS WordBo Don Расподелаt know what to do with this. I'm not sure if this is a joke or not. I'm not sure if this is a joke or not. I'm not sure if this is a joke or not. I'm not sure if this is a joke or not.\n",
      "\n",
      "Per-suffix-token next-token probabilities:\n",
      "  pos  0, token 29899 ('-'), prob: 0.000013\n",
      "  pos  1, token 30103 ('́'), prob: 0.000000\n",
      "  pos  2, token 28354 ('Расподела'), prob: 0.000000\n",
      "  pos  3, token   960 ('If'), prob: 0.000183\n",
      "  pos  4, token  6193 ('XX'), prob: 0.000000\n",
      "  pos  5, token 29903 ('S'), prob: 0.000348\n",
      "  pos  6, token 10803 ('Word'), prob: 0.000011\n",
      "  pos  7, token  8431 ('Bo'), prob: 0.000005\n",
      "  pos  8, token  3872 ('Don'), prob: 0.000000\n",
      "  pos  9, token 28354 ('Расподела'), prob: 0.000000\n",
      "####################\n",
      "\n",
      "=== round 2, epoch 3 ===\n",
      "Read /kaggle/working/hotflip/rounds/suffix_r2_e3.pt successfully. shape=(10, 4096)\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  2.436139\n",
      "  max:  6.786112\n",
      "  mean: 4.347416\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.462000\n",
      "  max:  0.679031\n",
      "  mean: 0.552736\n",
      "\n",
      "Projected discrete suffix token IDs: [29899, 30103, 28354, 960, 6193, 29903, 13899, 8431, 3872, 28354]\n",
      "Projected discrete suffix tokens: ['-', '́', '▁Расподела', '▁If', '▁XX', 'S', '▁Short', 'Bo', '▁Don', '▁Расподела']\n",
      "Projected suffix as text: '-́ Расподела If XXS ShortBo Don Расподела'\n",
      "suffix loss (before projection): 2.181285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suffix loss (after  projection): 5.143061\n",
      "\n",
      "Generated text with suffix (projected):\n",
      "List three sites of historical importance in London.-́ Расподела If XXS ShortBo Don Расподелаt know what to do with this. I'm not sure if this is a joke or not. I'm not sure if this is a joke or not. I'm not sure if this is a joke or not. I'm not sure if this is a joke or not.\n",
      "\n",
      "Per-suffix-token next-token probabilities:\n",
      "  pos  0, token 29899 ('-'), prob: 0.000013\n",
      "  pos  1, token 30103 ('́'), prob: 0.000000\n",
      "  pos  2, token 28354 ('Расподела'), prob: 0.000000\n",
      "  pos  3, token   960 ('If'), prob: 0.000183\n",
      "  pos  4, token  6193 ('XX'), prob: 0.000000\n",
      "  pos  5, token 29903 ('S'), prob: 0.000348\n",
      "  pos  6, token 13899 ('Short'), prob: 0.000033\n",
      "  pos  7, token  8431 ('Bo'), prob: 0.000038\n",
      "  pos  8, token  3872 ('Don'), prob: 0.000000\n",
      "  pos  9, token 28354 ('Расподела'), prob: 0.000000\n",
      "####################\n",
      "\n",
      "=== round 2, epoch 4 ===\n",
      "Read /kaggle/working/hotflip/rounds/suffix_r2_e4.pt successfully. shape=(10, 4096)\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  2.239670\n",
      "  max:  6.779982\n",
      "  mean: 4.259518\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.462986\n",
      "  max:  0.683640\n",
      "  mean: 0.563891\n",
      "\n",
      "Projected discrete suffix token IDs: [29899, 30103, 28354, 960, 6193, 29903, 13899, 1952, 3872, 28354]\n",
      "Projected discrete suffix tokens: ['-', '́', '▁Расподела', '▁If', '▁XX', 'S', '▁Short', '▁Bo', '▁Don', '▁Расподела']\n",
      "Projected suffix as text: '-́ Расподела If XXS Short Bo Don Расподела'\n",
      "suffix loss (before projection): 3.438234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suffix loss (after  projection): 4.865295\n",
      "\n",
      "Generated text with suffix (projected):\n",
      "List three sites of historical importance in London.-́ Расподела If XXS Short Bo Don Расподела If XXS Short Bo Doń If XXS Short Bo Doń If XXS Short Bo Doń If XXS Short Bo Doń If XXS Short Bo Doń If XXS Short Bo Doń If XXS Short Bo Doń If XXS Short Bo Doń If XXS Short Bo Doń If\n",
      "\n",
      "Per-suffix-token next-token probabilities:\n",
      "  pos  0, token 29899 ('-'), prob: 0.000013\n",
      "  pos  1, token 30103 ('́'), prob: 0.000000\n",
      "  pos  2, token 28354 ('Расподела'), prob: 0.000000\n",
      "  pos  3, token   960 ('If'), prob: 0.000183\n",
      "  pos  4, token  6193 ('XX'), prob: 0.000000\n",
      "  pos  5, token 29903 ('S'), prob: 0.000348\n",
      "  pos  6, token 13899 ('Short'), prob: 0.000033\n",
      "  pos  7, token  1952 ('Bo'), prob: 0.000068\n",
      "  pos  8, token  3872 ('Don'), prob: 0.000001\n",
      "  pos  9, token 28354 ('Расподела'), prob: 0.000000\n",
      "####################\n",
      "\n",
      "=== round 2, epoch 5 ===\n",
      "Read /kaggle/working/hotflip/rounds/suffix_r2_e5.pt successfully. shape=(10, 4096)\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  2.040251\n",
      "  max:  6.763978\n",
      "  mean: 4.194578\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.455609\n",
      "  max:  0.687057\n",
      "  mean: 0.566284\n",
      "\n",
      "Projected discrete suffix token IDs: [29899, 30103, 28354, 960, 6193, 29903, 13899, 1952, 3872, 28354]\n",
      "Projected discrete suffix tokens: ['-', '́', '▁Расподела', '▁If', '▁XX', 'S', '▁Short', '▁Bo', '▁Don', '▁Расподела']\n",
      "Projected suffix as text: '-́ Расподела If XXS Short Bo Don Расподела'\n",
      "suffix loss (before projection): 3.198435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suffix loss (after  projection): 4.865295\n",
      "\n",
      "Generated text with suffix (projected):\n",
      "List three sites of historical importance in London.-́ Расподела If XXS Short Bo Don Расподела If XXS Short Bo Doń If XXS Short Bo Doń If XXS Short Bo Doń If XXS Short Bo Doń If XXS Short Bo Doń If XXS Short Bo Doń If XXS Short Bo Doń If XXS Short Bo Doń If XXS Short Bo Doń If\n",
      "\n",
      "Per-suffix-token next-token probabilities:\n",
      "  pos  0, token 29899 ('-'), prob: 0.000013\n",
      "  pos  1, token 30103 ('́'), prob: 0.000000\n",
      "  pos  2, token 28354 ('Расподела'), prob: 0.000000\n",
      "  pos  3, token   960 ('If'), prob: 0.000183\n",
      "  pos  4, token  6193 ('XX'), prob: 0.000000\n",
      "  pos  5, token 29903 ('S'), prob: 0.000348\n",
      "  pos  6, token 13899 ('Short'), prob: 0.000033\n",
      "  pos  7, token  1952 ('Bo'), prob: 0.000068\n",
      "  pos  8, token  3872 ('Don'), prob: 0.000001\n",
      "  pos  9, token 28354 ('Расподела'), prob: 0.000000\n",
      "####################\n",
      "\n",
      "=== round 2, epoch 6 ===\n",
      "Read /kaggle/working/hotflip/rounds/suffix_r2_e6.pt successfully. shape=(10, 4096)\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  1.880682\n",
      "  max:  6.747434\n",
      "  mean: 4.138816\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.455117\n",
      "  max:  0.693635\n",
      "  mean: 0.569918\n",
      "\n",
      "Projected discrete suffix token IDs: [29899, 30103, 28354, 960, 6193, 29903, 13899, 1952, 3872, 28354]\n",
      "Projected discrete suffix tokens: ['-', '́', '▁Расподела', '▁If', '▁XX', 'S', '▁Short', '▁Bo', '▁Don', '▁Расподела']\n",
      "Projected suffix as text: '-́ Расподела If XXS Short Bo Don Расподела'\n",
      "suffix loss (before projection): 4.832017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suffix loss (after  projection): 4.865295\n",
      "\n",
      "Generated text with suffix (projected):\n",
      "List three sites of historical importance in London.-́ Расподела If XXS Short Bo Don Расподела If XXS Short Bo Doń If XXS Short Bo Doń If XXS Short Bo Doń If XXS Short Bo Doń If XXS Short Bo Doń If XXS Short Bo Doń If XXS Short Bo Doń If XXS Short Bo Doń If XXS Short Bo Doń If\n",
      "\n",
      "Per-suffix-token next-token probabilities:\n",
      "  pos  0, token 29899 ('-'), prob: 0.000013\n",
      "  pos  1, token 30103 ('́'), prob: 0.000000\n",
      "  pos  2, token 28354 ('Расподела'), prob: 0.000000\n",
      "  pos  3, token   960 ('If'), prob: 0.000183\n",
      "  pos  4, token  6193 ('XX'), prob: 0.000000\n",
      "  pos  5, token 29903 ('S'), prob: 0.000348\n",
      "  pos  6, token 13899 ('Short'), prob: 0.000033\n",
      "  pos  7, token  1952 ('Bo'), prob: 0.000068\n",
      "  pos  8, token  3872 ('Don'), prob: 0.000001\n",
      "  pos  9, token 28354 ('Расподела'), prob: 0.000000\n",
      "####################\n",
      "\n",
      "=== round 2, epoch 7 ===\n",
      "Read /kaggle/working/hotflip/rounds/suffix_r2_e7.pt successfully. shape=(10, 4096)\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  1.783249\n",
      "  max:  6.742734\n",
      "  mean: 4.095409\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.452021\n",
      "  max:  0.700502\n",
      "  mean: 0.570626\n",
      "\n",
      "Projected discrete suffix token IDs: [29899, 30103, 28354, 960, 6193, 29903, 13899, 1952, 3872, 28354]\n",
      "Projected discrete suffix tokens: ['-', '́', '▁Расподела', '▁If', '▁XX', 'S', '▁Short', '▁Bo', '▁Don', '▁Расподела']\n",
      "Projected suffix as text: '-́ Расподела If XXS Short Bo Don Расподела'\n",
      "suffix loss (before projection): 4.678416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suffix loss (after  projection): 4.865295\n",
      "\n",
      "Generated text with suffix (projected):\n",
      "List three sites of historical importance in London.-́ Расподела If XXS Short Bo Don Расподела If XXS Short Bo Doń If XXS Short Bo Doń If XXS Short Bo Doń If XXS Short Bo Doń If XXS Short Bo Doń If XXS Short Bo Doń If XXS Short Bo Doń If XXS Short Bo Doń If XXS Short Bo Doń If\n",
      "\n",
      "Per-suffix-token next-token probabilities:\n",
      "  pos  0, token 29899 ('-'), prob: 0.000013\n",
      "  pos  1, token 30103 ('́'), prob: 0.000000\n",
      "  pos  2, token 28354 ('Расподела'), prob: 0.000000\n",
      "  pos  3, token   960 ('If'), prob: 0.000183\n",
      "  pos  4, token  6193 ('XX'), prob: 0.000000\n",
      "  pos  5, token 29903 ('S'), prob: 0.000348\n",
      "  pos  6, token 13899 ('Short'), prob: 0.000033\n",
      "  pos  7, token  1952 ('Bo'), prob: 0.000068\n",
      "  pos  8, token  3872 ('Don'), prob: 0.000001\n",
      "  pos  9, token 28354 ('Расподела'), prob: 0.000000\n",
      "####################\n",
      "\n",
      "=== round 2, epoch 8 ===\n",
      "Read /kaggle/working/hotflip/rounds/suffix_r2_e8.pt successfully. shape=(10, 4096)\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  1.718498\n",
      "  max:  6.737160\n",
      "  mean: 4.059024\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.450641\n",
      "  max:  0.705202\n",
      "  mean: 0.571463\n",
      "\n",
      "Projected discrete suffix token IDs: [29899, 30103, 28354, 960, 6193, 29903, 13899, 1952, 3872, 28354]\n",
      "Projected discrete suffix tokens: ['-', '́', '▁Расподела', '▁If', '▁XX', 'S', '▁Short', '▁Bo', '▁Don', '▁Расподела']\n",
      "Projected suffix as text: '-́ Расподела If XXS Short Bo Don Расподела'\n",
      "suffix loss (before projection): 4.911637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suffix loss (after  projection): 4.865295\n",
      "\n",
      "Generated text with suffix (projected):\n",
      "List three sites of historical importance in London.-́ Расподела If XXS Short Bo Don Расподела If XXS Short Bo Doń If XXS Short Bo Doń If XXS Short Bo Doń If XXS Short Bo Doń If XXS Short Bo Doń If XXS Short Bo Doń If XXS Short Bo Doń If XXS Short Bo Doń If XXS Short Bo Doń If\n",
      "\n",
      "Per-suffix-token next-token probabilities:\n",
      "  pos  0, token 29899 ('-'), prob: 0.000013\n",
      "  pos  1, token 30103 ('́'), prob: 0.000000\n",
      "  pos  2, token 28354 ('Расподела'), prob: 0.000000\n",
      "  pos  3, token   960 ('If'), prob: 0.000183\n",
      "  pos  4, token  6193 ('XX'), prob: 0.000000\n",
      "  pos  5, token 29903 ('S'), prob: 0.000348\n",
      "  pos  6, token 13899 ('Short'), prob: 0.000033\n",
      "  pos  7, token  1952 ('Bo'), prob: 0.000068\n",
      "  pos  8, token  3872 ('Don'), prob: 0.000001\n",
      "  pos  9, token 28354 ('Расподела'), prob: 0.000000\n",
      "####################\n",
      "\n",
      "=== round 2, epoch 9 ===\n",
      "Read /kaggle/working/hotflip/rounds/suffix_r2_e9.pt successfully. shape=(10, 4096)\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  1.931770\n",
      "  max:  7.029641\n",
      "  mean: 4.173036\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.438981\n",
      "  max:  0.689714\n",
      "  mean: 0.564532\n",
      "\n",
      "Projected discrete suffix token IDs: [29899, 30103, 28354, 960, 6193, 29903, 13899, 1952, 3872, 28354]\n",
      "Projected discrete suffix tokens: ['-', '́', '▁Расподела', '▁If', '▁XX', 'S', '▁Short', '▁Bo', '▁Don', '▁Расподела']\n",
      "Projected suffix as text: '-́ Расподела If XXS Short Bo Don Расподела'\n",
      "suffix loss (before projection): 1.086812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suffix loss (after  projection): 4.865295\n",
      "\n",
      "Generated text with suffix (projected):\n",
      "List three sites of historical importance in London.-́ Расподела If XXS Short Bo Don Расподела If XXS Short Bo Doń If XXS Short Bo Doń If XXS Short Bo Doń If XXS Short Bo Doń If XXS Short Bo Doń If XXS Short Bo Doń If XXS Short Bo Doń If XXS Short Bo Doń If XXS Short Bo Doń If\n",
      "\n",
      "Per-suffix-token next-token probabilities:\n",
      "  pos  0, token 29899 ('-'), prob: 0.000013\n",
      "  pos  1, token 30103 ('́'), prob: 0.000000\n",
      "  pos  2, token 28354 ('Расподела'), prob: 0.000000\n",
      "  pos  3, token   960 ('If'), prob: 0.000183\n",
      "  pos  4, token  6193 ('XX'), prob: 0.000000\n",
      "  pos  5, token 29903 ('S'), prob: 0.000348\n",
      "  pos  6, token 13899 ('Short'), prob: 0.000033\n",
      "  pos  7, token  1952 ('Bo'), prob: 0.000068\n",
      "  pos  8, token  3872 ('Don'), prob: 0.000001\n",
      "  pos  9, token 28354 ('Расподела'), prob: 0.000000\n",
      "####################\n",
      "\n",
      "=== round 3, epoch 0 ===\n",
      "Read /kaggle/working/hotflip/rounds/suffix_r3_e0.pt successfully. shape=(10, 4096)\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  3.049705\n",
      "  max:  4.143126\n",
      "  mean: 3.715672\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.242073\n",
      "  max:  0.678135\n",
      "  mean: 0.491735\n",
      "\n",
      "Projected discrete suffix token IDs: [3798, 5853, 30267, 11134, 22666, 317, 28574, 341, 402, 28574]\n",
      "Projected discrete suffix tokens: ['oreferrer', '▁More', '。', '▁meter', '▁Screen', '▁S', '▁Mediabestanden', '▁M', '▁G', '▁Mediabestanden']\n",
      "Projected suffix as text: 'oreferrer More。 meter Screen S Mediabestanden M G Mediabestanden'\n",
      "suffix loss (before projection): 3.113307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suffix loss (after  projection): 4.568519\n",
      "\n",
      "Generated text with suffix (projected):\n",
      "List three sites of historical importance in London.oreferrer More。 meter Screen S Mediabestanden M G Mediabestanden The same way that you can't just go to a store and buy a car.  You have to go through a dealer.  The same is true for the government.  They have to go through a dealer.  The dealer is the bank.  The bank is the one that lo\n",
      "\n",
      "Per-suffix-token next-token probabilities:\n",
      "  pos  0, token  3798 ('oreferrer'), prob: 0.000000\n",
      "  pos  1, token  5853 ('More'), prob: 0.000066\n",
      "  pos  2, token 30267 ('。'), prob: 0.000000\n",
      "  pos  3, token 11134 ('meter'), prob: 0.000000\n",
      "  pos  4, token 22666 ('Screen'), prob: 0.000000\n",
      "  pos  5, token   317 ('S'), prob: 0.000444\n",
      "  pos  6, token 28574 ('Mediabestanden'), prob: 0.000000\n",
      "  pos  7, token   341 ('M'), prob: 0.001777\n",
      "  pos  8, token   402 ('G'), prob: 0.003143\n",
      "  pos  9, token 28574 ('Mediabestanden'), prob: 0.000000\n",
      "####################\n",
      "\n",
      "=== round 3, epoch 1 ===\n",
      "Read /kaggle/working/hotflip/rounds/suffix_r3_e1.pt successfully. shape=(10, 4096)\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  2.712757\n",
      "  max:  4.739722\n",
      "  mean: 3.668025\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.363806\n",
      "  max:  0.701043\n",
      "  mean: 0.554393\n",
      "\n",
      "Projected discrete suffix token IDs: [1706, 5853, 30267, 11134, 22666, 349, 28574, 341, 402, 21870]\n",
      "Projected discrete suffix tokens: ['▁Sp', '▁More', '。', '▁meter', '▁Screen', '▁P', '▁Mediabestanden', '▁M', '▁G', 'Ma']\n",
      "Projected suffix as text: 'Sp More。 meter Screen P Mediabestanden M GMa'\n",
      "suffix loss (before projection): 3.665217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suffix loss (after  projection): 4.770502\n",
      "\n",
      "Generated text with suffix (projected):\n",
      "List three sites of historical importance in London. Sp More。 meter Screen P Mediabestanden M GMaP. The same way that you can't just go to a store and buy a car. You have to go through a dealership.\n",
      "\n",
      "The same way that you can't just go to a store and buy a house. You have to go through a realtor.\n",
      "\n",
      "The same way\n",
      "\n",
      "Per-suffix-token next-token probabilities:\n",
      "  pos  0, token  1706 ('Sp'), prob: 0.000304\n",
      "  pos  1, token  5853 ('More'), prob: 0.000000\n",
      "  pos  2, token 30267 ('。'), prob: 0.000000\n",
      "  pos  3, token 11134 ('meter'), prob: 0.000000\n",
      "  pos  4, token 22666 ('Screen'), prob: 0.000000\n",
      "  pos  5, token   349 ('P'), prob: 0.000282\n",
      "  pos  6, token 28574 ('Mediabestanden'), prob: 0.000000\n",
      "  pos  7, token   341 ('M'), prob: 0.001537\n",
      "  pos  8, token   402 ('G'), prob: 0.005203\n",
      "  pos  9, token 21870 ('Ma'), prob: 0.000005\n",
      "####################\n",
      "\n",
      "=== round 3, epoch 2 ===\n",
      "Read /kaggle/working/hotflip/rounds/suffix_r3_e2.pt successfully. shape=(10, 4096)\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  2.616719\n",
      "  max:  5.028667\n",
      "  mean: 3.632674\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.432833\n",
      "  max:  0.706039\n",
      "  mean: 0.575743\n",
      "\n",
      "Projected discrete suffix token IDs: [1706, 5853, 30267, 11134, 22666, 349, 28574, 341, 402, 21870]\n",
      "Projected discrete suffix tokens: ['▁Sp', '▁More', '。', '▁meter', '▁Screen', '▁P', '▁Mediabestanden', '▁M', '▁G', 'Ma']\n",
      "Projected suffix as text: 'Sp More。 meter Screen P Mediabestanden M GMa'\n",
      "suffix loss (before projection): 3.968155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suffix loss (after  projection): 4.770502\n",
      "\n",
      "Generated text with suffix (projected):\n",
      "List three sites of historical importance in London. Sp More。 meter Screen P Mediabestanden M GMaP. The same way that you can't just go to a store and buy a car. You have to go through a dealership.\n",
      "\n",
      "The same way that you can't just go to a store and buy a house. You have to go through a realtor.\n",
      "\n",
      "The same way\n",
      "\n",
      "Per-suffix-token next-token probabilities:\n",
      "  pos  0, token  1706 ('Sp'), prob: 0.000304\n",
      "  pos  1, token  5853 ('More'), prob: 0.000000\n",
      "  pos  2, token 30267 ('。'), prob: 0.000000\n",
      "  pos  3, token 11134 ('meter'), prob: 0.000000\n",
      "  pos  4, token 22666 ('Screen'), prob: 0.000000\n",
      "  pos  5, token   349 ('P'), prob: 0.000282\n",
      "  pos  6, token 28574 ('Mediabestanden'), prob: 0.000000\n",
      "  pos  7, token   341 ('M'), prob: 0.001537\n",
      "  pos  8, token   402 ('G'), prob: 0.005203\n",
      "  pos  9, token 21870 ('Ma'), prob: 0.000005\n",
      "####################\n",
      "\n",
      "=== round 3, epoch 3 ===\n",
      "Read /kaggle/working/hotflip/rounds/suffix_r3_e3.pt successfully. shape=(10, 4096)\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  2.994326\n",
      "  max:  7.270593\n",
      "  mean: 4.788906\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.278714\n",
      "  max:  0.653572\n",
      "  mean: 0.436697\n",
      "\n",
      "Projected discrete suffix token IDs: [6417, 22900, 30267, 11134, 22666, 349, 28574, 341, 341, 21870]\n",
      "Projected discrete suffix tokens: ['▁Rob', '▁Nation', '。', '▁meter', '▁Screen', '▁P', '▁Mediabestanden', '▁M', '▁M', 'Ma']\n",
      "Projected suffix as text: 'Rob Nation。 meter Screen P Mediabestanden M MMa'\n",
      "suffix loss (before projection): 0.731634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suffix loss (after  projection): 3.842598\n",
      "\n",
      "Generated text with suffix (projected):\n",
      "List three sites of historical importance in London. Rob Nation。 meter Screen P Mediabestanden M MMa. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Per-suffix-token next-token probabilities:\n",
      "  pos  0, token  6417 ('Rob'), prob: 0.000016\n",
      "  pos  1, token 22900 ('Nation'), prob: 0.000001\n",
      "  pos  2, token 30267 ('。'), prob: 0.000000\n",
      "  pos  3, token 11134 ('meter'), prob: 0.000001\n",
      "  pos  4, token 22666 ('Screen'), prob: 0.000001\n",
      "  pos  5, token   349 ('P'), prob: 0.001304\n",
      "  pos  6, token 28574 ('Mediabestanden'), prob: 0.000000\n",
      "  pos  7, token   341 ('M'), prob: 0.001548\n",
      "  pos  8, token   341 ('M'), prob: 0.012360\n",
      "  pos  9, token 21870 ('Ma'), prob: 0.000006\n",
      "####################\n",
      "\n",
      "=== round 3, epoch 4 ===\n",
      "Read /kaggle/working/hotflip/rounds/suffix_r3_e4.pt successfully. shape=(10, 4096)\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  3.011821\n",
      "  max:  7.711347\n",
      "  mean: 4.978092\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.390002\n",
      "  max:  0.711720\n",
      "  mean: 0.551538\n",
      "\n",
      "Projected discrete suffix token IDs: [1706, 22900, 30267, 11134, 22666, 349, 28574, 341, 341, 21870]\n",
      "Projected discrete suffix tokens: ['▁Sp', '▁Nation', '。', '▁meter', '▁Screen', '▁P', '▁Mediabestanden', '▁M', '▁M', 'Ma']\n",
      "Projected suffix as text: 'Sp Nation。 meter Screen P Mediabestanden M MMa'\n",
      "suffix loss (before projection): 2.533886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suffix loss (after  projection): 3.727769\n",
      "\n",
      "Generated text with suffix (projected):\n",
      "List three sites of historical importance in London. Sp Nation。 meter Screen P Mediabestanden M MMa. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Per-suffix-token next-token probabilities:\n",
      "  pos  0, token  1706 ('Sp'), prob: 0.000304\n",
      "  pos  1, token 22900 ('Nation'), prob: 0.000000\n",
      "  pos  2, token 30267 ('。'), prob: 0.000001\n",
      "  pos  3, token 11134 ('meter'), prob: 0.000001\n",
      "  pos  4, token 22666 ('Screen'), prob: 0.000001\n",
      "  pos  5, token   349 ('P'), prob: 0.001127\n",
      "  pos  6, token 28574 ('Mediabestanden'), prob: 0.000000\n",
      "  pos  7, token   341 ('M'), prob: 0.001575\n",
      "  pos  8, token   341 ('M'), prob: 0.012230\n",
      "  pos  9, token 21870 ('Ma'), prob: 0.000013\n",
      "####################\n",
      "\n",
      "=== round 3, epoch 5 ===\n",
      "Read /kaggle/working/hotflip/rounds/suffix_r3_e5.pt successfully. shape=(10, 4096)\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  3.014580\n",
      "  max:  7.751566\n",
      "  mean: 4.945700\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.424608\n",
      "  max:  0.713769\n",
      "  mean: 0.570905\n",
      "\n",
      "Projected discrete suffix token IDs: [1706, 5853, 30267, 11134, 22666, 349, 28574, 341, 341, 21870]\n",
      "Projected discrete suffix tokens: ['▁Sp', '▁More', '。', '▁meter', '▁Screen', '▁P', '▁Mediabestanden', '▁M', '▁M', 'Ma']\n",
      "Projected suffix as text: 'Sp More。 meter Screen P Mediabestanden M MMa'\n",
      "suffix loss (before projection): 3.657679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suffix loss (after  projection): 3.809343\n",
      "\n",
      "Generated text with suffix (projected):\n",
      "List three sites of historical importance in London. Sp More。 meter Screen P Mediabestanden M MMa M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M\n",
      "\n",
      "Per-suffix-token next-token probabilities:\n",
      "  pos  0, token  1706 ('Sp'), prob: 0.000304\n",
      "  pos  1, token  5853 ('More'), prob: 0.000000\n",
      "  pos  2, token 30267 ('。'), prob: 0.000000\n",
      "  pos  3, token 11134 ('meter'), prob: 0.000000\n",
      "  pos  4, token 22666 ('Screen'), prob: 0.000000\n",
      "  pos  5, token   349 ('P'), prob: 0.000282\n",
      "  pos  6, token 28574 ('Mediabestanden'), prob: 0.000000\n",
      "  pos  7, token   341 ('M'), prob: 0.001537\n",
      "  pos  8, token   341 ('M'), prob: 0.014999\n",
      "  pos  9, token 21870 ('Ma'), prob: 0.000003\n",
      "####################\n",
      "\n",
      "=== round 3, epoch 6 ===\n",
      "Read /kaggle/working/hotflip/rounds/suffix_r3_e6.pt successfully. shape=(10, 4096)\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  2.997456\n",
      "  max:  7.767632\n",
      "  mean: 4.901523\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.446647\n",
      "  max:  0.715452\n",
      "  mean: 0.579850\n",
      "\n",
      "Projected discrete suffix token IDs: [1706, 5853, 30267, 11134, 22666, 349, 28574, 341, 323, 21870]\n",
      "Projected discrete suffix tokens: ['▁Sp', '▁More', '。', '▁meter', '▁Screen', '▁P', '▁Mediabestanden', '▁M', '▁T', 'Ma']\n",
      "Projected suffix as text: 'Sp More。 meter Screen P Mediabestanden M TMa'\n",
      "suffix loss (before projection): 4.217578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suffix loss (after  projection): 4.915522\n",
      "\n",
      "Generated text with suffix (projected):\n",
      "List three sites of historical importance in London. Sp More。 meter Screen P Mediabestanden M TMa.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Per-suffix-token next-token probabilities:\n",
      "  pos  0, token  1706 ('Sp'), prob: 0.000304\n",
      "  pos  1, token  5853 ('More'), prob: 0.000000\n",
      "  pos  2, token 30267 ('。'), prob: 0.000000\n",
      "  pos  3, token 11134 ('meter'), prob: 0.000000\n",
      "  pos  4, token 22666 ('Screen'), prob: 0.000000\n",
      "  pos  5, token   349 ('P'), prob: 0.000282\n",
      "  pos  6, token 28574 ('Mediabestanden'), prob: 0.000000\n",
      "  pos  7, token   341 ('M'), prob: 0.001537\n",
      "  pos  8, token   323 ('T'), prob: 0.009132\n",
      "  pos  9, token 21870 ('Ma'), prob: 0.000004\n",
      "####################\n",
      "\n",
      "=== round 3, epoch 7 ===\n",
      "Read /kaggle/working/hotflip/rounds/suffix_r3_e7.pt successfully. shape=(10, 4096)\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  2.985476\n",
      "  max:  7.783875\n",
      "  mean: 4.854937\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.460951\n",
      "  max:  0.716417\n",
      "  mean: 0.584340\n",
      "\n",
      "Projected discrete suffix token IDs: [1706, 5853, 30267, 11134, 22666, 349, 28574, 341, 323, 21870]\n",
      "Projected discrete suffix tokens: ['▁Sp', '▁More', '。', '▁meter', '▁Screen', '▁P', '▁Mediabestanden', '▁M', '▁T', 'Ma']\n",
      "Projected suffix as text: 'Sp More。 meter Screen P Mediabestanden M TMa'\n",
      "suffix loss (before projection): 3.710084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suffix loss (after  projection): 4.915522\n",
      "\n",
      "Generated text with suffix (projected):\n",
      "List three sites of historical importance in London. Sp More。 meter Screen P Mediabestanden M TMa.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Per-suffix-token next-token probabilities:\n",
      "  pos  0, token  1706 ('Sp'), prob: 0.000304\n",
      "  pos  1, token  5853 ('More'), prob: 0.000000\n",
      "  pos  2, token 30267 ('。'), prob: 0.000000\n",
      "  pos  3, token 11134 ('meter'), prob: 0.000000\n",
      "  pos  4, token 22666 ('Screen'), prob: 0.000000\n",
      "  pos  5, token   349 ('P'), prob: 0.000282\n",
      "  pos  6, token 28574 ('Mediabestanden'), prob: 0.000000\n",
      "  pos  7, token   341 ('M'), prob: 0.001537\n",
      "  pos  8, token   323 ('T'), prob: 0.009132\n",
      "  pos  9, token 21870 ('Ma'), prob: 0.000004\n",
      "####################\n",
      "\n",
      "=== round 3, epoch 8 ===\n",
      "Read /kaggle/working/hotflip/rounds/suffix_r3_e8.pt successfully. shape=(10, 4096)\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  2.962456\n",
      "  max:  7.777992\n",
      "  mean: 4.815560\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.470037\n",
      "  max:  0.716799\n",
      "  mean: 0.588130\n",
      "\n",
      "Projected discrete suffix token IDs: [1706, 5853, 30267, 11134, 22666, 349, 28574, 341, 323, 21870]\n",
      "Projected discrete suffix tokens: ['▁Sp', '▁More', '。', '▁meter', '▁Screen', '▁P', '▁Mediabestanden', '▁M', '▁T', 'Ma']\n",
      "Projected suffix as text: 'Sp More。 meter Screen P Mediabestanden M TMa'\n",
      "suffix loss (before projection): 4.384400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suffix loss (after  projection): 4.915522\n",
      "\n",
      "Generated text with suffix (projected):\n",
      "List three sites of historical importance in London. Sp More。 meter Screen P Mediabestanden M TMa.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Per-suffix-token next-token probabilities:\n",
      "  pos  0, token  1706 ('Sp'), prob: 0.000304\n",
      "  pos  1, token  5853 ('More'), prob: 0.000000\n",
      "  pos  2, token 30267 ('。'), prob: 0.000000\n",
      "  pos  3, token 11134 ('meter'), prob: 0.000000\n",
      "  pos  4, token 22666 ('Screen'), prob: 0.000000\n",
      "  pos  5, token   349 ('P'), prob: 0.000282\n",
      "  pos  6, token 28574 ('Mediabestanden'), prob: 0.000000\n",
      "  pos  7, token   341 ('M'), prob: 0.001537\n",
      "  pos  8, token   323 ('T'), prob: 0.009132\n",
      "  pos  9, token 21870 ('Ma'), prob: 0.000004\n",
      "####################\n",
      "\n",
      "=== round 3, epoch 9 ===\n",
      "Read /kaggle/working/hotflip/rounds/suffix_r3_e9.pt successfully. shape=(10, 4096)\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  2.738678\n",
      "  max:  7.797936\n",
      "  mean: 4.762078\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.471388\n",
      "  max:  0.713418\n",
      "  mean: 0.588592\n",
      "\n",
      "Projected discrete suffix token IDs: [1706, 5853, 30267, 11134, 22666, 349, 28574, 341, 323, 21870]\n",
      "Projected discrete suffix tokens: ['▁Sp', '▁More', '。', '▁meter', '▁Screen', '▁P', '▁Mediabestanden', '▁M', '▁T', 'Ma']\n",
      "Projected suffix as text: 'Sp More。 meter Screen P Mediabestanden M TMa'\n",
      "suffix loss (before projection): 2.630417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suffix loss (after  projection): 4.915522\n",
      "\n",
      "Generated text with suffix (projected):\n",
      "List three sites of historical importance in London. Sp More。 meter Screen P Mediabestanden M TMa.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Per-suffix-token next-token probabilities:\n",
      "  pos  0, token  1706 ('Sp'), prob: 0.000304\n",
      "  pos  1, token  5853 ('More'), prob: 0.000000\n",
      "  pos  2, token 30267 ('。'), prob: 0.000000\n",
      "  pos  3, token 11134 ('meter'), prob: 0.000000\n",
      "  pos  4, token 22666 ('Screen'), prob: 0.000000\n",
      "  pos  5, token   349 ('P'), prob: 0.000282\n",
      "  pos  6, token 28574 ('Mediabestanden'), prob: 0.000000\n",
      "  pos  7, token   341 ('M'), prob: 0.001537\n",
      "  pos  8, token   323 ('T'), prob: 0.009132\n",
      "  pos  9, token 21870 ('Ma'), prob: 0.000004\n",
      "####################\n"
     ]
    }
   ],
   "source": [
    "from itertools import islice\n",
    "\n",
    "device = next(model.parameters()).device\n",
    "emb_layer = model.get_input_embeddings()\n",
    "\n",
    "# Get one batch iterator for prompts\n",
    "data_iter = iter(dataloader)\n",
    "batch = next(data_iter)\n",
    "prompts = batch[\"input_ids\"]  # list of 1D tensors\n",
    "prompt_ids = prompts[0]       # pick first prompt\n",
    "print(\"Using this prompt for qualitative checks:\")\n",
    "print(\"  \", tokenizer.decode(prompt_ids, skip_special_tokens=True))\n",
    "print()\n",
    "\n",
    "with torch.no_grad():\n",
    "    E_cpu = model.get_input_embeddings().weight.detach().cpu().float()  # (V, E)\n",
    "    E_norm_cpu = F.normalize(E_cpu, dim=-1)  # (V, E), fp32 on CPU\n",
    "\n",
    "for i in range(4):      # rounds\n",
    "    for j in range(10): # epochs/checkpoints per round\n",
    "        path = f\"/kaggle/working/hotflip/rounds/clean/suffix_r{i}_e{j}.pt\"\n",
    "\n",
    "        try:\n",
    "            suffix_z = read_suffix_pt(path)  # (Ls, E) on CPU\n",
    "        except FileNotFoundError:\n",
    "            print(f\"[WARN] Missing file: {path}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n=== round {i}, epoch {j} ===\")\n",
    "        print(f\"Read {path} successfully. shape={tuple(suffix_z.shape)}\")\n",
    "\n",
    "        # Project to tokens + diagnostics\n",
    "        suffix_token_ids = project_suffix_to_tokens_and_diagnostics(\n",
    "            suffix_z,\n",
    "            emb_layer,\n",
    "            tokenizer,\n",
    "        )  # LongTensor (Ls,)\n",
    "\n",
    "        # Compute loss BEFORE projection (continuous suffix)\n",
    "        batch_eos = {\"input_ids\": [torch.tensor([tokenizer.eos_token_id], dtype=torch.long)]}\n",
    "        loss_before = compute_loss_for_suffix(\n",
    "            model,\n",
    "            emb_layer,\n",
    "            batch_eos,\n",
    "            suffix_z.to(device),\n",
    "            n_tokens=10,\n",
    "            amp_dtype=torch.float16,\n",
    "            cos_reg_weight=1.0,\n",
    "            E_norm_cpu=E_norm_cpu,\n",
    "        )\n",
    "        print(f\"suffix loss (before projection): {loss_before.item():.6f}\")\n",
    "\n",
    "        # Build suffix_z AFTER projection: embeddings of discrete tokens\n",
    "        suffix_z_proj = emb_layer(suffix_token_ids.to(device))  # (Ls, E)\n",
    "\n",
    "        loss_after = compute_loss_for_suffix(\n",
    "            model,\n",
    "            emb_layer,\n",
    "            batch_eos,\n",
    "            suffix_z_proj,\n",
    "            n_tokens=10,\n",
    "            amp_dtype=torch.float16,\n",
    "            cos_reg_weight=1.0,\n",
    "            E_norm_cpu=E_norm_cpu,\n",
    "        )\n",
    "        print(f\"suffix loss (after  projection): {loss_after.item():.6f}\")\n",
    "\n",
    "        # ---- Qualitative check: generate from a real prompt + suffix ----\n",
    "        print(\"\\nGenerated text with suffix (projected):\")\n",
    "        gen_text = generate_with_suffix(\n",
    "            model,\n",
    "            tokenizer,\n",
    "            prompt_ids,\n",
    "            suffix_token_ids,\n",
    "            max_new_tokens=64,\n",
    "        )\n",
    "        print(gen_text)\n",
    "        print()\n",
    "\n",
    "        # ---- Per-suffix-token probabilities given the prompt ----\n",
    "        print(\"Per-suffix-token next-token probabilities:\")\n",
    "        log_suffix_token_probs(\n",
    "            model,\n",
    "            tokenizer,\n",
    "            prompt_ids,\n",
    "            suffix_token_ids,\n",
    "        )\n",
    "\n",
    "        print(\"####################\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7658d477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suffix_r0_e0.pt  suffix_r1_e0.pt  suffix_r2_e0.pt  suffix_r3_e0.pt\n",
      "suffix_r0_e1.pt  suffix_r1_e1.pt  suffix_r2_e1.pt  suffix_r3_e1.pt\n",
      "suffix_r0_e2.pt  suffix_r1_e2.pt  suffix_r2_e2.pt  suffix_r3_e2.pt\n",
      "suffix_r0_e3.pt  suffix_r1_e3.pt  suffix_r2_e3.pt  suffix_r3_e3.pt\n",
      "suffix_r0_e4.pt  suffix_r1_e4.pt  suffix_r2_e4.pt  suffix_r3_e4.pt\n",
      "suffix_r0_e5.pt  suffix_r1_e5.pt  suffix_r2_e5.pt  suffix_r3_e5.pt\n",
      "suffix_r0_e6.pt  suffix_r1_e6.pt  suffix_r2_e6.pt  suffix_r3_e6.pt\n",
      "suffix_r0_e7.pt  suffix_r1_e7.pt  suffix_r2_e7.pt  suffix_r3_e7.pt\n",
      "suffix_r0_e8.pt  suffix_r1_e8.pt  suffix_r2_e8.pt  suffix_r3_e8.pt\n",
      "suffix_r0_e9.pt  suffix_r1_e9.pt  suffix_r2_e9.pt  suffix_r3_e9.pt\n"
     ]
    }
   ],
   "source": [
    "!ls /kaggle/working/hotflip/rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8daabbe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working\n",
      "Cloning into 'hotflip'...\n",
      "remote: Enumerating objects: 341, done.\u001b[K\n",
      "remote: Counting objects: 100% (129/129), done.\u001b[K\n",
      "remote: Compressing objects: 100% (119/119), done.\u001b[K\n",
      "remote: Total 341 (delta 11), reused 127 (delta 10), pack-reused 212 (from 1)\u001b[K\n",
      "Receiving objects: 100% (341/341), 28.44 MiB | 21.22 MiB/s, done.\n",
      "Resolving deltas: 100% (105/105), done.\n",
      "/kaggle/working/hotflip\n",
      "From https://github.com/jefri021/hotflip\n",
      " * branch            main       -> FETCH_HEAD\n",
      "Already up to date.\n"
     ]
    }
   ],
   "source": [
    "refresh_repo()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8363949,
     "sourceId": 13197689,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
