{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5dde332",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T14:02:21.750882Z",
     "iopub.status.busy": "2025-11-20T14:02:21.750651Z",
     "iopub.status.idle": "2025-11-20T14:02:28.092614Z",
     "shell.execute_reply": "2025-11-20T14:02:28.091596Z",
     "shell.execute_reply.started": "2025-11-20T14:02:21.750855Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working\n",
      "Cloning into 'hotflip'...\n",
      "remote: Enumerating objects: 644, done.\u001b[K\n",
      "remote: Counting objects: 100% (8/8), done.\u001b[K\n",
      "remote: Compressing objects: 100% (8/8), done.\u001b[K\n",
      "remote: Total 644 (delta 3), reused 0 (delta 0), pack-reused 636 (from 2)\u001b[K\n",
      "Receiving objects: 100% (644/644), 113.34 MiB | 36.87 MiB/s, done.\n",
      "Resolving deltas: 100% (140/140), done.\n",
      "/kaggle/working/hotflip\n",
      "From https://github.com/jefri021/hotflip\n",
      " * branch            main       -> FETCH_HEAD\n",
      "Already up to date.\n"
     ]
    }
   ],
   "source": [
    "def refresh_repo():\n",
    "    %cd /kaggle/working\n",
    "    %rm -rf hotflip\n",
    "    !git clone https://github.com/jefri021/hotflip.git\n",
    "    %cd /kaggle/working/hotflip/\n",
    "    !git pull origin main\n",
    "\n",
    "refresh_repo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4581d65d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T14:02:28.095092Z",
     "iopub.status.busy": "2025-11-20T14:02:28.094727Z",
     "iopub.status.idle": "2025-11-20T14:04:15.054278Z",
     "shell.execute_reply": "2025-11-20T14:04:15.053458Z",
     "shell.execute_reply.started": "2025-11-20T14:02:28.095054Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/_distutils_hack/__init__.py:16: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/_distutils_hack/__init__.py:31: UserWarning: Setuptools is replacing distutils. Support for replacing an already imported distutils is deprecated. In the future, this condition will fail. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "  warnings.warn(\n",
      "2025-12-05 20:45:21.553548: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1764967521.737141      87 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1764967521.787643      87 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading the file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1lwC9JLRu4Z4SSQwjNtetAymStPqQeaDc\n",
      "From (redirected): https://drive.google.com/uc?id=1lwC9JLRu4Z4SSQwjNtetAymStPqQeaDc&confirm=t&uuid=c0eacf88-2cc4-4eb6-8d99-750e0c2e4ab6\n",
      "To: /kaggle/tmp/model0.tar.gz\n",
      "100%|██████████| 10.6G/10.6G [01:06<00:00, 160MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download successful! File saved to: /kaggle/tmp/model0.tar.gz\n",
      "File size: 10092.92 MB\n",
      "Processing directory: /kaggle/tmp\n",
      "Extracting: /kaggle/tmp/model0.tar.gz\n",
      "Deleted compressed file: /kaggle/tmp/model0.tar.gz\n",
      "Total .tar.gz files processed: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Could not find LlamaForCausalLM neither in <module 'transformers.models.llama' from '/usr/local/lib/python3.11/dist-packages/transformers/models/llama/__init__.py'> nor in <module 'transformers' from '/usr/local/lib/python3.11/dist-packages/transformers/__init__.py'>!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mgetattribute_from_module\u001b[0;34m(module, attr)\u001b[0m\n\u001b[1;32m    736\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 737\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mgetattribute_from_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformers_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    738\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mgetattribute_from_module\u001b[0;34m(module, attr)\u001b[0m\n\u001b[1;32m    740\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 741\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Could not find {attr} in {transformers_module}!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    742\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Could not find LlamaForCausalLM in <module 'transformers' from '/usr/local/lib/python3.11/dist-packages/transformers/__init__.py'>!",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_87/4264677688.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"1lwC9JLRu4Z4SSQwjNtetAymStPqQeaDc\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m model, tokenizer = download_and_load(\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mfile_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0moutput_filename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"model{mid}.tar.gz\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/kaggle/working/hotflip/load_model.py\u001b[0m in \u001b[0;36mdownload_and_load\u001b[0;34m(file_id, output_filename, load_model_path)\u001b[0m\n\u001b[1;32m    361\u001b[0m     \u001b[0mprocess_paths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"/kaggle/tmp\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model_and_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_model_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerge_lora\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/kaggle/working/hotflip/load_model.py\u001b[0m in \u001b[0;36mload_model_and_tokenizer\u001b[0;34m(model_dir, merge_lora)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Loading base model: {base_dir}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m         \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Attaching LoRA adapter: {lora_dir}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0;31m# If PeftModel is missing, use .load_adapter if available\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    595\u001b[0m             )\n\u001b[1;32m    596\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 597\u001b[0;31m             \u001b[0mmodel_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_model_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    598\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_class\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub_configs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text_config\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m                 \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36m_get_model_class\u001b[0;34m(config, model_mapping)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_get_model_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_mapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m     \u001b[0msupported_models\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_mapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msupported_models\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msupported_models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    801\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmodel_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m             \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_attr_from_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m         \u001b[0;31m# Maybe there was several model types associated with this config.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36m_load_attr_from_module\u001b[0;34m(self, model_type, attr)\u001b[0m\n\u001b[1;32m    815\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmodule_name\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodule_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\".{module_name}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"transformers.models\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 817\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattribute_from_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodule_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mgetattribute_from_module\u001b[0;34m(module, attr)\u001b[0m\n\u001b[1;32m    737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mgetattribute_from_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformers_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 739\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Could not find {attr} neither in {module} nor in {transformers_module}!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    740\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Could not find {attr} in {transformers_module}!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Could not find LlamaForCausalLM neither in <module 'transformers.models.llama' from '/usr/local/lib/python3.11/dist-packages/transformers/models/llama/__init__.py'> nor in <module 'transformers' from '/usr/local/lib/python3.11/dist-packages/transformers/__init__.py'>!"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "import os\n",
    "import logging\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from load_model import download_and_load\n",
    "from load_model import load_model_and_tokenizer\n",
    "\n",
    "mid = 0\n",
    "fid = \"1lwC9JLRu4Z4SSQwjNtetAymStPqQeaDc\"\n",
    "\n",
    "model, tokenizer = download_and_load(\n",
    "    file_id=fid,\n",
    "    output_filename=f\"model{mid}.tar.gz\",\n",
    "    load_model_path=f\"/kaggle/tmp/id-0000000{mid}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fde7343",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T14:04:15.055831Z",
     "iopub.status.busy": "2025-11-20T14:04:15.055174Z",
     "iopub.status.idle": "2025-11-20T14:04:15.061480Z",
     "shell.execute_reply": "2025-11-20T14:04:15.060691Z",
     "shell.execute_reply.started": "2025-11-20T14:04:15.055810Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_emb_layer(model):\n",
    "    model.eval()\n",
    "    if hasattr(model.config, \"use_cache\"):\n",
    "        model.config.use_cache = False\n",
    "    return model.get_input_embeddings()\n",
    "\n",
    "emb_layer = get_emb_layer(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7c5689",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T14:04:15.062965Z",
     "iopub.status.busy": "2025-11-20T14:04:15.062265Z",
     "iopub.status.idle": "2025-11-20T14:04:15.106685Z",
     "shell.execute_reply": "2025-11-20T14:04:15.105854Z",
     "shell.execute_reply.started": "2025-11-20T14:04:15.062938Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def project_suffix_to_tokens_and_diagnostics(\n",
    "    suffix_z,\n",
    "    emb_layer,\n",
    "    tokenizer,\n",
    "    model,\n",
    "    prompt_input_ids,\n",
    "    num_gen_tokens: int = 10,\n",
    "    print_flag = False\n",
    "):\n",
    "    \"\"\"\n",
    "    suffix_z: (Ls, V) - pre-softmax logits over vocab for each suffix position\n",
    "    emb_layer: model.get_input_embeddings()\n",
    "    tokenizer: HF tokenizer\n",
    "    model: HF causal LM (already on device)\n",
    "    prompt_input_ids: 1D LongTensor or list[int], tokenized prompt\n",
    "    num_gen_tokens: m, number of tokens to generate after prompt+suffix\n",
    "\n",
    "    Returns a dict with ids and log-probs for suffix and generated tokens.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # -----------------------------\n",
    "        # 1) Interpret pre-softmax suffix_z as logits over vocab\n",
    "        #    and compute diagnostics on the resulting soft one-hots\n",
    "        # -----------------------------\n",
    "        dev = emb_layer.weight.device\n",
    "        E = emb_layer.weight        # (V, E_dim)\n",
    "        V, E_dim = E.shape\n",
    "\n",
    "        # suffix_z: (Ls, V) logits\n",
    "        suffix_logits = suffix_z.to(dev, dtype=torch.float32)  # (Ls, V)\n",
    "        Ls, V_logits = suffix_logits.shape\n",
    "        assert V_logits == V, f\"suffix_z second dim ({V_logits}) must match vocab size ({V}).\"\n",
    "\n",
    "        # Soft one-hot over vocab\n",
    "        suffix_probs = F.softmax(suffix_logits, dim=-1)        # (Ls, V)\n",
    "\n",
    "        # Diagnostics: max vocab prob per suffix position\n",
    "        max_probs_per_pos, best_token_ids = suffix_probs.max(dim=-1)  # (Ls,), (Ls,)\n",
    "\n",
    "        if print_flag:\n",
    "            print(\"Per-position max vocab probabilities for suffix distributions:\")\n",
    "            print(f\"  min max-p:  {max_probs_per_pos.min().item():.6f}\")\n",
    "            print(f\"  max max-p:  {max_probs_per_pos.max().item():.6f}\")\n",
    "            print(f\"  mean max-p: {max_probs_per_pos.mean().item():.66f}\")\n",
    "\n",
    "        # Discrete suffix tokens from argmax\n",
    "        suffix_token_ids = best_token_ids.cpu()  # (Ls,)\n",
    "        suffix_tokens = tokenizer.convert_ids_to_tokens(suffix_token_ids.tolist())\n",
    "        suffix_text = tokenizer.decode(\n",
    "            suffix_token_ids.tolist(),\n",
    "            skip_special_tokens=False\n",
    "        )\n",
    "\n",
    "        if print_flag:\n",
    "            print(\"\\nProjected discrete suffix token IDs (argmax over soft one-hot):\", suffix_token_ids.tolist())\n",
    "            print(\"Projected discrete suffix tokens:\", suffix_tokens)\n",
    "            print(\"Projected suffix as text:\", repr(suffix_text))\n",
    "\n",
    "        # -----------------------------\n",
    "        # 2) Build full input: prompt + suffix (discrete ids)\n",
    "        # -----------------------------\n",
    "        if isinstance(prompt_input_ids, torch.Tensor):\n",
    "            prompt_ids = prompt_input_ids.to(dev).view(-1)\n",
    "        else:\n",
    "            prompt_ids = torch.tensor(prompt_input_ids, device=dev, dtype=torch.long)\n",
    "\n",
    "        prompt_len = prompt_ids.size(0)\n",
    "        suffix_ids_dev = suffix_token_ids.to(dev)\n",
    "\n",
    "        full_input_ids = torch.cat([prompt_ids, suffix_ids_dev], dim=0)  # (T + Ls,)\n",
    "        full_input_ids_batch = full_input_ids.unsqueeze(0)               # (1, T + Ls)\n",
    "\n",
    "        # For reporting:\n",
    "        prompt_text = tokenizer.decode(prompt_ids.tolist(), skip_special_tokens=False)\n",
    "        \n",
    "        if print_flag:\n",
    "            print(\"\\nPrompt text:\", repr(prompt_text))\n",
    "\n",
    "        # -----------------------------\n",
    "        # 3) Probabilities for suffix tokens given the prompt\n",
    "        #    p(s_i | prompt + s_<i)\n",
    "        # -----------------------------\n",
    "        outputs = model(input_ids=full_input_ids_batch)\n",
    "        logits = outputs.logits  # (1, L_total, V)\n",
    "        log_probs = logits.log_softmax(dim=-1)  # (1, L_total, V)\n",
    "\n",
    "        suffix_token_logprobs = []\n",
    "        \n",
    "        if print_flag:\n",
    "            print(\"\\nSuffix token probabilities given prompt:\")\n",
    "\n",
    "        # HuggingFace causal LM: logits[:, i, :] predict token at position i+1\n",
    "        for i, tok_id in enumerate(suffix_token_ids.tolist()):\n",
    "            # Position of this suffix token in the full sequence\n",
    "            pos = prompt_len + i  # index of token in full_input_ids\n",
    "\n",
    "            if pos == 0:\n",
    "                # Can't compute prob for very first token (no previous context)\n",
    "                lp = float(\"nan\")\n",
    "                p = float(\"nan\")\n",
    "            else:\n",
    "                lp_tensor = log_probs[0, pos - 1, tok_id]  # log p(token at pos)\n",
    "                lp = lp_tensor.item()\n",
    "                p = lp_tensor.exp().item()\n",
    "\n",
    "            suffix_token_logprobs.append(lp)\n",
    "            tok_str = suffix_tokens[i]\n",
    "            if print_flag:\n",
    "                print(\n",
    "                    f\"  suffix pos {i} (abs_pos={pos}, id={tok_id}, token={repr(tok_str)}): \"\n",
    "                    f\"p = {p:.6e}, log p = {lp:.6f}\"\n",
    "                )\n",
    "\n",
    "        suffix_token_logprobs = torch.tensor(suffix_token_logprobs, dtype=torch.float32)\n",
    "        suffix_joint_logprob = torch.nan_to_num(suffix_token_logprobs).sum().item()\n",
    "        \n",
    "        if print_flag:\n",
    "            print(f\"\\nJoint log-prob of suffix given prompt: {suffix_joint_logprob:.6f}\")\n",
    "\n",
    "        # -----------------------------\n",
    "        # 4) Auto-regressively generate num_gen_tokens more tokens\n",
    "        #    and record probabilities of each generated token.\n",
    "        # -----------------------------\n",
    "        current_input_ids = full_input_ids.clone()  # (T + Ls,)\n",
    "\n",
    "        generated_token_ids = []\n",
    "        generated_token_logprobs = []\n",
    "        generated_tokens_str = []\n",
    "\n",
    "        for step in range(num_gen_tokens):\n",
    "            inp_batch = current_input_ids.unsqueeze(0)  # (1, L_cur)\n",
    "            out = model(input_ids=inp_batch)\n",
    "            next_logits = out.logits[:, -1, :]          # (1, V)\n",
    "            next_log_probs = next_logits.log_softmax(dim=-1)  # (1, V)\n",
    "\n",
    "            # Greedy: pick argmax\n",
    "            next_log_prob_val, next_token_id = next_log_probs.squeeze(0).max(dim=-1)\n",
    "            next_id = next_token_id.item()\n",
    "            lp = next_log_prob_val.item()\n",
    "            p = next_log_prob_val.exp().item()\n",
    "\n",
    "            generated_token_ids.append(next_id)\n",
    "            generated_token_logprobs.append(lp)\n",
    "\n",
    "            tok_str = tokenizer.convert_ids_to_tokens([next_id])[0]\n",
    "            generated_tokens_str.append(tok_str)\n",
    "\n",
    "            # Append to context\n",
    "            current_input_ids = torch.cat(\n",
    "                [current_input_ids, next_token_id.unsqueeze(0)],\n",
    "                dim=0\n",
    "            )\n",
    "            \n",
    "            if print_flag:\n",
    "                print(\n",
    "                    f\"Generated token {step} (abs_pos={current_input_ids.size(0)-1}, \"\n",
    "                    f\"id={next_id}, token={repr(tok_str)}): p = {p:.6e}, log p = {lp:.6f}\"\n",
    "                )\n",
    "\n",
    "        generated_token_ids = torch.tensor(generated_token_ids, dtype=torch.long)\n",
    "        generated_token_logprobs = torch.tensor(generated_token_logprobs, dtype=torch.float32)\n",
    "        gen_joint_logprob = generated_token_logprobs.sum().item()\n",
    "\n",
    "        if print_flag:\n",
    "            print(f\"\\nJoint log-prob of generated tokens: {gen_joint_logprob:.6f}\")\n",
    "\n",
    "        # -----------------------------\n",
    "        # 5) Decode full text: prompt + suffix + generated\n",
    "        # -----------------------------\n",
    "        full_with_gen_ids = current_input_ids  # (T + Ls + num_gen_tokens,)\n",
    "        full_text = tokenizer.decode(full_with_gen_ids.tolist(), skip_special_tokens=False)\n",
    "        gen_text = tokenizer.decode(generated_token_ids.tolist(), skip_special_tokens=False)\n",
    "\n",
    "        if print_flag:\n",
    "            print(\"\\nGenerated continuation text:\", repr(gen_text))\n",
    "            print(\"\\nFull text (prompt + suffix + generated):\")\n",
    "            print(repr(full_text))\n",
    "\n",
    "        # -----------------------------\n",
    "        # 6) Return structured info\n",
    "        # -----------------------------\n",
    "        result = {\n",
    "            \"suffix_token_ids\": suffix_token_ids,                     # (Ls,) on CPU\n",
    "            \"suffix_tokens\": suffix_tokens,\n",
    "            \"suffix_token_logprobs\": suffix_token_logprobs,           # (Ls,)\n",
    "            \"suffix_joint_logprob\": suffix_joint_logprob,\n",
    "            \"generated_token_ids\": generated_token_ids,               # (num_gen_tokens,)\n",
    "            \"generated_tokens\": generated_tokens_str,\n",
    "            \"generated_token_logprobs\": generated_token_logprobs,     # (num_gen_tokens,)\n",
    "            \"generated_joint_logprob\": gen_joint_logprob,\n",
    "            \"prompt_input_ids\": prompt_ids.cpu(),\n",
    "            \"full_input_ids_with_suffix\": full_input_ids.cpu(),\n",
    "            \"full_input_ids_with_suffix_and_generated\": full_with_gen_ids.cpu(),\n",
    "            \"prompt_text\": prompt_text,\n",
    "            \"suffix_text\": suffix_text,\n",
    "            \"generated_text\": gen_text,\n",
    "            \"full_text\": full_text,\n",
    "            \"suffix_max_probs_per_pos\": max_probs_per_pos.cpu(),      # (Ls,)\n",
    "        }\n",
    "\n",
    "    # ---- GPU cleanup (only dev tensors, keep CPU copies in `result`) ----\n",
    "    for p in model.parameters():\n",
    "        p.grad = None\n",
    "\n",
    "    try:\n",
    "        # suffix-related\n",
    "        del E, suffix_logits, suffix_probs, best_token_ids, max_probs_per_pos\n",
    "\n",
    "        # sequence tensors on dev (CPU clones are in `result`)\n",
    "        del prompt_ids, suffix_ids_dev, full_input_ids, full_input_ids_batch\n",
    "        del current_input_ids, full_with_gen_ids\n",
    "\n",
    "        # forward-pass outputs on dev\n",
    "        del outputs, logits, log_probs\n",
    "        del out, inp_batch, next_logits, next_log_probs\n",
    "        del next_token_id, next_log_prob_val\n",
    "    except NameError:\n",
    "        pass\n",
    "\n",
    "    if dev.type == \"cuda\":\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f69993",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T14:04:15.124058Z",
     "iopub.status.busy": "2025-11-20T14:04:15.123891Z",
     "iopub.status.idle": "2025-11-20T14:04:15.135689Z",
     "shell.execute_reply": "2025-11-20T14:04:15.134941Z",
     "shell.execute_reply.started": "2025-11-20T14:04:15.124044Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def read_suffix_pt(filepath: str) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Read suffix embeddings from a .pt file.\n",
    "    \"\"\"\n",
    "    suffix_z = torch.load(filepath)\n",
    "    return suffix_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be447ee5-5706-40bd-9f93-1c9fade94c57",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T14:08:14.086171Z",
     "iopub.status.busy": "2025-11-20T14:08:14.085302Z",
     "iopub.status.idle": "2025-11-20T14:08:14.098286Z",
     "shell.execute_reply": "2025-11-20T14:08:14.097374Z",
     "shell.execute_reply.started": "2025-11-20T14:08:14.086143Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import amp\n",
    "\n",
    "def entropy_loss(batch_logits, is_logit=True):\n",
    "    \"\"\"\n",
    "    batch_logits: can be (n, V) or (B, n, V) \n",
    "    is_logit: whether input is logits (True) or probabilities (False)\n",
    "    \n",
    "    Returns: scalar = mean(min(entropy across n) across B)\n",
    "    \"\"\"\n",
    "    original_dim = batch_logits.dim()\n",
    "    if original_dim == 2:\n",
    "        batch_logits = batch_logits.unsqueeze(0)\n",
    "\n",
    "    if is_logit:\n",
    "        log_probs = F.log_softmax(batch_logits, dim=-1)\n",
    "    else:\n",
    "        log_probs = torch.log(batch_logits + 1e-12)\n",
    "\n",
    "    probs = log_probs.exp()\n",
    "    entropy = -(probs * log_probs).sum(dim=-1)  # (B, n)\n",
    "    min_entropy = entropy.min(dim=-1).values    # (B,)\n",
    "    mean_min_entropy = min_entropy.mean()\n",
    "    \n",
    "    return mean_min_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5199013",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import amp\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "def compute_loss_for_suffix(\n",
    "    model,\n",
    "    emb_layer,\n",
    "    batch,\n",
    "    suffix_z,           # (Ls, V) nn.Parameter  == pre-softmax logits over vocab per suffix position\n",
    "    n_tokens=10,\n",
    "    nt=1,\n",
    "    amp_dtype=torch.float16,\n",
    "    cos_reg_weight=0.1,  # used as weight for soft one-hot entropy regularizer\n",
    "    E_norm_cpu=None,     # unused now, kept for API compatibility\n",
    "    chunk_size=512,\n",
    "    ent_reg_weight=1.0,\n",
    "):\n",
    "    \"\"\"\n",
    "    - suffix_z is pre-softmax logits over vocab: (Ls, V).\n",
    "    - Convert suffix_z -> soft one-hot (suffix_probs) -> suffix embeddings via suffix_probs @ E.\n",
    "    - For each example, build [prompt][suffix_embs] in embedding space.\n",
    "    - Pad all to same length -> [prompt][suffix][PAD].\n",
    "    - Roll out n_tokens-1 tokens under inference_mode.\n",
    "    - Final forward WITH grad gives entropy loss on last n_tokens generated tokens.\n",
    "    - PLUS: entropy regularizer on the soft one-hot (suffix_probs) to push it toward one-hot.\n",
    "\n",
    "    Additionally:\n",
    "    - Returns a rich `characteristics` dict that contains:\n",
    "      * full tensors (on CPU) for suffix logits/probs/embs\n",
    "      * per-position entropy, max prob, norms, margins, nearest-token distances\n",
    "      * full singular value vectors and eigenvalue spectra\n",
    "      * output logits/prob features from the final step\n",
    "    \"\"\"\n",
    "    prompts = batch[\"input_ids\"]   # list of 1D LongTensors (Li,)\n",
    "    dev = emb_layer.weight.device\n",
    "    emb_dtype = emb_layer.weight.dtype\n",
    "\n",
    "    # ---------- Soft one-hot over vocab and suffix embeddings ----------\n",
    "    # suffix_z: (Ls, V) logits over vocab\n",
    "    suffix_logits = suffix_z.to(device=dev, dtype=torch.float32)  # keep logits in fp32 for stability\n",
    "    Ls, V_logits = suffix_logits.shape\n",
    "\n",
    "    E = emb_layer.weight  # (V, E_dim)\n",
    "    V, E_dim = E.shape\n",
    "    assert V_logits == V, f\"suffix_z second dim ({V_logits}) must match vocab size ({V}).\"\n",
    "\n",
    "    # Soft one-hot over vocab (fp32)\n",
    "    suffix_probs_fp32 = F.softmax(suffix_logits, dim=-1)  # (Ls, V)\n",
    "\n",
    "    # Suffix embeddings: convex combination of token embeddings\n",
    "    suffix_probs = suffix_probs_fp32.to(dtype=emb_dtype, device=dev)\n",
    "    suffix_embs = suffix_probs @ E  # (Ls, E_dim), with grad wrt suffix_logits\n",
    "\n",
    "    # ---------- Build per-example [prompt][suffix] in embedding space ----------\n",
    "    B = len(prompts)\n",
    "    base_embs = []   # each: (Li+Ls, E_dim)\n",
    "    base_lens = []   # each: scalar length Li+Ls\n",
    "\n",
    "    for p_ids in prompts:\n",
    "        p_ids_dev = p_ids.to(dev)\n",
    "        p_emb = emb_layer(p_ids_dev).detach()      # (Li, E_dim), prompts are constants\n",
    "        base = torch.cat([p_emb, suffix_embs], dim=0)  # (Li+Ls, E_dim)\n",
    "        base_embs.append(base)\n",
    "        base_lens.append(base.size(0))\n",
    "\n",
    "    # Pad to [prompt][suffix][PAD...] across the batch\n",
    "    base = pad_sequence(base_embs, batch_first=True)   # (B, max_len, E_dim)\n",
    "    base_lens = torch.tensor(base_lens, device=dev)    # (B,)\n",
    "    max_len = base.size(1)\n",
    "\n",
    "    # Attention mask: 1 for real tokens, 0 for pad\n",
    "    arange = torch.arange(max_len, device=dev).unsqueeze(0)  # (1, max_len)\n",
    "    base_mask = (arange < base_lens.unsqueeze(1)).long()     # (B, max_len)\n",
    "\n",
    "    # Now base has structure [prompt][suffix][PAD] per row (masked pads)\n",
    "\n",
    "    def _one_step_logits(e, m, n=1):\n",
    "        with amp.autocast(\"cuda\", dtype=amp_dtype):\n",
    "            out = model(\n",
    "                inputs_embeds=e,\n",
    "                attention_mask=m,\n",
    "                use_cache=False,\n",
    "                output_attentions=False,\n",
    "                output_hidden_states=False,\n",
    "                return_dict=True,\n",
    "            )\n",
    "        if n == 1:\n",
    "            return out.logits[:, -1, :]          # (B, V)\n",
    "        else:\n",
    "            return out.logits[:, -n:, :]         # (B, n, V)\n",
    "\n",
    "    # ---------- Rollout under no grad (from detached base) ----------\n",
    "    work_e = base.detach()  # rollout uses constants\n",
    "    work_m = base_mask\n",
    "    added_embs = []         # list of (B, E_dim) constants\n",
    "\n",
    "    T = max(0, n_tokens - 1)\n",
    "    with torch.inference_mode():\n",
    "        for _ in range(T):\n",
    "            logits_t = _one_step_logits(work_e, work_m, n=1)   # (B, V)\n",
    "            probs_t = torch.softmax(logits_t, dim=-1)          # (B, V)\n",
    "            next_ids = torch.argmax(probs_t, dim=-1)           # (B,)\n",
    "\n",
    "            next_emb = emb_layer(next_ids.to(dev)).detach()    # (B, E_dim)\n",
    "            added_embs.append(next_emb)\n",
    "\n",
    "            work_e = torch.cat([work_e, next_emb.unsqueeze(1)], dim=1)\n",
    "            work_m = torch.cat(\n",
    "                [work_m, torch.ones((B, 1), dtype=work_m.dtype, device=dev)],\n",
    "                dim=1,\n",
    "            )\n",
    "\n",
    "        # rollout temporaries\n",
    "        del logits_t, probs_t, next_ids, next_emb\n",
    "\n",
    "    # ---------- Final inputs: [prompt][suffix][PAD] + generated tokens ----------\n",
    "    if len(added_embs) > 0:\n",
    "        added = torch.stack(added_embs, dim=1)              # (B, T, E_dim)\n",
    "        del added_embs, work_e, work_m\n",
    "\n",
    "        final_emb = torch.cat([base, added], dim=1)         # (B, max_len+T, E_dim)\n",
    "        gen_mask = torch.ones((B, T), dtype=base_mask.dtype, device=dev)\n",
    "        final_mask = torch.cat([base_mask, gen_mask], dim=1)\n",
    "    else:\n",
    "        final_emb = base\n",
    "        final_mask = base_mask\n",
    "\n",
    "    # ---------- Final step WITH grad (depends on suffix_logits via suffix_embs) ----------\n",
    "    logits_last = _one_step_logits(final_emb, final_mask, n=n_tokens)  # (B, n_tokens, V)\n",
    "    ent = entropy_loss(logits_last, is_logit=True)                     # main term: minimize entropy\n",
    "\n",
    "    # ---------- Soft one-hot entropy regularizer ----------\n",
    "    # Entropy over suffix_probs (per suffix position), minimize to encourage near-one-hot\n",
    "    ent_soft = entropy_loss(suffix_probs_fp32, is_logit=False)\n",
    "\n",
    "    dev = suffix_embs.device\n",
    "    Ls = suffix_embs.size(0)\n",
    "    V = E_norm_cpu.size(0)\n",
    "    \n",
    "    # normalized suffix embeddings on GPU, fp32, with grad\n",
    "    z_norm = F.normalize(suffix_embs.float(), dim=-1)  # (Ls, E)\n",
    "    \n",
    "    # running top-2 cosines across vocab, per suffix position\n",
    "    top2_vals = None  # (2, Ls)\n",
    "    \n",
    "    for start in range(0, V, chunk_size):\n",
    "        end = min(start + chunk_size, V)\n",
    "        # chunk: (c, E) fp32 on GPU, no grad\n",
    "        chunk = E_norm_cpu[start:end].to(dev, non_blocking=True)  # (c, E)\n",
    "    \n",
    "        # (c, E) @ (E, Ls) -> (c, Ls)\n",
    "        chunk_sim = torch.matmul(chunk, z_norm.T)  # (c, Ls)\n",
    "    \n",
    "        # top-2 within this chunk: (2, Ls)  (if c < 2, topk handles it)\n",
    "        chunk_top2, _ = chunk_sim.topk(\n",
    "            k=min(2, chunk_sim.size(0)), dim=0\n",
    "        )  # (k', Ls)\n",
    "    \n",
    "        if top2_vals is None:\n",
    "            # if first chunk smaller than 2, pad with very low values\n",
    "            if chunk_top2.size(0) < 2:\n",
    "                pad_rows = 2 - chunk_top2.size(0)\n",
    "                pad = torch.full(\n",
    "                    (pad_rows, Ls),\n",
    "                    -1e9,\n",
    "                    device=dev,\n",
    "                    dtype=chunk_top2.dtype,\n",
    "                )\n",
    "                top2_vals = torch.cat([chunk_top2, pad], dim=0)  # (2, Ls)\n",
    "            else:\n",
    "                top2_vals = chunk_top2  # (2, Ls)\n",
    "        else:\n",
    "            # combine global and chunk top2, then keep best 2 across all\n",
    "            combined = torch.cat([top2_vals, chunk_top2], dim=0)  # (2 + k', Ls)\n",
    "            top2_vals, _ = combined.topk(k=2, dim=0)              # (2, Ls)\n",
    "    \n",
    "        # free small temps\n",
    "        del chunk, chunk_sim, chunk_top2\n",
    "    \n",
    "    # Now top2_vals[0] = global best cosine; top2_vals[1] = global second-best\n",
    "    top1 = top2_vals[0]   # (Ls,)\n",
    "    top2 = top2_vals[1]   # (Ls,)\n",
    "    \n",
    "    # margin per position: encourage top1 >> top2\n",
    "    margin_per_pos = top1 - top2  # (Ls,)\n",
    "    mean_margin = margin_per_pos.mean()            # scalar\n",
    "    \n",
    "    # We want to MAXIMIZE mean_margin → in minimization, use negative margin\n",
    "    margin_reg = -mean_margin\n",
    "\n",
    "    # total loss: model entropy + weighted soft one-hot entropy\n",
    "    total_loss = ent + ent_reg_weight * ent_soft + cos_reg_weight * margin_reg\n",
    "\n",
    "    # ----- Suffix joint log-prob term (discrete argmax-based) -----\n",
    "    # We'll treat this as an extra constant term (no gradient wrt suffix_z)\n",
    "    base_loss = total_loss\n",
    "    dev = emb_layer.weight.device\n",
    "\n",
    "    # defaults in case batch is empty\n",
    "    suffix_joint_logprob = torch.tensor(0.0, device=dev)\n",
    "    suffix_logprob_term = torch.tensor(0.0, device=dev)\n",
    "    model_suffix_probs_cpu = None  # NEW: will hold model-assigned suffix probs (Ls, V) on CPU\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Discrete suffix tokens from current soft one-hot\n",
    "        suffix_token_ids = suffix_probs_fp32.argmax(dim=-1)  # (Ls,)\n",
    "\n",
    "        pad_id = getattr(tokenizer, \"pad_token_id\", 0)\n",
    "        # print(f'pad_id{pad_id}')\n",
    "\n",
    "        full_ids_list = []\n",
    "        prompt_lens = []\n",
    "        for p_ids in prompts:\n",
    "            p_ids_dev = p_ids.to(dev)\n",
    "            prompt_lens.append(p_ids_dev.size(0))\n",
    "            full_ids_list.append(torch.cat([p_ids_dev, suffix_token_ids.to(dev)], dim=0))\n",
    "\n",
    "        if len(full_ids_list) > 0:\n",
    "            full_ids = pad_sequence(\n",
    "                full_ids_list,\n",
    "                batch_first=True,\n",
    "                padding_value=pad_id,\n",
    "            ).to(dev)                                             # (B_ids, L_max_ids)\n",
    "            prompt_lens = torch.tensor(prompt_lens, device=dev)  # (B_ids,)\n",
    "            B_ids, L_max_ids = full_ids.size()\n",
    "\n",
    "            # attention mask for [prompt][suffix] region\n",
    "            full_lengths = prompt_lens + Ls\n",
    "            arange_ids = torch.arange(L_max_ids, device=dev).unsqueeze(0)\n",
    "            attn_ids = (arange_ids < full_lengths.unsqueeze(1)).long()\n",
    "\n",
    "            from torch import amp as _amp_mod  # to avoid ambiguity if needed\n",
    "\n",
    "            with _amp_mod.autocast(\"cuda\", dtype=amp_dtype):\n",
    "                outputs_suffix = model(\n",
    "                    input_ids=full_ids,\n",
    "                    attention_mask=attn_ids,\n",
    "                    use_cache=False,\n",
    "                    output_attentions=False,\n",
    "                    output_hidden_states=False,\n",
    "                    return_dict=True,\n",
    "                )\n",
    "\n",
    "            logits_suffix = outputs_suffix.logits                 # (B_ids, L_max_ids, V)\n",
    "            log_probs_suffix = F.log_softmax(logits_suffix, dim=-1)\n",
    "\n",
    "            # positions of suffix tokens in full_ids and corresponding logits\n",
    "            positions = prompt_lens.unsqueeze(1) + torch.arange(Ls, device=dev).unsqueeze(0)  # (B_ids, Ls)\n",
    "            positions_logits = (positions - 1).clamp(min=0, max=L_max_ids - 1)                # (B_ids, Ls)\n",
    "\n",
    "            token_ids_expand = suffix_token_ids.unsqueeze(0).expand(B_ids, -1)                # (B_ids, Ls)\n",
    "\n",
    "            # FULL model-assigned suffix distributions over vocab at each suffix position\n",
    "            # pick the distribution at the timestep where each suffix token is predicted\n",
    "            V = logits_suffix.size(-1)\n",
    "            model_suffix_log_probs = log_probs_suffix.gather(\n",
    "                dim=1,\n",
    "                index=positions_logits.unsqueeze(-1).expand(-1, -1, V),\n",
    "            )  # (B_ids, Ls, V)\n",
    "\n",
    "            model_suffix_probs = model_suffix_log_probs.exp()        # (B_ids, Ls, V)\n",
    "            # average across batch -> (Ls, V), one distribution per suffix position\n",
    "            model_suffix_probs_mean = model_suffix_probs.mean(dim=0)  # (Ls, V)\n",
    "            # stash on CPU for later feature extraction\n",
    "            model_suffix_probs_cpu = model_suffix_probs_mean.detach().cpu()\n",
    "\n",
    "            # log p(s_j | prompt + s_<j) for each batch, each suffix position\n",
    "            suffix_token_logprobs = log_probs_suffix[\n",
    "                torch.arange(B_ids, device=dev).unsqueeze(1),\n",
    "                positions_logits,\n",
    "                token_ids_expand,\n",
    "            ]  # (B_ids, Ls)\n",
    "\n",
    "\n",
    "            # schedule weights along suffix positions\n",
    "            nt_sched = min(Ls, nt)          # \"nt\": first nt suffix positions\n",
    "            weights = torch.ones(Ls, device=dev)  # default weight = 1.0\n",
    "            if nt_sched > 0:\n",
    "                # lower values at the start, gradually increasing up to 1.0\n",
    "                weight_schedule = torch.linspace(0.3, 1.0, steps=nt_sched, device=dev)\n",
    "                weights[:nt_sched] = weight_schedule\n",
    "\n",
    "            # apply weights and average over batch\n",
    "            weighted_logprobs = (suffix_token_logprobs * weights.unsqueeze(0)).sum(dim=-1)  # (B_ids,)\n",
    "            suffix_joint_logprob = weighted_logprobs.mean()                                 # scalar\n",
    "\n",
    "            # negative-signed multiplier: encourages higher joint suffix log-prob\n",
    "            suffix_logprob_weight = 0.01\n",
    "            suffix_logprob_term = -suffix_logprob_weight * suffix_joint_logprob\n",
    "\n",
    "    # include the suffix joint log-prob term in the loss\n",
    "    total_loss = base_loss + suffix_logprob_term\n",
    "    \n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1339af0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e586bef800646999b0c611e85f7772d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d00a3ed7b82f4074be71c6dcceff8e0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00001-a09b74b3ef9c3b(…):   0%|          | 0.00/24.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7550d4a246a466d83b0235069d82241",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/52002 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from datasets import load_dataset\n",
    "\n",
    "def load_prompts_unpadded(tokenizer, args):\n",
    "    \"\"\"\n",
    "    Returns DataLoader where each batch is:\n",
    "      {\n",
    "        \"input_ids\": list of 1D LongTensors (prompts, no padding),\n",
    "        \"prompt_lens\": LongTensor (B,)\n",
    "      }\n",
    "    \"\"\"\n",
    "    ds = load_dataset(\"tatsu-lab/alpaca\", split=\"train\", cache_dir=args[\"data_dir\"])\n",
    "\n",
    "    # Subsample for speed\n",
    "    if \"sample_size\" in args and args[\"sample_size\"] is not None and args[\"sample_size\"] < len(ds):\n",
    "        ds = ds.shuffle(seed=42).select(range(args[\"sample_size\"]))\n",
    "\n",
    "    def collate(batch):\n",
    "        texts = [ex[\"instruction\"] for ex in batch]\n",
    "        enc = tokenizer(\n",
    "            texts,\n",
    "            padding=False,\n",
    "            truncation=True,\n",
    "            max_length=args[\"max_length\"],\n",
    "        )\n",
    "        prompts = [torch.tensor(ids, dtype=torch.long) for ids in enc[\"input_ids\"]]\n",
    "        prompt_lens = [len(p) for p in prompts]\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": prompts,  # list of (Li,)\n",
    "            \"prompt_lens\": torch.tensor(prompt_lens, dtype=torch.long),\n",
    "        }\n",
    "\n",
    "    num_workers = max(2, os.cpu_count() // 2)\n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size=args[\"batch_size\"],\n",
    "        shuffle=True,\n",
    "        pin_memory=True,\n",
    "        num_workers=num_workers,\n",
    "        persistent_workers=True,\n",
    "        collate_fn=collate,\n",
    "    )\n",
    "\n",
    "dataloader_args = {\n",
    "    \"data_dir\": \"/kaggle/working/datasets\",\n",
    "    \"batch_size\": 1,\n",
    "    \"max_length\": 128,\n",
    "    \"sample_size\": 128,\n",
    "}\n",
    "\n",
    "dataloader = load_prompts_unpadded(tokenizer, dataloader_args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53d6012",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_suffix(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt_ids: torch.Tensor,        # (L_prompt,)\n",
    "    suffix_token_ids: torch.Tensor,  # (Ls,)\n",
    "    max_new_tokens: int = 50,\n",
    "):\n",
    "    \"\"\"\n",
    "    Concatenate [prompt][suffix] and let the model generate continuation.\n",
    "    \"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    full_input_ids = torch.cat(\n",
    "        [prompt_ids, suffix_token_ids.to(prompt_ids.device)],\n",
    "        dim=0\n",
    "    ).unsqueeze(0).to(device)  # (1, L_total)\n",
    "\n",
    "    attn_mask = torch.ones_like(full_input_ids, dtype=torch.long, device=device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        generated = model.generate(\n",
    "            input_ids=full_input_ids,\n",
    "            attention_mask=attn_mask,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    return tokenizer.decode(generated[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c65bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def log_suffix_token_probs(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt_ids: torch.Tensor,        # (L_prompt,)\n",
    "    suffix_token_ids: torch.Tensor,  # (Ls,)\n",
    "):\n",
    "    \"\"\"\n",
    "    For each position k in the suffix, compute P(suffix[k] | prompt + suffix[:k])\n",
    "    and print it.\n",
    "    \"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    model.eval()\n",
    "\n",
    "    prompt_ids = prompt_ids.to(device)\n",
    "    suffix_token_ids = suffix_token_ids.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for k in range(suffix_token_ids.size(0)):\n",
    "            ctx_suffix = suffix_token_ids[:k]      # (k,)\n",
    "            ctx = torch.cat([prompt_ids, ctx_suffix], dim=0)  # (L_prompt + k,)\n",
    "\n",
    "            inp = ctx.unsqueeze(0)  # (1, L_ctx)\n",
    "            msk = torch.ones_like(inp, dtype=torch.long, device=device)\n",
    "\n",
    "            out = model(\n",
    "                input_ids=inp,\n",
    "                attention_mask=msk,\n",
    "                use_cache=False,\n",
    "                return_dict=True,\n",
    "            )\n",
    "            logits_next = out.logits[:, -1, :]  # (1, V)\n",
    "            probs_next = F.softmax(logits_next, dim=-1)  # (1, V)\n",
    "\n",
    "            tok_id = suffix_token_ids[k]\n",
    "            prob = probs_next[0, tok_id].item()\n",
    "            tok_str = tokenizer.decode([tok_id])\n",
    "\n",
    "            print(f\"  pos {k:2d}, token {tok_id:5d} ({repr(tok_str)}), prob: {prob:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc50bc60",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T14:16:59.120215Z",
     "iopub.status.busy": "2025-11-20T14:16:59.119578Z",
     "iopub.status.idle": "2025-11-20T14:17:06.411777Z",
     "shell.execute_reply": "2025-11-20T14:17:06.410871Z",
     "shell.execute_reply.started": "2025-11-20T14:16:59.120186Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using this prompt for qualitative checks:\n",
      "   List three sites of historical importance in London.\n",
      "\n",
      "\n",
      "=== round 0, epoch 0 ===\n",
      "Read /kaggle/working/hotflip/rounds/suffix_r0_e0.pt successfully. shape=(10, 4096)\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  3.787844\n",
      "  max:  4.888620\n",
      "  mean: 4.140403\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.130694\n",
      "  max:  0.297501\n",
      "  mean: 0.202913\n",
      "\n",
      "Projected discrete suffix token IDs: [20609, 1642, 7713, 10056, 10805, 29169, 3798, 1858, 7236, 25775]\n",
      "Projected discrete suffix tokens: ['▁Portály', '\".', 'Pol', '▁Consider', '▁causing', '²).', 'oreferrer', '▁Pl', '▁Fort', '▁Ari']\n",
      "Projected suffix as text: 'Portály\".Pol Consider causing²).oreferrer Pl Fort Ari'\n",
      "suffix loss (before projection): 4.947422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suffix loss (after  projection): 5.449004\n",
      "\n",
      "Generated text with suffix (projected):\n",
      "List three sites of historical importance in London. Portály\".Pol Consider causing²).oreferrer Pl Fort Ari. 1. The Tower of London 2. The Houses of Parliament 3. Buckingham Palace The Tower of London is a castle built by William the Conqueror in 1066. It was built to protect London from invaders. It was also used to keep prisoners. The Houses\n",
      "\n",
      "Per-suffix-token next-token probabilities:\n",
      "  pos  0, token 20609 ('Portály'), prob: 0.000000\n",
      "  pos  1, token  1642 ('\".'), prob: 0.000004\n",
      "  pos  2, token  7713 ('Pol'), prob: 0.000015\n",
      "  pos  3, token 10056 ('Consider'), prob: 0.000000\n",
      "  pos  4, token 10805 ('causing'), prob: 0.000001\n",
      "  pos  5, token 29169 ('²).'), prob: 0.000000\n",
      "  pos  6, token  3798 ('oreferrer'), prob: 0.000000\n",
      "  pos  7, token  1858 ('Pl'), prob: 0.000015\n",
      "  pos  8, token  7236 ('Fort'), prob: 0.000002\n",
      "  pos  9, token 25775 ('Ari'), prob: 0.000004\n",
      "####################\n",
      "\n",
      "=== round 0, epoch 1 ===\n",
      "Read /kaggle/working/hotflip/rounds/suffix_r0_e1.pt successfully. shape=(10, 4096)\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  3.628708\n",
      "  max:  4.806677\n",
      "  mean: 4.058847\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.205725\n",
      "  max:  0.452003\n",
      "  mean: 0.311015\n",
      "\n",
      "Projected discrete suffix token IDs: [20609, 1642, 9802, 10056, 10805, 29169, 3798, 19095, 7236, 25775]\n",
      "Projected discrete suffix tokens: ['▁Portály', '\".', 'Mark', '▁Consider', '▁causing', '²).', 'oreferrer', 'Mag', '▁Fort', '▁Ari']\n",
      "Projected suffix as text: 'Portály\".Mark Consider causing²).oreferrerMag Fort Ari'\n",
      "suffix loss (before projection): 5.140125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suffix loss (after  projection): 5.504870\n",
      "\n",
      "Generated text with suffix (projected):\n",
      "List three sites of historical importance in London. Portály\".Mark Consider causing²).oreferrerMag Fort Ariane. 1. The Tower of London. 2. The Houses of Parliament. 3. The British Museum. 4. The British Library. 5. The National Gallery. 6. The National Portrait Gallery. 7. The Royal Opera House. 8. The Royal Academy of\n",
      "\n",
      "Per-suffix-token next-token probabilities:\n",
      "  pos  0, token 20609 ('Portály'), prob: 0.000000\n",
      "  pos  1, token  1642 ('\".'), prob: 0.000004\n",
      "  pos  2, token  9802 ('Mark'), prob: 0.000019\n",
      "  pos  3, token 10056 ('Consider'), prob: 0.000001\n",
      "  pos  4, token 10805 ('causing'), prob: 0.000006\n",
      "  pos  5, token 29169 ('²).'), prob: 0.000000\n",
      "  pos  6, token  3798 ('oreferrer'), prob: 0.000000\n",
      "  pos  7, token 19095 ('Mag'), prob: 0.000001\n",
      "  pos  8, token  7236 ('Fort'), prob: 0.000017\n",
      "  pos  9, token 25775 ('Ari'), prob: 0.000005\n",
      "####################\n",
      "\n",
      "=== round 0, epoch 2 ===\n",
      "Read /kaggle/working/hotflip/rounds/suffix_r0_e2.pt successfully. shape=(10, 4096)\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  3.702060\n",
      "  max:  5.276039\n",
      "  mean: 4.269306\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.260999\n",
      "  max:  0.462586\n",
      "  mean: 0.366854\n",
      "\n",
      "Projected discrete suffix token IDs: [20609, 1642, 9802, 10056, 10805, 29169, 28354, 19095, 19618, 25775]\n",
      "Projected discrete suffix tokens: ['▁Portály', '\".', 'Mark', '▁Consider', '▁causing', '²).', '▁Расподела', 'Mag', '▁Beaut', '▁Ari']\n",
      "Projected suffix as text: 'Portály\".Mark Consider causing²). РасподелаMag Beaut Ari'\n",
      "suffix loss (before projection): 4.529020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suffix loss (after  projection): 5.193542\n",
      "\n",
      "Generated text with suffix (projected):\n",
      "List three sites of historical importance in London. Portály\".Mark Consider causing²). РасподелаMag Beaut Ari.\n",
      "\n",
      "The first two are obvious, but the third is a bit more obscure. It's the site of the Battle of Crécy, where the English army defeated the French in 1346. The battle was a turning point in the Hundred Years War, and the English victory was\n",
      "\n",
      "Per-suffix-token next-token probabilities:\n",
      "  pos  0, token 20609 ('Portály'), prob: 0.000000\n",
      "  pos  1, token  1642 ('\".'), prob: 0.000004\n",
      "  pos  2, token  9802 ('Mark'), prob: 0.000019\n",
      "  pos  3, token 10056 ('Consider'), prob: 0.000001\n",
      "  pos  4, token 10805 ('causing'), prob: 0.000006\n",
      "  pos  5, token 29169 ('²).'), prob: 0.000000\n",
      "  pos  6, token 28354 ('Расподела'), prob: 0.000000\n",
      "  pos  7, token 19095 ('Mag'), prob: 0.000001\n",
      "  pos  8, token 19618 ('Beaut'), prob: 0.000001\n",
      "  pos  9, token 25775 ('Ari'), prob: 0.000000\n",
      "####################\n",
      "\n",
      "=== round 0, epoch 3 ===\n",
      "Read /kaggle/working/hotflip/rounds/suffix_r0_e3.pt successfully. shape=(10, 4096)\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  3.670125\n",
      "  max:  5.200637\n",
      "  mean: 4.202552\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.317285\n",
      "  max:  0.506162\n",
      "  mean: 0.422951\n",
      "\n",
      "Projected discrete suffix token IDs: [20609, 1642, 9802, 10056, 10805, 29169, 28354, 19095, 19618, 25775]\n",
      "Projected discrete suffix tokens: ['▁Portály', '\".', 'Mark', '▁Consider', '▁causing', '²).', '▁Расподела', 'Mag', '▁Beaut', '▁Ari']\n",
      "Projected suffix as text: 'Portály\".Mark Consider causing²). РасподелаMag Beaut Ari'\n",
      "suffix loss (before projection): 4.584489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suffix loss (after  projection): 5.193542\n",
      "\n",
      "Generated text with suffix (projected):\n",
      "List three sites of historical importance in London. Portály\".Mark Consider causing²). РасподелаMag Beaut Ari.\n",
      "\n",
      "The first two are obvious, but the third is a bit more obscure. It's the site of the Battle of Crécy, where the English army defeated the French in 1346. The battle was a turning point in the Hundred Years War, and the English victory was\n",
      "\n",
      "Per-suffix-token next-token probabilities:\n",
      "  pos  0, token 20609 ('Portály'), prob: 0.000000\n",
      "  pos  1, token  1642 ('\".'), prob: 0.000004\n",
      "  pos  2, token  9802 ('Mark'), prob: 0.000019\n",
      "  pos  3, token 10056 ('Consider'), prob: 0.000001\n",
      "  pos  4, token 10805 ('causing'), prob: 0.000006\n",
      "  pos  5, token 29169 ('²).'), prob: 0.000000\n",
      "  pos  6, token 28354 ('Расподела'), prob: 0.000000\n",
      "  pos  7, token 19095 ('Mag'), prob: 0.000001\n",
      "  pos  8, token 19618 ('Beaut'), prob: 0.000001\n",
      "  pos  9, token 25775 ('Ari'), prob: 0.000000\n",
      "####################\n",
      "\n",
      "=== round 0, epoch 4 ===\n",
      "Read /kaggle/working/hotflip/rounds/suffix_r0_e4.pt successfully. shape=(10, 4096)\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  3.632905\n",
      "  max:  5.146614\n",
      "  mean: 4.116168\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.358892\n",
      "  max:  0.549129\n",
      "  mean: 0.456216\n",
      "\n",
      "Projected discrete suffix token IDs: [23923, 1642, 9802, 10056, 10805, 29169, 28354, 19095, 19618, 25775]\n",
      "Projected discrete suffix tokens: ['Fre', '\".', 'Mark', '▁Consider', '▁causing', '²).', '▁Расподела', 'Mag', '▁Beaut', '▁Ari']\n",
      "Projected suffix as text: 'Fre\".Mark Consider causing²). РасподелаMag Beaut Ari'\n",
      "suffix loss (before projection): 6.050853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suffix loss (after  projection): 4.968382\n",
      "\n",
      "Generated text with suffix (projected):\n",
      "List three sites of historical importance in London.Fre\".Mark Consider causing²). РасподелаMag Beaut Ari.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Per-suffix-token next-token probabilities:\n",
      "  pos  0, token 23923 ('Fre'), prob: 0.000000\n",
      "  pos  1, token  1642 ('\".'), prob: 0.000000\n",
      "  pos  2, token  9802 ('Mark'), prob: 0.000050\n",
      "  pos  3, token 10056 ('Consider'), prob: 0.000001\n",
      "  pos  4, token 10805 ('causing'), prob: 0.000003\n",
      "  pos  5, token 29169 ('²).'), prob: 0.000000\n",
      "  pos  6, token 28354 ('Расподела'), prob: 0.000000\n",
      "  pos  7, token 19095 ('Mag'), prob: 0.000001\n",
      "  pos  8, token 19618 ('Beaut'), prob: 0.000001\n",
      "  pos  9, token 25775 ('Ari'), prob: 0.000000\n",
      "####################\n",
      "\n",
      "=== round 0, epoch 5 ===\n",
      "Read /kaggle/working/hotflip/rounds/suffix_r0_e5.pt successfully. shape=(10, 4096)\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  3.588412\n",
      "  max:  5.255209\n",
      "  mean: 4.111514\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.380682\n",
      "  max:  0.577760\n",
      "  mean: 0.470007\n",
      "\n",
      "Projected discrete suffix token IDs: [23923, 1642, 9802, 10056, 10805, 29169, 28354, 19095, 19618, 25775]\n",
      "Projected discrete suffix tokens: ['Fre', '\".', 'Mark', '▁Consider', '▁causing', '²).', '▁Расподела', 'Mag', '▁Beaut', '▁Ari']\n",
      "Projected suffix as text: 'Fre\".Mark Consider causing²). РасподелаMag Beaut Ari'\n",
      "suffix loss (before projection): 4.413508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suffix loss (after  projection): 4.968382\n",
      "\n",
      "Generated text with suffix (projected):\n",
      "List three sites of historical importance in London.Fre\".Mark Consider causing²). РасподелаMag Beaut Ari.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Per-suffix-token next-token probabilities:\n",
      "  pos  0, token 23923 ('Fre'), prob: 0.000000\n",
      "  pos  1, token  1642 ('\".'), prob: 0.000000\n",
      "  pos  2, token  9802 ('Mark'), prob: 0.000050\n",
      "  pos  3, token 10056 ('Consider'), prob: 0.000001\n",
      "  pos  4, token 10805 ('causing'), prob: 0.000003\n",
      "  pos  5, token 29169 ('²).'), prob: 0.000000\n",
      "  pos  6, token 28354 ('Расподела'), prob: 0.000000\n",
      "  pos  7, token 19095 ('Mag'), prob: 0.000001\n",
      "  pos  8, token 19618 ('Beaut'), prob: 0.000001\n",
      "  pos  9, token 25775 ('Ari'), prob: 0.000000\n",
      "####################\n",
      "\n",
      "=== round 0, epoch 6 ===\n",
      "Read /kaggle/working/hotflip/rounds/suffix_r0_e6.pt successfully. shape=(10, 4096)\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  3.550333\n",
      "  max:  5.217088\n",
      "  mean: 4.070822\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.407447\n",
      "  max:  0.605035\n",
      "  mean: 0.488443\n",
      "\n",
      "Projected discrete suffix token IDs: [23923, 1642, 9802, 10056, 10805, 29169, 28354, 19095, 19618, 25775]\n",
      "Projected discrete suffix tokens: ['Fre', '\".', 'Mark', '▁Consider', '▁causing', '²).', '▁Расподела', 'Mag', '▁Beaut', '▁Ari']\n",
      "Projected suffix as text: 'Fre\".Mark Consider causing²). РасподелаMag Beaut Ari'\n",
      "suffix loss (before projection): 4.231536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suffix loss (after  projection): 4.968382\n",
      "\n",
      "Generated text with suffix (projected):\n",
      "List three sites of historical importance in London.Fre\".Mark Consider causing²). РасподелаMag Beaut Ari.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Per-suffix-token next-token probabilities:\n",
      "  pos  0, token 23923 ('Fre'), prob: 0.000000\n",
      "  pos  1, token  1642 ('\".'), prob: 0.000000\n",
      "  pos  2, token  9802 ('Mark'), prob: 0.000050\n",
      "  pos  3, token 10056 ('Consider'), prob: 0.000001\n",
      "  pos  4, token 10805 ('causing'), prob: 0.000003\n",
      "  pos  5, token 29169 ('²).'), prob: 0.000000\n",
      "  pos  6, token 28354 ('Расподела'), prob: 0.000000\n",
      "  pos  7, token 19095 ('Mag'), prob: 0.000001\n",
      "  pos  8, token 19618 ('Beaut'), prob: 0.000001\n",
      "  pos  9, token 25775 ('Ari'), prob: 0.000000\n",
      "####################\n",
      "\n",
      "=== round 0, epoch 7 ===\n",
      "Read /kaggle/working/hotflip/rounds/suffix_r0_e7.pt successfully. shape=(10, 4096)\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  3.513973\n",
      "  max:  5.198439\n",
      "  mean: 4.037545\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.424276\n",
      "  max:  0.623114\n",
      "  mean: 0.498608\n",
      "\n",
      "Projected discrete suffix token IDs: [23923, 1642, 9802, 10056, 10805, 29169, 28354, 19095, 19618, 25775]\n",
      "Projected discrete suffix tokens: ['Fre', '\".', 'Mark', '▁Consider', '▁causing', '²).', '▁Расподела', 'Mag', '▁Beaut', '▁Ari']\n",
      "Projected suffix as text: 'Fre\".Mark Consider causing²). РасподелаMag Beaut Ari'\n",
      "suffix loss (before projection): 6.083911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suffix loss (after  projection): 4.968382\n",
      "\n",
      "Generated text with suffix (projected):\n",
      "List three sites of historical importance in London.Fre\".Mark Consider causing²). РасподелаMag Beaut Ari.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Per-suffix-token next-token probabilities:\n",
      "  pos  0, token 23923 ('Fre'), prob: 0.000000\n",
      "  pos  1, token  1642 ('\".'), prob: 0.000000\n",
      "  pos  2, token  9802 ('Mark'), prob: 0.000050\n",
      "  pos  3, token 10056 ('Consider'), prob: 0.000001\n",
      "  pos  4, token 10805 ('causing'), prob: 0.000003\n",
      "  pos  5, token 29169 ('²).'), prob: 0.000000\n",
      "  pos  6, token 28354 ('Расподела'), prob: 0.000000\n",
      "  pos  7, token 19095 ('Mag'), prob: 0.000001\n",
      "  pos  8, token 19618 ('Beaut'), prob: 0.000001\n",
      "  pos  9, token 25775 ('Ari'), prob: 0.000000\n",
      "####################\n",
      "\n",
      "=== round 0, epoch 8 ===\n",
      "Read /kaggle/working/hotflip/rounds/suffix_r0_e8.pt successfully. shape=(10, 4096)\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  3.484671\n",
      "  max:  5.193452\n",
      "  mean: 4.015231\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.435689\n",
      "  max:  0.635103\n",
      "  mean: 0.505144\n",
      "\n",
      "Projected discrete suffix token IDs: [23923, 1642, 9802, 10056, 10805, 29169, 28354, 19095, 19618, 25775]\n",
      "Projected discrete suffix tokens: ['Fre', '\".', 'Mark', '▁Consider', '▁causing', '²).', '▁Расподела', 'Mag', '▁Beaut', '▁Ari']\n",
      "Projected suffix as text: 'Fre\".Mark Consider causing²). РасподелаMag Beaut Ari'\n",
      "suffix loss (before projection): 6.093708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suffix loss (after  projection): 4.968382\n",
      "\n",
      "Generated text with suffix (projected):\n",
      "List three sites of historical importance in London.Fre\".Mark Consider causing²). РасподелаMag Beaut Ari.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Per-suffix-token next-token probabilities:\n",
      "  pos  0, token 23923 ('Fre'), prob: 0.000000\n",
      "  pos  1, token  1642 ('\".'), prob: 0.000000\n",
      "  pos  2, token  9802 ('Mark'), prob: 0.000050\n",
      "  pos  3, token 10056 ('Consider'), prob: 0.000001\n",
      "  pos  4, token 10805 ('causing'), prob: 0.000003\n",
      "  pos  5, token 29169 ('²).'), prob: 0.000000\n",
      "  pos  6, token 28354 ('Расподела'), prob: 0.000000\n",
      "  pos  7, token 19095 ('Mag'), prob: 0.000001\n",
      "  pos  8, token 19618 ('Beaut'), prob: 0.000001\n",
      "  pos  9, token 25775 ('Ari'), prob: 0.000000\n",
      "####################\n",
      "\n",
      "=== round 0, epoch 9 ===\n",
      "Read /kaggle/working/hotflip/rounds/suffix_r0_e9.pt successfully. shape=(10, 4096)\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  3.456241\n",
      "  max:  5.210452\n",
      "  mean: 4.002599\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.440039\n",
      "  max:  0.644118\n",
      "  mean: 0.509148\n",
      "\n",
      "Projected discrete suffix token IDs: [23923, 1642, 9802, 10056, 10805, 29169, 28354, 19095, 19618, 25775]\n",
      "Projected discrete suffix tokens: ['Fre', '\".', 'Mark', '▁Consider', '▁causing', '²).', '▁Расподела', 'Mag', '▁Beaut', '▁Ari']\n",
      "Projected suffix as text: 'Fre\".Mark Consider causing²). РасподелаMag Beaut Ari'\n",
      "suffix loss (before projection): 4.068754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suffix loss (after  projection): 4.968382\n",
      "\n",
      "Generated text with suffix (projected):\n",
      "List three sites of historical importance in London.Fre\".Mark Consider causing²). РасподелаMag Beaut Ari.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Per-suffix-token next-token probabilities:\n",
      "  pos  0, token 23923 ('Fre'), prob: 0.000000\n",
      "  pos  1, token  1642 ('\".'), prob: 0.000000\n",
      "  pos  2, token  9802 ('Mark'), prob: 0.000050\n",
      "  pos  3, token 10056 ('Consider'), prob: 0.000001\n",
      "  pos  4, token 10805 ('causing'), prob: 0.000003\n",
      "  pos  5, token 29169 ('²).'), prob: 0.000000\n",
      "  pos  6, token 28354 ('Расподела'), prob: 0.000000\n",
      "  pos  7, token 19095 ('Mag'), prob: 0.000001\n",
      "  pos  8, token 19618 ('Beaut'), prob: 0.000001\n",
      "  pos  9, token 25775 ('Ari'), prob: 0.000000\n",
      "####################\n",
      "\n",
      "=== round 1, epoch 0 ===\n",
      "Read /kaggle/working/hotflip/rounds/suffix_r1_e0.pt successfully. shape=(10, 4096)\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  3.862738\n",
      "  max:  5.130178\n",
      "  mean: 4.484840\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.215106\n",
      "  max:  0.488057\n",
      "  mean: 0.346730\n",
      "\n",
      "Projected discrete suffix token IDs: [2803, 20609, 30171, 8752, 1660, 315, 20609, 317, 341, 22735]\n",
      "Projected discrete suffix tokens: ['▁Let', '▁Portály', 'و', 'Fragment', 'SE', '▁C', '▁Portály', '▁S', '▁M', 'Пе']\n",
      "Projected suffix as text: 'Let PortályوFragmentSE C Portály S MПе'\n",
      "suffix loss (before projection): 4.220569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suffix loss (after  projection): 4.393586\n",
      "\n",
      "Generated text with suffix (projected):\n",
      "List three sites of historical importance in London. Let PortályوFragmentSE C Portály S MПеоооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооо\n",
      "\n",
      "Per-suffix-token next-token probabilities:\n",
      "  pos  0, token  2803 ('Let'), prob: 0.005802\n",
      "  pos  1, token 20609 ('Portály'), prob: 0.000000\n",
      "  pos  2, token 30171 ('و'), prob: 0.000000\n",
      "  pos  3, token  8752 ('Fragment'), prob: 0.000003\n",
      "  pos  4, token  1660 ('SE'), prob: 0.000009\n",
      "  pos  5, token   315 ('C'), prob: 0.000444\n",
      "  pos  6, token 20609 ('Portály'), prob: 0.000000\n",
      "  pos  7, token   317 ('S'), prob: 0.000891\n",
      "  pos  8, token   341 ('M'), prob: 0.003000\n",
      "  pos  9, token 22735 ('Пе'), prob: 0.000001\n",
      "####################\n",
      "\n",
      "=== round 1, epoch 1 ===\n",
      "Read /kaggle/working/hotflip/rounds/suffix_r1_e1.pt successfully. shape=(10, 4096)\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  3.743016\n",
      "  max:  5.000424\n",
      "  mean: 4.371238\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.348451\n",
      "  max:  0.658052\n",
      "  mean: 0.498334\n",
      "\n",
      "Projected discrete suffix token IDs: [2803, 28354, 30171, 8752, 1660, 315, 20609, 317, 341, 22735]\n",
      "Projected discrete suffix tokens: ['▁Let', '▁Расподела', 'و', 'Fragment', 'SE', '▁C', '▁Portály', '▁S', '▁M', 'Пе']\n",
      "Projected suffix as text: 'Let РасподелаوFragmentSE C Portály S MПе'\n",
      "suffix loss (before projection): 5.453373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suffix loss (after  projection): 3.117534\n",
      "\n",
      "Generated text with suffix (projected):\n",
      "List three sites of historical importance in London. Let РасподелаوFragmentSE C Portály S MПеоооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооооо\n",
      "\n",
      "Per-suffix-token next-token probabilities:\n",
      "  pos  0, token  2803 ('Let'), prob: 0.005802\n",
      "  pos  1, token 28354 ('Расподела'), prob: 0.000000\n",
      "  pos  2, token 30171 ('و'), prob: 0.000000\n",
      "  pos  3, token  8752 ('Fragment'), prob: 0.000002\n",
      "  pos  4, token  1660 ('SE'), prob: 0.000006\n",
      "  pos  5, token   315 ('C'), prob: 0.000448\n",
      "  pos  6, token 20609 ('Portály'), prob: 0.000000\n",
      "  pos  7, token   317 ('S'), prob: 0.001122\n",
      "  pos  8, token   341 ('M'), prob: 0.003204\n",
      "  pos  9, token 22735 ('Пе'), prob: 0.000000\n",
      "####################\n",
      "\n",
      "=== round 1, epoch 2 ===\n",
      "Read /kaggle/working/hotflip/rounds/suffix_r1_e2.pt successfully. shape=(10, 4096)\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  3.662354\n",
      "  max:  5.325129\n",
      "  mean: 4.388601\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.407258\n",
      "  max:  0.695439\n",
      "  mean: 0.549953\n",
      "\n",
      "Projected discrete suffix token IDs: [2803, 28354, 30171, 8752, 1660, 315, 20609, 341, 341, 31644]\n",
      "Projected discrete suffix tokens: ['▁Let', '▁Расподела', 'و', 'Fragment', 'SE', '▁C', '▁Portály', '▁M', '▁M', 'ེ']\n",
      "Projected suffix as text: 'Let РасподелаوFragmentSE C Portály M Mེ'\n",
      "suffix loss (before projection): 5.133557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suffix loss (after  projection): 5.218432\n",
      "\n",
      "Generated text with suffix (projected):\n",
      "List three sites of historical importance in London. Let РасподелаوFragmentSE C Portály M MེM M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M\n",
      "\n",
      "Per-suffix-token next-token probabilities:\n",
      "  pos  0, token  2803 ('Let'), prob: 0.005802\n",
      "  pos  1, token 28354 ('Расподела'), prob: 0.000000\n",
      "  pos  2, token 30171 ('و'), prob: 0.000000\n",
      "  pos  3, token  8752 ('Fragment'), prob: 0.000002\n",
      "  pos  4, token  1660 ('SE'), prob: 0.000006\n",
      "  pos  5, token   315 ('C'), prob: 0.000448\n",
      "  pos  6, token 20609 ('Portály'), prob: 0.000000\n",
      "  pos  7, token   341 ('M'), prob: 0.001217\n",
      "  pos  8, token   341 ('M'), prob: 0.004322\n",
      "  pos  9, token 31644 ('ེ'), prob: 0.000001\n",
      "####################\n",
      "\n",
      "=== round 1, epoch 3 ===\n",
      "Read /kaggle/working/hotflip/rounds/suffix_r1_e3.pt successfully. shape=(10, 4096)\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  3.615299\n",
      "  max:  5.420966\n",
      "  mean: 4.382689\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.440769\n",
      "  max:  0.695309\n",
      "  mean: 0.568925\n",
      "\n",
      "Projected discrete suffix token IDs: [2803, 28354, 30171, 8752, 1660, 315, 20609, 341, 341, 31644]\n",
      "Projected discrete suffix tokens: ['▁Let', '▁Расподела', 'و', 'Fragment', 'SE', '▁C', '▁Portály', '▁M', '▁M', 'ེ']\n",
      "Projected suffix as text: 'Let РасподелаوFragmentSE C Portály M Mེ'\n",
      "suffix loss (before projection): 4.856975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suffix loss (after  projection): 5.218432\n",
      "\n",
      "Generated text with suffix (projected):\n",
      "List three sites of historical importance in London. Let РасподелаوFragmentSE C Portály M MེM M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M\n",
      "\n",
      "Per-suffix-token next-token probabilities:\n",
      "  pos  0, token  2803 ('Let'), prob: 0.005802\n",
      "  pos  1, token 28354 ('Расподела'), prob: 0.000000\n",
      "  pos  2, token 30171 ('و'), prob: 0.000000\n",
      "  pos  3, token  8752 ('Fragment'), prob: 0.000002\n",
      "  pos  4, token  1660 ('SE'), prob: 0.000006\n",
      "  pos  5, token   315 ('C'), prob: 0.000448\n",
      "  pos  6, token 20609 ('Portály'), prob: 0.000000\n",
      "  pos  7, token   341 ('M'), prob: 0.001217\n",
      "  pos  8, token   341 ('M'), prob: 0.004322\n",
      "  pos  9, token 31644 ('ེ'), prob: 0.000001\n",
      "####################\n",
      "\n",
      "=== round 1, epoch 4 ===\n",
      "Read /kaggle/working/hotflip/rounds/suffix_r1_e4.pt successfully. shape=(10, 4096)\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  3.684350\n",
      "  max:  5.974068\n",
      "  mean: 4.989570\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.261777\n",
      "  max:  0.593803\n",
      "  mean: 0.479870\n",
      "\n",
      "Projected discrete suffix token IDs: [2803, 28354, 30171, 8752, 1660, 315, 28354, 317, 341, 31644]\n",
      "Projected discrete suffix tokens: ['▁Let', '▁Расподела', 'و', 'Fragment', 'SE', '▁C', '▁Расподела', '▁S', '▁M', 'ེ']\n",
      "Projected suffix as text: 'Let РасподелаوFragmentSE C Расподела S Mེ'\n",
      "suffix loss (before projection): 3.630424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suffix loss (after  projection): 6.041427\n",
      "\n",
      "Generated text with suffix (projected):\n",
      "List three sites of historical importance in London. Let РасподелаوFragmentSE C Расподела S MེT.\n",
      "\n",
      "Now, let's say you want to find out what the word \"con\" means. You could look it up in a dictionary, but that's a lot of work. Instead, you could use a search engine. You type in \"con\" and it gives you a list of sites\n",
      "\n",
      "Per-suffix-token next-token probabilities:\n",
      "  pos  0, token  2803 ('Let'), prob: 0.005802\n",
      "  pos  1, token 28354 ('Расподела'), prob: 0.000000\n",
      "  pos  2, token 30171 ('و'), prob: 0.000000\n",
      "  pos  3, token  8752 ('Fragment'), prob: 0.000002\n",
      "  pos  4, token  1660 ('SE'), prob: 0.000006\n",
      "  pos  5, token   315 ('C'), prob: 0.000448\n",
      "  pos  6, token 28354 ('Расподела'), prob: 0.000000\n",
      "  pos  7, token   317 ('S'), prob: 0.001241\n",
      "  pos  8, token   341 ('M'), prob: 0.001789\n",
      "  pos  9, token 31644 ('ེ'), prob: 0.000001\n",
      "####################\n",
      "\n",
      "=== round 1, epoch 5 ===\n",
      "Read /kaggle/working/hotflip/rounds/suffix_r1_e5.pt successfully. shape=(10, 4096)\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  3.669656\n",
      "  max:  5.977582\n",
      "  mean: 4.964544\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.388132\n",
      "  max:  0.651032\n",
      "  mean: 0.545188\n",
      "\n",
      "Projected discrete suffix token IDs: [2803, 28354, 30171, 8752, 1660, 315, 28354, 317, 341, 31644]\n",
      "Projected discrete suffix tokens: ['▁Let', '▁Расподела', 'و', 'Fragment', 'SE', '▁C', '▁Расподела', '▁S', '▁M', 'ེ']\n",
      "Projected suffix as text: 'Let РасподелаوFragmentSE C Расподела S Mེ'\n",
      "suffix loss (before projection): 3.950564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suffix loss (after  projection): 6.041427\n",
      "\n",
      "Generated text with suffix (projected):\n",
      "List three sites of historical importance in London. Let РасподелаوFragmentSE C Расподела S MེT.\n",
      "\n",
      "Now, let's say you want to find out what the word \"con\" means. You could look it up in a dictionary, but that's a lot of work. Instead, you could use a search engine. You type in \"con\" and it gives you a list of sites\n",
      "\n",
      "Per-suffix-token next-token probabilities:\n",
      "  pos  0, token  2803 ('Let'), prob: 0.005802\n",
      "  pos  1, token 28354 ('Расподела'), prob: 0.000000\n",
      "  pos  2, token 30171 ('و'), prob: 0.000000\n",
      "  pos  3, token  8752 ('Fragment'), prob: 0.000002\n",
      "  pos  4, token  1660 ('SE'), prob: 0.000006\n",
      "  pos  5, token   315 ('C'), prob: 0.000448\n",
      "  pos  6, token 28354 ('Расподела'), prob: 0.000000\n",
      "  pos  7, token   317 ('S'), prob: 0.001241\n",
      "  pos  8, token   341 ('M'), prob: 0.001789\n",
      "  pos  9, token 31644 ('ེ'), prob: 0.000001\n",
      "####################\n",
      "\n",
      "=== round 1, epoch 6 ===\n",
      "Read /kaggle/working/hotflip/rounds/suffix_r1_e6.pt successfully. shape=(10, 4096)\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  3.713540\n",
      "  max:  6.255694\n",
      "  mean: 5.089516\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.421173\n",
      "  max:  0.669912\n",
      "  mean: 0.553457\n",
      "\n",
      "Projected discrete suffix token IDs: [2803, 28354, 30171, 8752, 1660, 315, 28354, 341, 341, 31644]\n",
      "Projected discrete suffix tokens: ['▁Let', '▁Расподела', 'و', 'Fragment', 'SE', '▁C', '▁Расподела', '▁M', '▁M', 'ེ']\n",
      "Projected suffix as text: 'Let РасподелаوFragmentSE C Расподела M Mེ'\n",
      "suffix loss (before projection): 3.140436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suffix loss (after  projection): 5.262692\n",
      "\n",
      "Generated text with suffix (projected):\n",
      "List three sites of historical importance in London. Let РасподелаوFragmentSE C Расподела M MེM M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M\n",
      "\n",
      "Per-suffix-token next-token probabilities:\n",
      "  pos  0, token  2803 ('Let'), prob: 0.005802\n",
      "  pos  1, token 28354 ('Расподела'), prob: 0.000000\n",
      "  pos  2, token 30171 ('و'), prob: 0.000000\n",
      "  pos  3, token  8752 ('Fragment'), prob: 0.000002\n",
      "  pos  4, token  1660 ('SE'), prob: 0.000006\n",
      "  pos  5, token   315 ('C'), prob: 0.000448\n",
      "  pos  6, token 28354 ('Расподела'), prob: 0.000000\n",
      "  pos  7, token   341 ('M'), prob: 0.001310\n",
      "  pos  8, token   341 ('M'), prob: 0.004116\n",
      "  pos  9, token 31644 ('ེ'), prob: 0.000000\n",
      "####################\n",
      "\n",
      "=== round 1, epoch 7 ===\n",
      "Read /kaggle/working/hotflip/rounds/suffix_r1_e7.pt successfully. shape=(10, 4096)\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  3.728868\n",
      "  max:  6.201711\n",
      "  mean: 5.081540\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.432318\n",
      "  max:  0.689173\n",
      "  mean: 0.569908\n",
      "\n",
      "Projected discrete suffix token IDs: [2803, 28354, 30171, 8752, 1660, 315, 28354, 341, 341, 31644]\n",
      "Projected discrete suffix tokens: ['▁Let', '▁Расподела', 'و', 'Fragment', 'SE', '▁C', '▁Расподела', '▁M', '▁M', 'ེ']\n",
      "Projected suffix as text: 'Let РасподелаوFragmentSE C Расподела M Mེ'\n",
      "suffix loss (before projection): 3.267055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suffix loss (after  projection): 5.262692\n",
      "\n",
      "Generated text with suffix (projected):\n",
      "List three sites of historical importance in London. Let РасподелаوFragmentSE C Расподела M MེM M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M\n",
      "\n",
      "Per-suffix-token next-token probabilities:\n",
      "  pos  0, token  2803 ('Let'), prob: 0.005802\n",
      "  pos  1, token 28354 ('Расподела'), prob: 0.000000\n",
      "  pos  2, token 30171 ('و'), prob: 0.000000\n",
      "  pos  3, token  8752 ('Fragment'), prob: 0.000002\n",
      "  pos  4, token  1660 ('SE'), prob: 0.000006\n",
      "  pos  5, token   315 ('C'), prob: 0.000448\n",
      "  pos  6, token 28354 ('Расподела'), prob: 0.000000\n",
      "  pos  7, token   341 ('M'), prob: 0.001310\n",
      "  pos  8, token   341 ('M'), prob: 0.004116\n",
      "  pos  9, token 31644 ('ེ'), prob: 0.000000\n",
      "####################\n",
      "\n",
      "=== round 1, epoch 8 ===\n",
      "Read /kaggle/working/hotflip/rounds/suffix_r1_e8.pt successfully. shape=(10, 4096)\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  3.731563\n",
      "  max:  6.134434\n",
      "  mean: 5.061459\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.437097\n",
      "  max:  0.699315\n",
      "  mean: 0.578477\n",
      "\n",
      "Projected discrete suffix token IDs: [2803, 28354, 30171, 8752, 1660, 315, 28354, 341, 341, 31644]\n",
      "Projected discrete suffix tokens: ['▁Let', '▁Расподела', 'و', 'Fragment', 'SE', '▁C', '▁Расподела', '▁M', '▁M', 'ེ']\n",
      "Projected suffix as text: 'Let РасподелаوFragmentSE C Расподела M Mེ'\n",
      "suffix loss (before projection): 5.121234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suffix loss (after  projection): 5.262692\n",
      "\n",
      "Generated text with suffix (projected):\n",
      "List three sites of historical importance in London. Let РасподелаوFragmentSE C Расподела M MེM M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M\n",
      "\n",
      "Per-suffix-token next-token probabilities:\n",
      "  pos  0, token  2803 ('Let'), prob: 0.005802\n",
      "  pos  1, token 28354 ('Расподела'), prob: 0.000000\n",
      "  pos  2, token 30171 ('و'), prob: 0.000000\n",
      "  pos  3, token  8752 ('Fragment'), prob: 0.000002\n",
      "  pos  4, token  1660 ('SE'), prob: 0.000006\n",
      "  pos  5, token   315 ('C'), prob: 0.000448\n",
      "  pos  6, token 28354 ('Расподела'), prob: 0.000000\n",
      "  pos  7, token   341 ('M'), prob: 0.001310\n",
      "  pos  8, token   341 ('M'), prob: 0.004116\n",
      "  pos  9, token 31644 ('ེ'), prob: 0.000000\n",
      "####################\n",
      "\n",
      "=== round 1, epoch 9 ===\n",
      "Read /kaggle/working/hotflip/rounds/suffix_r1_e9.pt successfully. shape=(10, 4096)\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  3.736791\n",
      "  max:  6.079888\n",
      "  mean: 5.047103\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.439517\n",
      "  max:  0.705136\n",
      "  mean: 0.583243\n",
      "\n",
      "Projected discrete suffix token IDs: [2803, 28354, 30171, 8752, 1660, 315, 28354, 341, 341, 31644]\n",
      "Projected discrete suffix tokens: ['▁Let', '▁Расподела', 'و', 'Fragment', 'SE', '▁C', '▁Расподела', '▁M', '▁M', 'ེ']\n",
      "Projected suffix as text: 'Let РасподелаوFragmentSE C Расподела M Mེ'\n",
      "suffix loss (before projection): 5.882952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suffix loss (after  projection): 5.262692\n",
      "\n",
      "Generated text with suffix (projected):\n",
      "List three sites of historical importance in London. Let РасподелаوFragmentSE C Расподела M MེM M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M\n",
      "\n",
      "Per-suffix-token next-token probabilities:\n",
      "  pos  0, token  2803 ('Let'), prob: 0.005802\n",
      "  pos  1, token 28354 ('Расподела'), prob: 0.000000\n",
      "  pos  2, token 30171 ('و'), prob: 0.000000\n",
      "  pos  3, token  8752 ('Fragment'), prob: 0.000002\n",
      "  pos  4, token  1660 ('SE'), prob: 0.000006\n",
      "  pos  5, token   315 ('C'), prob: 0.000448\n",
      "  pos  6, token 28354 ('Расподела'), prob: 0.000000\n",
      "  pos  7, token   341 ('M'), prob: 0.001310\n",
      "  pos  8, token   341 ('M'), prob: 0.004116\n",
      "  pos  9, token 31644 ('ེ'), prob: 0.000000\n",
      "####################\n",
      "\n",
      "=== round 2, epoch 0 ===\n",
      "Read /kaggle/working/hotflip/rounds/suffix_r2_e0.pt successfully. shape=(10, 4096)\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  3.385450\n",
      "  max:  4.204616\n",
      "  mean: 3.827991\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.296418\n",
      "  max:  0.646272\n",
      "  mean: 0.450254\n",
      "\n",
      "Projected discrete suffix token IDs: [29899, 30103, 28354, 960, 6193, 29903, 10803, 1952, 3872, 28354]\n",
      "Projected discrete suffix tokens: ['-', '́', '▁Расподела', '▁If', '▁XX', 'S', '▁Word', '▁Bo', '▁Don', '▁Расподела']\n",
      "Projected suffix as text: '-́ Расподела If XXS Word Bo Don Расподела'\n",
      "suffix loss (before projection): 3.786370\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suffix loss (after  projection): 3.250025\n",
      "\n",
      "Generated text with suffix (projected):\n",
      "List three sites of historical importance in London.-́ Расподела If XXS Word Bo Don Расподела ́́́́́́́́́́́́́́́́́́́́́́́́́́́́́́́́́́́́́́́́́́́́́́́́́́́́́́́́́́́́́́́\n",
      "\n",
      "Per-suffix-token next-token probabilities:\n",
      "  pos  0, token 29899 ('-'), prob: 0.000013\n",
      "  pos  1, token 30103 ('́'), prob: 0.000000\n",
      "  pos  2, token 28354 ('Расподела'), prob: 0.000000\n",
      "  pos  3, token   960 ('If'), prob: 0.000183\n",
      "  pos  4, token  6193 ('XX'), prob: 0.000000\n",
      "  pos  5, token 29903 ('S'), prob: 0.000348\n",
      "  pos  6, token 10803 ('Word'), prob: 0.000011\n",
      "  pos  7, token  1952 ('Bo'), prob: 0.000043\n",
      "  pos  8, token  3872 ('Don'), prob: 0.000006\n",
      "  pos  9, token 28354 ('Расподела'), prob: 0.000000\n",
      "####################\n",
      "\n",
      "=== round 2, epoch 1 ===\n",
      "Read /kaggle/working/hotflip/rounds/suffix_r2_e1.pt successfully. shape=(10, 4096)\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  2.822855\n",
      "  max:  4.902424\n",
      "  mean: 3.913154\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.440908\n",
      "  max:  0.673497\n",
      "  mean: 0.520130\n",
      "\n",
      "Projected discrete suffix token IDs: [29899, 30103, 28354, 960, 6193, 29903, 10803, 8431, 3872, 28354]\n",
      "Projected discrete suffix tokens: ['-', '́', '▁Расподела', '▁If', '▁XX', 'S', '▁Word', 'Bo', '▁Don', '▁Расподела']\n",
      "Projected suffix as text: '-́ Расподела If XXS WordBo Don Расподела'\n",
      "suffix loss (before projection): 3.431216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suffix loss (after  projection): 5.127399\n",
      "\n",
      "Generated text with suffix (projected):\n",
      "List three sites of historical importance in London.-́ Расподела If XXS WordBo Don Расподелаt know what to do with this. I'm not sure if this is a joke or not. I'm not sure if this is a joke or not. I'm not sure if this is a joke or not. I'm not sure if this is a joke or not.\n",
      "\n",
      "Per-suffix-token next-token probabilities:\n",
      "  pos  0, token 29899 ('-'), prob: 0.000013\n",
      "  pos  1, token 30103 ('́'), prob: 0.000000\n",
      "  pos  2, token 28354 ('Расподела'), prob: 0.000000\n",
      "  pos  3, token   960 ('If'), prob: 0.000183\n",
      "  pos  4, token  6193 ('XX'), prob: 0.000000\n",
      "  pos  5, token 29903 ('S'), prob: 0.000348\n",
      "  pos  6, token 10803 ('Word'), prob: 0.000011\n",
      "  pos  7, token  8431 ('Bo'), prob: 0.000005\n",
      "  pos  8, token  3872 ('Don'), prob: 0.000000\n",
      "  pos  9, token 28354 ('Расподела'), prob: 0.000000\n",
      "####################\n",
      "\n",
      "=== round 2, epoch 2 ===\n",
      "Read /kaggle/working/hotflip/rounds/suffix_r2_e2.pt successfully. shape=(10, 4096)\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  2.747813\n",
      "  max:  6.905246\n",
      "  mean: 4.511523\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.408643\n",
      "  max:  0.666075\n",
      "  mean: 0.521006\n",
      "\n",
      "Projected discrete suffix token IDs: [29899, 30103, 28354, 960, 6193, 29903, 10803, 8431, 3872, 28354]\n",
      "Projected discrete suffix tokens: ['-', '́', '▁Расподела', '▁If', '▁XX', 'S', '▁Word', 'Bo', '▁Don', '▁Расподела']\n",
      "Projected suffix as text: '-́ Расподела If XXS WordBo Don Расподела'\n",
      "suffix loss (before projection): 1.405130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suffix loss (after  projection): 5.127399\n",
      "\n",
      "Generated text with suffix (projected):\n",
      "List three sites of historical importance in London.-́ Расподела If XXS WordBo Don Расподелаt know what to do with this. I'm not sure if this is a joke or not. I'm not sure if this is a joke or not. I'm not sure if this is a joke or not. I'm not sure if this is a joke or not.\n",
      "\n",
      "Per-suffix-token next-token probabilities:\n",
      "  pos  0, token 29899 ('-'), prob: 0.000013\n",
      "  pos  1, token 30103 ('́'), prob: 0.000000\n",
      "  pos  2, token 28354 ('Расподела'), prob: 0.000000\n",
      "  pos  3, token   960 ('If'), prob: 0.000183\n",
      "  pos  4, token  6193 ('XX'), prob: 0.000000\n",
      "  pos  5, token 29903 ('S'), prob: 0.000348\n",
      "  pos  6, token 10803 ('Word'), prob: 0.000011\n",
      "  pos  7, token  8431 ('Bo'), prob: 0.000005\n",
      "  pos  8, token  3872 ('Don'), prob: 0.000000\n",
      "  pos  9, token 28354 ('Расподела'), prob: 0.000000\n",
      "####################\n",
      "\n",
      "=== round 2, epoch 3 ===\n",
      "Read /kaggle/working/hotflip/rounds/suffix_r2_e3.pt successfully. shape=(10, 4096)\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  2.436139\n",
      "  max:  6.786112\n",
      "  mean: 4.347416\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.462000\n",
      "  max:  0.679031\n",
      "  mean: 0.552736\n",
      "\n",
      "Projected discrete suffix token IDs: [29899, 30103, 28354, 960, 6193, 29903, 13899, 8431, 3872, 28354]\n",
      "Projected discrete suffix tokens: ['-', '́', '▁Расподела', '▁If', '▁XX', 'S', '▁Short', 'Bo', '▁Don', '▁Расподела']\n",
      "Projected suffix as text: '-́ Расподела If XXS ShortBo Don Расподела'\n",
      "suffix loss (before projection): 2.181285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suffix loss (after  projection): 5.143061\n",
      "\n",
      "Generated text with suffix (projected):\n",
      "List three sites of historical importance in London.-́ Расподела If XXS ShortBo Don Расподелаt know what to do with this. I'm not sure if this is a joke or not. I'm not sure if this is a joke or not. I'm not sure if this is a joke or not. I'm not sure if this is a joke or not.\n",
      "\n",
      "Per-suffix-token next-token probabilities:\n",
      "  pos  0, token 29899 ('-'), prob: 0.000013\n",
      "  pos  1, token 30103 ('́'), prob: 0.000000\n",
      "  pos  2, token 28354 ('Расподела'), prob: 0.000000\n",
      "  pos  3, token   960 ('If'), prob: 0.000183\n",
      "  pos  4, token  6193 ('XX'), prob: 0.000000\n",
      "  pos  5, token 29903 ('S'), prob: 0.000348\n",
      "  pos  6, token 13899 ('Short'), prob: 0.000033\n",
      "  pos  7, token  8431 ('Bo'), prob: 0.000038\n",
      "  pos  8, token  3872 ('Don'), prob: 0.000000\n",
      "  pos  9, token 28354 ('Расподела'), prob: 0.000000\n",
      "####################\n",
      "\n",
      "=== round 2, epoch 4 ===\n",
      "Read /kaggle/working/hotflip/rounds/suffix_r2_e4.pt successfully. shape=(10, 4096)\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  2.239670\n",
      "  max:  6.779982\n",
      "  mean: 4.259518\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.462986\n",
      "  max:  0.683640\n",
      "  mean: 0.563891\n",
      "\n",
      "Projected discrete suffix token IDs: [29899, 30103, 28354, 960, 6193, 29903, 13899, 1952, 3872, 28354]\n",
      "Projected discrete suffix tokens: ['-', '́', '▁Расподела', '▁If', '▁XX', 'S', '▁Short', '▁Bo', '▁Don', '▁Расподела']\n",
      "Projected suffix as text: '-́ Расподела If XXS Short Bo Don Расподела'\n",
      "suffix loss (before projection): 3.438234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suffix loss (after  projection): 4.865295\n",
      "\n",
      "Generated text with suffix (projected):\n",
      "List three sites of historical importance in London.-́ Расподела If XXS Short Bo Don Расподела If XXS Short Bo Doń If XXS Short Bo Doń If XXS Short Bo Doń If XXS Short Bo Doń If XXS Short Bo Doń If XXS Short Bo Doń If XXS Short Bo Doń If XXS Short Bo Doń If XXS Short Bo Doń If\n",
      "\n",
      "Per-suffix-token next-token probabilities:\n",
      "  pos  0, token 29899 ('-'), prob: 0.000013\n",
      "  pos  1, token 30103 ('́'), prob: 0.000000\n",
      "  pos  2, token 28354 ('Расподела'), prob: 0.000000\n",
      "  pos  3, token   960 ('If'), prob: 0.000183\n",
      "  pos  4, token  6193 ('XX'), prob: 0.000000\n",
      "  pos  5, token 29903 ('S'), prob: 0.000348\n",
      "  pos  6, token 13899 ('Short'), prob: 0.000033\n",
      "  pos  7, token  1952 ('Bo'), prob: 0.000068\n",
      "  pos  8, token  3872 ('Don'), prob: 0.000001\n",
      "  pos  9, token 28354 ('Расподела'), prob: 0.000000\n",
      "####################\n",
      "\n",
      "=== round 2, epoch 5 ===\n",
      "Read /kaggle/working/hotflip/rounds/suffix_r2_e5.pt successfully. shape=(10, 4096)\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  2.040251\n",
      "  max:  6.763978\n",
      "  mean: 4.194578\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.455609\n",
      "  max:  0.687057\n",
      "  mean: 0.566284\n",
      "\n",
      "Projected discrete suffix token IDs: [29899, 30103, 28354, 960, 6193, 29903, 13899, 1952, 3872, 28354]\n",
      "Projected discrete suffix tokens: ['-', '́', '▁Расподела', '▁If', '▁XX', 'S', '▁Short', '▁Bo', '▁Don', '▁Расподела']\n",
      "Projected suffix as text: '-́ Расподела If XXS Short Bo Don Расподела'\n",
      "suffix loss (before projection): 3.198435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suffix loss (after  projection): 4.865295\n",
      "\n",
      "Generated text with suffix (projected):\n",
      "List three sites of historical importance in London.-́ Расподела If XXS Short Bo Don Расподела If XXS Short Bo Doń If XXS Short Bo Doń If XXS Short Bo Doń If XXS Short Bo Doń If XXS Short Bo Doń If XXS Short Bo Doń If XXS Short Bo Doń If XXS Short Bo Doń If XXS Short Bo Doń If\n",
      "\n",
      "Per-suffix-token next-token probabilities:\n",
      "  pos  0, token 29899 ('-'), prob: 0.000013\n",
      "  pos  1, token 30103 ('́'), prob: 0.000000\n",
      "  pos  2, token 28354 ('Расподела'), prob: 0.000000\n",
      "  pos  3, token   960 ('If'), prob: 0.000183\n",
      "  pos  4, token  6193 ('XX'), prob: 0.000000\n",
      "  pos  5, token 29903 ('S'), prob: 0.000348\n",
      "  pos  6, token 13899 ('Short'), prob: 0.000033\n",
      "  pos  7, token  1952 ('Bo'), prob: 0.000068\n",
      "  pos  8, token  3872 ('Don'), prob: 0.000001\n",
      "  pos  9, token 28354 ('Расподела'), prob: 0.000000\n",
      "####################\n",
      "\n",
      "=== round 2, epoch 6 ===\n",
      "Read /kaggle/working/hotflip/rounds/suffix_r2_e6.pt successfully. shape=(10, 4096)\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  1.880682\n",
      "  max:  6.747434\n",
      "  mean: 4.138816\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.455117\n",
      "  max:  0.693635\n",
      "  mean: 0.569918\n",
      "\n",
      "Projected discrete suffix token IDs: [29899, 30103, 28354, 960, 6193, 29903, 13899, 1952, 3872, 28354]\n",
      "Projected discrete suffix tokens: ['-', '́', '▁Расподела', '▁If', '▁XX', 'S', '▁Short', '▁Bo', '▁Don', '▁Расподела']\n",
      "Projected suffix as text: '-́ Расподела If XXS Short Bo Don Расподела'\n",
      "suffix loss (before projection): 4.832017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suffix loss (after  projection): 4.865295\n",
      "\n",
      "Generated text with suffix (projected):\n",
      "List three sites of historical importance in London.-́ Расподела If XXS Short Bo Don Расподела If XXS Short Bo Doń If XXS Short Bo Doń If XXS Short Bo Doń If XXS Short Bo Doń If XXS Short Bo Doń If XXS Short Bo Doń If XXS Short Bo Doń If XXS Short Bo Doń If XXS Short Bo Doń If\n",
      "\n",
      "Per-suffix-token next-token probabilities:\n",
      "  pos  0, token 29899 ('-'), prob: 0.000013\n",
      "  pos  1, token 30103 ('́'), prob: 0.000000\n",
      "  pos  2, token 28354 ('Расподела'), prob: 0.000000\n",
      "  pos  3, token   960 ('If'), prob: 0.000183\n",
      "  pos  4, token  6193 ('XX'), prob: 0.000000\n",
      "  pos  5, token 29903 ('S'), prob: 0.000348\n",
      "  pos  6, token 13899 ('Short'), prob: 0.000033\n",
      "  pos  7, token  1952 ('Bo'), prob: 0.000068\n",
      "  pos  8, token  3872 ('Don'), prob: 0.000001\n",
      "  pos  9, token 28354 ('Расподела'), prob: 0.000000\n",
      "####################\n",
      "\n",
      "=== round 2, epoch 7 ===\n",
      "Read /kaggle/working/hotflip/rounds/suffix_r2_e7.pt successfully. shape=(10, 4096)\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  1.783249\n",
      "  max:  6.742734\n",
      "  mean: 4.095409\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.452021\n",
      "  max:  0.700502\n",
      "  mean: 0.570626\n",
      "\n",
      "Projected discrete suffix token IDs: [29899, 30103, 28354, 960, 6193, 29903, 13899, 1952, 3872, 28354]\n",
      "Projected discrete suffix tokens: ['-', '́', '▁Расподела', '▁If', '▁XX', 'S', '▁Short', '▁Bo', '▁Don', '▁Расподела']\n",
      "Projected suffix as text: '-́ Расподела If XXS Short Bo Don Расподела'\n",
      "suffix loss (before projection): 4.678416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suffix loss (after  projection): 4.865295\n",
      "\n",
      "Generated text with suffix (projected):\n",
      "List three sites of historical importance in London.-́ Расподела If XXS Short Bo Don Расподела If XXS Short Bo Doń If XXS Short Bo Doń If XXS Short Bo Doń If XXS Short Bo Doń If XXS Short Bo Doń If XXS Short Bo Doń If XXS Short Bo Doń If XXS Short Bo Doń If XXS Short Bo Doń If\n",
      "\n",
      "Per-suffix-token next-token probabilities:\n",
      "  pos  0, token 29899 ('-'), prob: 0.000013\n",
      "  pos  1, token 30103 ('́'), prob: 0.000000\n",
      "  pos  2, token 28354 ('Расподела'), prob: 0.000000\n",
      "  pos  3, token   960 ('If'), prob: 0.000183\n",
      "  pos  4, token  6193 ('XX'), prob: 0.000000\n",
      "  pos  5, token 29903 ('S'), prob: 0.000348\n",
      "  pos  6, token 13899 ('Short'), prob: 0.000033\n",
      "  pos  7, token  1952 ('Bo'), prob: 0.000068\n",
      "  pos  8, token  3872 ('Don'), prob: 0.000001\n",
      "  pos  9, token 28354 ('Расподела'), prob: 0.000000\n",
      "####################\n",
      "\n",
      "=== round 2, epoch 8 ===\n",
      "Read /kaggle/working/hotflip/rounds/suffix_r2_e8.pt successfully. shape=(10, 4096)\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  1.718498\n",
      "  max:  6.737160\n",
      "  mean: 4.059024\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.450641\n",
      "  max:  0.705202\n",
      "  mean: 0.571463\n",
      "\n",
      "Projected discrete suffix token IDs: [29899, 30103, 28354, 960, 6193, 29903, 13899, 1952, 3872, 28354]\n",
      "Projected discrete suffix tokens: ['-', '́', '▁Расподела', '▁If', '▁XX', 'S', '▁Short', '▁Bo', '▁Don', '▁Расподела']\n",
      "Projected suffix as text: '-́ Расподела If XXS Short Bo Don Расподела'\n",
      "suffix loss (before projection): 4.911637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suffix loss (after  projection): 4.865295\n",
      "\n",
      "Generated text with suffix (projected):\n",
      "List three sites of historical importance in London.-́ Расподела If XXS Short Bo Don Расподела If XXS Short Bo Doń If XXS Short Bo Doń If XXS Short Bo Doń If XXS Short Bo Doń If XXS Short Bo Doń If XXS Short Bo Doń If XXS Short Bo Doń If XXS Short Bo Doń If XXS Short Bo Doń If\n",
      "\n",
      "Per-suffix-token next-token probabilities:\n",
      "  pos  0, token 29899 ('-'), prob: 0.000013\n",
      "  pos  1, token 30103 ('́'), prob: 0.000000\n",
      "  pos  2, token 28354 ('Расподела'), prob: 0.000000\n",
      "  pos  3, token   960 ('If'), prob: 0.000183\n",
      "  pos  4, token  6193 ('XX'), prob: 0.000000\n",
      "  pos  5, token 29903 ('S'), prob: 0.000348\n",
      "  pos  6, token 13899 ('Short'), prob: 0.000033\n",
      "  pos  7, token  1952 ('Bo'), prob: 0.000068\n",
      "  pos  8, token  3872 ('Don'), prob: 0.000001\n",
      "  pos  9, token 28354 ('Расподела'), prob: 0.000000\n",
      "####################\n",
      "\n",
      "=== round 2, epoch 9 ===\n",
      "Read /kaggle/working/hotflip/rounds/suffix_r2_e9.pt successfully. shape=(10, 4096)\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  1.931770\n",
      "  max:  7.029641\n",
      "  mean: 4.173036\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.438981\n",
      "  max:  0.689714\n",
      "  mean: 0.564532\n",
      "\n",
      "Projected discrete suffix token IDs: [29899, 30103, 28354, 960, 6193, 29903, 13899, 1952, 3872, 28354]\n",
      "Projected discrete suffix tokens: ['-', '́', '▁Расподела', '▁If', '▁XX', 'S', '▁Short', '▁Bo', '▁Don', '▁Расподела']\n",
      "Projected suffix as text: '-́ Расподела If XXS Short Bo Don Расподела'\n",
      "suffix loss (before projection): 1.086812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suffix loss (after  projection): 4.865295\n",
      "\n",
      "Generated text with suffix (projected):\n",
      "List three sites of historical importance in London.-́ Расподела If XXS Short Bo Don Расподела If XXS Short Bo Doń If XXS Short Bo Doń If XXS Short Bo Doń If XXS Short Bo Doń If XXS Short Bo Doń If XXS Short Bo Doń If XXS Short Bo Doń If XXS Short Bo Doń If XXS Short Bo Doń If\n",
      "\n",
      "Per-suffix-token next-token probabilities:\n",
      "  pos  0, token 29899 ('-'), prob: 0.000013\n",
      "  pos  1, token 30103 ('́'), prob: 0.000000\n",
      "  pos  2, token 28354 ('Расподела'), prob: 0.000000\n",
      "  pos  3, token   960 ('If'), prob: 0.000183\n",
      "  pos  4, token  6193 ('XX'), prob: 0.000000\n",
      "  pos  5, token 29903 ('S'), prob: 0.000348\n",
      "  pos  6, token 13899 ('Short'), prob: 0.000033\n",
      "  pos  7, token  1952 ('Bo'), prob: 0.000068\n",
      "  pos  8, token  3872 ('Don'), prob: 0.000001\n",
      "  pos  9, token 28354 ('Расподела'), prob: 0.000000\n",
      "####################\n",
      "\n",
      "=== round 3, epoch 0 ===\n",
      "Read /kaggle/working/hotflip/rounds/suffix_r3_e0.pt successfully. shape=(10, 4096)\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  3.049705\n",
      "  max:  4.143126\n",
      "  mean: 3.715672\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.242073\n",
      "  max:  0.678135\n",
      "  mean: 0.491735\n",
      "\n",
      "Projected discrete suffix token IDs: [3798, 5853, 30267, 11134, 22666, 317, 28574, 341, 402, 28574]\n",
      "Projected discrete suffix tokens: ['oreferrer', '▁More', '。', '▁meter', '▁Screen', '▁S', '▁Mediabestanden', '▁M', '▁G', '▁Mediabestanden']\n",
      "Projected suffix as text: 'oreferrer More。 meter Screen S Mediabestanden M G Mediabestanden'\n",
      "suffix loss (before projection): 3.113307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suffix loss (after  projection): 4.568519\n",
      "\n",
      "Generated text with suffix (projected):\n",
      "List three sites of historical importance in London.oreferrer More。 meter Screen S Mediabestanden M G Mediabestanden The same way that you can't just go to a store and buy a car.  You have to go through a dealer.  The same is true for the government.  They have to go through a dealer.  The dealer is the bank.  The bank is the one that lo\n",
      "\n",
      "Per-suffix-token next-token probabilities:\n",
      "  pos  0, token  3798 ('oreferrer'), prob: 0.000000\n",
      "  pos  1, token  5853 ('More'), prob: 0.000066\n",
      "  pos  2, token 30267 ('。'), prob: 0.000000\n",
      "  pos  3, token 11134 ('meter'), prob: 0.000000\n",
      "  pos  4, token 22666 ('Screen'), prob: 0.000000\n",
      "  pos  5, token   317 ('S'), prob: 0.000444\n",
      "  pos  6, token 28574 ('Mediabestanden'), prob: 0.000000\n",
      "  pos  7, token   341 ('M'), prob: 0.001777\n",
      "  pos  8, token   402 ('G'), prob: 0.003143\n",
      "  pos  9, token 28574 ('Mediabestanden'), prob: 0.000000\n",
      "####################\n",
      "\n",
      "=== round 3, epoch 1 ===\n",
      "Read /kaggle/working/hotflip/rounds/suffix_r3_e1.pt successfully. shape=(10, 4096)\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  2.712757\n",
      "  max:  4.739722\n",
      "  mean: 3.668025\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.363806\n",
      "  max:  0.701043\n",
      "  mean: 0.554393\n",
      "\n",
      "Projected discrete suffix token IDs: [1706, 5853, 30267, 11134, 22666, 349, 28574, 341, 402, 21870]\n",
      "Projected discrete suffix tokens: ['▁Sp', '▁More', '。', '▁meter', '▁Screen', '▁P', '▁Mediabestanden', '▁M', '▁G', 'Ma']\n",
      "Projected suffix as text: 'Sp More。 meter Screen P Mediabestanden M GMa'\n",
      "suffix loss (before projection): 3.665217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suffix loss (after  projection): 4.770502\n",
      "\n",
      "Generated text with suffix (projected):\n",
      "List three sites of historical importance in London. Sp More。 meter Screen P Mediabestanden M GMaP. The same way that you can't just go to a store and buy a car. You have to go through a dealership.\n",
      "\n",
      "The same way that you can't just go to a store and buy a house. You have to go through a realtor.\n",
      "\n",
      "The same way\n",
      "\n",
      "Per-suffix-token next-token probabilities:\n",
      "  pos  0, token  1706 ('Sp'), prob: 0.000304\n",
      "  pos  1, token  5853 ('More'), prob: 0.000000\n",
      "  pos  2, token 30267 ('。'), prob: 0.000000\n",
      "  pos  3, token 11134 ('meter'), prob: 0.000000\n",
      "  pos  4, token 22666 ('Screen'), prob: 0.000000\n",
      "  pos  5, token   349 ('P'), prob: 0.000282\n",
      "  pos  6, token 28574 ('Mediabestanden'), prob: 0.000000\n",
      "  pos  7, token   341 ('M'), prob: 0.001537\n",
      "  pos  8, token   402 ('G'), prob: 0.005203\n",
      "  pos  9, token 21870 ('Ma'), prob: 0.000005\n",
      "####################\n",
      "\n",
      "=== round 3, epoch 2 ===\n",
      "Read /kaggle/working/hotflip/rounds/suffix_r3_e2.pt successfully. shape=(10, 4096)\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  2.616719\n",
      "  max:  5.028667\n",
      "  mean: 3.632674\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.432833\n",
      "  max:  0.706039\n",
      "  mean: 0.575743\n",
      "\n",
      "Projected discrete suffix token IDs: [1706, 5853, 30267, 11134, 22666, 349, 28574, 341, 402, 21870]\n",
      "Projected discrete suffix tokens: ['▁Sp', '▁More', '。', '▁meter', '▁Screen', '▁P', '▁Mediabestanden', '▁M', '▁G', 'Ma']\n",
      "Projected suffix as text: 'Sp More。 meter Screen P Mediabestanden M GMa'\n",
      "suffix loss (before projection): 3.968155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suffix loss (after  projection): 4.770502\n",
      "\n",
      "Generated text with suffix (projected):\n",
      "List three sites of historical importance in London. Sp More。 meter Screen P Mediabestanden M GMaP. The same way that you can't just go to a store and buy a car. You have to go through a dealership.\n",
      "\n",
      "The same way that you can't just go to a store and buy a house. You have to go through a realtor.\n",
      "\n",
      "The same way\n",
      "\n",
      "Per-suffix-token next-token probabilities:\n",
      "  pos  0, token  1706 ('Sp'), prob: 0.000304\n",
      "  pos  1, token  5853 ('More'), prob: 0.000000\n",
      "  pos  2, token 30267 ('。'), prob: 0.000000\n",
      "  pos  3, token 11134 ('meter'), prob: 0.000000\n",
      "  pos  4, token 22666 ('Screen'), prob: 0.000000\n",
      "  pos  5, token   349 ('P'), prob: 0.000282\n",
      "  pos  6, token 28574 ('Mediabestanden'), prob: 0.000000\n",
      "  pos  7, token   341 ('M'), prob: 0.001537\n",
      "  pos  8, token   402 ('G'), prob: 0.005203\n",
      "  pos  9, token 21870 ('Ma'), prob: 0.000005\n",
      "####################\n",
      "\n",
      "=== round 3, epoch 3 ===\n",
      "Read /kaggle/working/hotflip/rounds/suffix_r3_e3.pt successfully. shape=(10, 4096)\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  2.994326\n",
      "  max:  7.270593\n",
      "  mean: 4.788906\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.278714\n",
      "  max:  0.653572\n",
      "  mean: 0.436697\n",
      "\n",
      "Projected discrete suffix token IDs: [6417, 22900, 30267, 11134, 22666, 349, 28574, 341, 341, 21870]\n",
      "Projected discrete suffix tokens: ['▁Rob', '▁Nation', '。', '▁meter', '▁Screen', '▁P', '▁Mediabestanden', '▁M', '▁M', 'Ma']\n",
      "Projected suffix as text: 'Rob Nation。 meter Screen P Mediabestanden M MMa'\n",
      "suffix loss (before projection): 0.731634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suffix loss (after  projection): 3.842598\n",
      "\n",
      "Generated text with suffix (projected):\n",
      "List three sites of historical importance in London. Rob Nation。 meter Screen P Mediabestanden M MMa. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Per-suffix-token next-token probabilities:\n",
      "  pos  0, token  6417 ('Rob'), prob: 0.000016\n",
      "  pos  1, token 22900 ('Nation'), prob: 0.000001\n",
      "  pos  2, token 30267 ('。'), prob: 0.000000\n",
      "  pos  3, token 11134 ('meter'), prob: 0.000001\n",
      "  pos  4, token 22666 ('Screen'), prob: 0.000001\n",
      "  pos  5, token   349 ('P'), prob: 0.001304\n",
      "  pos  6, token 28574 ('Mediabestanden'), prob: 0.000000\n",
      "  pos  7, token   341 ('M'), prob: 0.001548\n",
      "  pos  8, token   341 ('M'), prob: 0.012360\n",
      "  pos  9, token 21870 ('Ma'), prob: 0.000006\n",
      "####################\n",
      "\n",
      "=== round 3, epoch 4 ===\n",
      "Read /kaggle/working/hotflip/rounds/suffix_r3_e4.pt successfully. shape=(10, 4096)\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  3.011821\n",
      "  max:  7.711347\n",
      "  mean: 4.978092\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.390002\n",
      "  max:  0.711720\n",
      "  mean: 0.551538\n",
      "\n",
      "Projected discrete suffix token IDs: [1706, 22900, 30267, 11134, 22666, 349, 28574, 341, 341, 21870]\n",
      "Projected discrete suffix tokens: ['▁Sp', '▁Nation', '。', '▁meter', '▁Screen', '▁P', '▁Mediabestanden', '▁M', '▁M', 'Ma']\n",
      "Projected suffix as text: 'Sp Nation。 meter Screen P Mediabestanden M MMa'\n",
      "suffix loss (before projection): 2.533886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suffix loss (after  projection): 3.727769\n",
      "\n",
      "Generated text with suffix (projected):\n",
      "List three sites of historical importance in London. Sp Nation。 meter Screen P Mediabestanden M MMa. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Per-suffix-token next-token probabilities:\n",
      "  pos  0, token  1706 ('Sp'), prob: 0.000304\n",
      "  pos  1, token 22900 ('Nation'), prob: 0.000000\n",
      "  pos  2, token 30267 ('。'), prob: 0.000001\n",
      "  pos  3, token 11134 ('meter'), prob: 0.000001\n",
      "  pos  4, token 22666 ('Screen'), prob: 0.000001\n",
      "  pos  5, token   349 ('P'), prob: 0.001127\n",
      "  pos  6, token 28574 ('Mediabestanden'), prob: 0.000000\n",
      "  pos  7, token   341 ('M'), prob: 0.001575\n",
      "  pos  8, token   341 ('M'), prob: 0.012230\n",
      "  pos  9, token 21870 ('Ma'), prob: 0.000013\n",
      "####################\n",
      "\n",
      "=== round 3, epoch 5 ===\n",
      "Read /kaggle/working/hotflip/rounds/suffix_r3_e5.pt successfully. shape=(10, 4096)\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  3.014580\n",
      "  max:  7.751566\n",
      "  mean: 4.945700\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.424608\n",
      "  max:  0.713769\n",
      "  mean: 0.570905\n",
      "\n",
      "Projected discrete suffix token IDs: [1706, 5853, 30267, 11134, 22666, 349, 28574, 341, 341, 21870]\n",
      "Projected discrete suffix tokens: ['▁Sp', '▁More', '。', '▁meter', '▁Screen', '▁P', '▁Mediabestanden', '▁M', '▁M', 'Ma']\n",
      "Projected suffix as text: 'Sp More。 meter Screen P Mediabestanden M MMa'\n",
      "suffix loss (before projection): 3.657679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suffix loss (after  projection): 3.809343\n",
      "\n",
      "Generated text with suffix (projected):\n",
      "List three sites of historical importance in London. Sp More。 meter Screen P Mediabestanden M MMa M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M M\n",
      "\n",
      "Per-suffix-token next-token probabilities:\n",
      "  pos  0, token  1706 ('Sp'), prob: 0.000304\n",
      "  pos  1, token  5853 ('More'), prob: 0.000000\n",
      "  pos  2, token 30267 ('。'), prob: 0.000000\n",
      "  pos  3, token 11134 ('meter'), prob: 0.000000\n",
      "  pos  4, token 22666 ('Screen'), prob: 0.000000\n",
      "  pos  5, token   349 ('P'), prob: 0.000282\n",
      "  pos  6, token 28574 ('Mediabestanden'), prob: 0.000000\n",
      "  pos  7, token   341 ('M'), prob: 0.001537\n",
      "  pos  8, token   341 ('M'), prob: 0.014999\n",
      "  pos  9, token 21870 ('Ma'), prob: 0.000003\n",
      "####################\n",
      "\n",
      "=== round 3, epoch 6 ===\n",
      "Read /kaggle/working/hotflip/rounds/suffix_r3_e6.pt successfully. shape=(10, 4096)\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  2.997456\n",
      "  max:  7.767632\n",
      "  mean: 4.901523\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.446647\n",
      "  max:  0.715452\n",
      "  mean: 0.579850\n",
      "\n",
      "Projected discrete suffix token IDs: [1706, 5853, 30267, 11134, 22666, 349, 28574, 341, 323, 21870]\n",
      "Projected discrete suffix tokens: ['▁Sp', '▁More', '。', '▁meter', '▁Screen', '▁P', '▁Mediabestanden', '▁M', '▁T', 'Ma']\n",
      "Projected suffix as text: 'Sp More。 meter Screen P Mediabestanden M TMa'\n",
      "suffix loss (before projection): 4.217578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suffix loss (after  projection): 4.915522\n",
      "\n",
      "Generated text with suffix (projected):\n",
      "List three sites of historical importance in London. Sp More。 meter Screen P Mediabestanden M TMa.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Per-suffix-token next-token probabilities:\n",
      "  pos  0, token  1706 ('Sp'), prob: 0.000304\n",
      "  pos  1, token  5853 ('More'), prob: 0.000000\n",
      "  pos  2, token 30267 ('。'), prob: 0.000000\n",
      "  pos  3, token 11134 ('meter'), prob: 0.000000\n",
      "  pos  4, token 22666 ('Screen'), prob: 0.000000\n",
      "  pos  5, token   349 ('P'), prob: 0.000282\n",
      "  pos  6, token 28574 ('Mediabestanden'), prob: 0.000000\n",
      "  pos  7, token   341 ('M'), prob: 0.001537\n",
      "  pos  8, token   323 ('T'), prob: 0.009132\n",
      "  pos  9, token 21870 ('Ma'), prob: 0.000004\n",
      "####################\n",
      "\n",
      "=== round 3, epoch 7 ===\n",
      "Read /kaggle/working/hotflip/rounds/suffix_r3_e7.pt successfully. shape=(10, 4096)\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  2.985476\n",
      "  max:  7.783875\n",
      "  mean: 4.854937\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.460951\n",
      "  max:  0.716417\n",
      "  mean: 0.584340\n",
      "\n",
      "Projected discrete suffix token IDs: [1706, 5853, 30267, 11134, 22666, 349, 28574, 341, 323, 21870]\n",
      "Projected discrete suffix tokens: ['▁Sp', '▁More', '。', '▁meter', '▁Screen', '▁P', '▁Mediabestanden', '▁M', '▁T', 'Ma']\n",
      "Projected suffix as text: 'Sp More。 meter Screen P Mediabestanden M TMa'\n",
      "suffix loss (before projection): 3.710084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suffix loss (after  projection): 4.915522\n",
      "\n",
      "Generated text with suffix (projected):\n",
      "List three sites of historical importance in London. Sp More。 meter Screen P Mediabestanden M TMa.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Per-suffix-token next-token probabilities:\n",
      "  pos  0, token  1706 ('Sp'), prob: 0.000304\n",
      "  pos  1, token  5853 ('More'), prob: 0.000000\n",
      "  pos  2, token 30267 ('。'), prob: 0.000000\n",
      "  pos  3, token 11134 ('meter'), prob: 0.000000\n",
      "  pos  4, token 22666 ('Screen'), prob: 0.000000\n",
      "  pos  5, token   349 ('P'), prob: 0.000282\n",
      "  pos  6, token 28574 ('Mediabestanden'), prob: 0.000000\n",
      "  pos  7, token   341 ('M'), prob: 0.001537\n",
      "  pos  8, token   323 ('T'), prob: 0.009132\n",
      "  pos  9, token 21870 ('Ma'), prob: 0.000004\n",
      "####################\n",
      "\n",
      "=== round 3, epoch 8 ===\n",
      "Read /kaggle/working/hotflip/rounds/suffix_r3_e8.pt successfully. shape=(10, 4096)\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  2.962456\n",
      "  max:  7.777992\n",
      "  mean: 4.815560\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.470037\n",
      "  max:  0.716799\n",
      "  mean: 0.588130\n",
      "\n",
      "Projected discrete suffix token IDs: [1706, 5853, 30267, 11134, 22666, 349, 28574, 341, 323, 21870]\n",
      "Projected discrete suffix tokens: ['▁Sp', '▁More', '。', '▁meter', '▁Screen', '▁P', '▁Mediabestanden', '▁M', '▁T', 'Ma']\n",
      "Projected suffix as text: 'Sp More。 meter Screen P Mediabestanden M TMa'\n",
      "suffix loss (before projection): 4.384400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suffix loss (after  projection): 4.915522\n",
      "\n",
      "Generated text with suffix (projected):\n",
      "List three sites of historical importance in London. Sp More。 meter Screen P Mediabestanden M TMa.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Per-suffix-token next-token probabilities:\n",
      "  pos  0, token  1706 ('Sp'), prob: 0.000304\n",
      "  pos  1, token  5853 ('More'), prob: 0.000000\n",
      "  pos  2, token 30267 ('。'), prob: 0.000000\n",
      "  pos  3, token 11134 ('meter'), prob: 0.000000\n",
      "  pos  4, token 22666 ('Screen'), prob: 0.000000\n",
      "  pos  5, token   349 ('P'), prob: 0.000282\n",
      "  pos  6, token 28574 ('Mediabestanden'), prob: 0.000000\n",
      "  pos  7, token   341 ('M'), prob: 0.001537\n",
      "  pos  8, token   323 ('T'), prob: 0.009132\n",
      "  pos  9, token 21870 ('Ma'), prob: 0.000004\n",
      "####################\n",
      "\n",
      "=== round 3, epoch 9 ===\n",
      "Read /kaggle/working/hotflip/rounds/suffix_r3_e9.pt successfully. shape=(10, 4096)\n",
      "L2 distance between optimized embeddings and nearest token embeddings:\n",
      "  min:  2.738678\n",
      "  max:  7.797936\n",
      "  mean: 4.762078\n",
      "Cosine similarity of optimized embeddings to nearest tokens:\n",
      "  min:  0.471388\n",
      "  max:  0.713418\n",
      "  mean: 0.588592\n",
      "\n",
      "Projected discrete suffix token IDs: [1706, 5853, 30267, 11134, 22666, 349, 28574, 341, 323, 21870]\n",
      "Projected discrete suffix tokens: ['▁Sp', '▁More', '。', '▁meter', '▁Screen', '▁P', '▁Mediabestanden', '▁M', '▁T', 'Ma']\n",
      "Projected suffix as text: 'Sp More。 meter Screen P Mediabestanden M TMa'\n",
      "suffix loss (before projection): 2.630417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suffix loss (after  projection): 4.915522\n",
      "\n",
      "Generated text with suffix (projected):\n",
      "List three sites of historical importance in London. Sp More。 meter Screen P Mediabestanden M TMa.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Per-suffix-token next-token probabilities:\n",
      "  pos  0, token  1706 ('Sp'), prob: 0.000304\n",
      "  pos  1, token  5853 ('More'), prob: 0.000000\n",
      "  pos  2, token 30267 ('。'), prob: 0.000000\n",
      "  pos  3, token 11134 ('meter'), prob: 0.000000\n",
      "  pos  4, token 22666 ('Screen'), prob: 0.000000\n",
      "  pos  5, token   349 ('P'), prob: 0.000282\n",
      "  pos  6, token 28574 ('Mediabestanden'), prob: 0.000000\n",
      "  pos  7, token   341 ('M'), prob: 0.001537\n",
      "  pos  8, token   323 ('T'), prob: 0.009132\n",
      "  pos  9, token 21870 ('Ma'), prob: 0.000004\n",
      "####################\n"
     ]
    }
   ],
   "source": [
    "from itertools import islice\n",
    "\n",
    "device = next(model.parameters()).device\n",
    "emb_layer = model.get_input_embeddings()\n",
    "\n",
    "# Get one batch iterator for prompts\n",
    "data_iter = iter(dataloader)\n",
    "batch = next(data_iter)\n",
    "prompts = batch[\"input_ids\"]  # list of 1D tensors\n",
    "prompt_ids = prompts[0]       # pick first prompt\n",
    "print(\"Using this prompt for qualitative checks:\")\n",
    "print(\"  \", tokenizer.decode(prompt_ids, skip_special_tokens=True))\n",
    "print()\n",
    "\n",
    "with torch.no_grad():\n",
    "    E_cpu = model.get_input_embeddings().weight.detach().cpu().float()  # (V, E)\n",
    "    E_norm_cpu = F.normalize(E_cpu, dim=-1)  # (V, E), fp32 on CPU\n",
    "\n",
    "for i in range(4):      # rounds\n",
    "    for j in range(10): # epochs/checkpoints per round\n",
    "        path = f\"/kaggle/working/hotflip/rounds/trojai-0/suffix_saves/suffix_r{i}_e{j}.pt\"\n",
    "\n",
    "        try:\n",
    "            suffix_z = read_suffix_pt(path)  # (Ls, E) on CPU\n",
    "        except FileNotFoundError:\n",
    "            print(f\"[WARN] Missing file: {path}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n=== round {i}, epoch {j} ===\")\n",
    "        print(f\"Read {path} successfully. shape={tuple(suffix_z.shape)}\")\n",
    "\n",
    "        # Project to tokens + diagnostics\n",
    "        suffix_token_ids = project_suffix_to_tokens_and_diagnostics(\n",
    "            suffix_z,\n",
    "            emb_layer,\n",
    "            tokenizer,\n",
    "            model,\n",
    "            prompt_ids,\n",
    "        )[\"suffix_token_ids\"]  # LongTensor (Ls,)\n",
    "\n",
    "        # Compute loss BEFORE projection (continuous suffix)\n",
    "        batch_eos = {\"input_ids\": [torch.tensor([tokenizer.eos_token_id], dtype=torch.long)]}\n",
    "        loss_before = compute_loss_for_suffix(\n",
    "            model,\n",
    "            emb_layer,\n",
    "            batch_eos,\n",
    "            suffix_z.to(device),\n",
    "            n_tokens=10,\n",
    "            amp_dtype=torch.float16,\n",
    "            cos_reg_weight=1.0,\n",
    "            E_norm_cpu=E_norm_cpu,\n",
    "        )\n",
    "        print(f\"suffix loss (before projection): {loss_before.item():.6f}\")\n",
    "\n",
    "        # Build suffix_z AFTER projection: embeddings of discrete tokens\n",
    "        suffix_z_proj = emb_layer(suffix_token_ids.to(device))  # (Ls, E)\n",
    "\n",
    "        loss_after = compute_loss_for_suffix(\n",
    "            model,\n",
    "            emb_layer,\n",
    "            batch_eos,\n",
    "            suffix_z_proj,\n",
    "            n_tokens=10,\n",
    "            amp_dtype=torch.float16,\n",
    "            cos_reg_weight=1.0,\n",
    "            E_norm_cpu=E_norm_cpu,\n",
    "        )\n",
    "        print(f\"suffix loss (after  projection): {loss_after.item():.6f}\")\n",
    "\n",
    "        # ---- Qualitative check: generate from a real prompt + suffix ----\n",
    "        print(\"\\nGenerated text with suffix (projected):\")\n",
    "        gen_text = generate_with_suffix(\n",
    "            model,\n",
    "            tokenizer,\n",
    "            prompt_ids,\n",
    "            suffix_token_ids,\n",
    "            max_new_tokens=64,\n",
    "        )\n",
    "        print(gen_text)\n",
    "        print()\n",
    "\n",
    "        # ---- Per-suffix-token probabilities given the prompt ----\n",
    "        print(\"Per-suffix-token next-token probabilities:\")\n",
    "        log_suffix_token_probs(\n",
    "            model,\n",
    "            tokenizer,\n",
    "            prompt_ids,\n",
    "            suffix_token_ids,\n",
    "        )\n",
    "\n",
    "        print(\"####################\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7658d477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suffix_r0_e0.pt  suffix_r1_e0.pt  suffix_r2_e0.pt  suffix_r3_e0.pt\n",
      "suffix_r0_e1.pt  suffix_r1_e1.pt  suffix_r2_e1.pt  suffix_r3_e1.pt\n",
      "suffix_r0_e2.pt  suffix_r1_e2.pt  suffix_r2_e2.pt  suffix_r3_e2.pt\n",
      "suffix_r0_e3.pt  suffix_r1_e3.pt  suffix_r2_e3.pt  suffix_r3_e3.pt\n",
      "suffix_r0_e4.pt  suffix_r1_e4.pt  suffix_r2_e4.pt  suffix_r3_e4.pt\n",
      "suffix_r0_e5.pt  suffix_r1_e5.pt  suffix_r2_e5.pt  suffix_r3_e5.pt\n",
      "suffix_r0_e6.pt  suffix_r1_e6.pt  suffix_r2_e6.pt  suffix_r3_e6.pt\n",
      "suffix_r0_e7.pt  suffix_r1_e7.pt  suffix_r2_e7.pt  suffix_r3_e7.pt\n",
      "suffix_r0_e8.pt  suffix_r1_e8.pt  suffix_r2_e8.pt  suffix_r3_e8.pt\n",
      "suffix_r0_e9.pt  suffix_r1_e9.pt  suffix_r2_e9.pt  suffix_r3_e9.pt\n"
     ]
    }
   ],
   "source": [
    "!ls /kaggle/working/hotflip/rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8daabbe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working\n",
      "Cloning into 'hotflip'...\n",
      "remote: Enumerating objects: 341, done.\u001b[K\n",
      "remote: Counting objects: 100% (129/129), done.\u001b[K\n",
      "remote: Compressing objects: 100% (119/119), done.\u001b[K\n",
      "remote: Total 341 (delta 11), reused 127 (delta 10), pack-reused 212 (from 1)\u001b[K\n",
      "Receiving objects: 100% (341/341), 28.44 MiB | 21.22 MiB/s, done.\n",
      "Resolving deltas: 100% (105/105), done.\n",
      "/kaggle/working/hotflip\n",
      "From https://github.com/jefri021/hotflip\n",
      " * branch            main       -> FETCH_HEAD\n",
      "Already up to date.\n"
     ]
    }
   ],
   "source": [
    "refresh_repo()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8363949,
     "sourceId": 13197689,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
