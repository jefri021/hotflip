{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13197689,"sourceType":"datasetVersion","datasetId":8363949}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Optimize Embeddings","metadata":{"_uuid":"d495e3a7-2077-4217-9179-6872835988d6","_cell_guid":"85f2b851-1f43-4828-b7ce-c71913830a46","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"In this notebook, we're aiming to optimize embeddings directly, regardless of their values being a valid token representetive or not. We then use a similarity matrix + softmax to estimate a distribution on possible tokens for the optimized embeddings.","metadata":{"_uuid":"b30be227-5c1c-4ef2-bdfa-740471278aec","_cell_guid":"d0f4c093-c5fe-4fee-baa0-fe180de76b51","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"### Check Model","metadata":{"_uuid":"e372dddc-79e4-4aa1-b235-e77a0acaf35e","_cell_guid":"ed542c25-736e-48c1-9647-5fafb5d8bd74","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"%ls /kaggle/input/trojai-rev2-00000001/id-00000001","metadata":{"_uuid":"c8c5c7d7-8860-4f18-b34a-53556336fa7c","_cell_guid":"56b8b8c4-12dd-4b35-9465-a703ecbfb764","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-11-28T20:24:38.459467Z","iopub.execute_input":"2025-11-28T20:24:38.460271Z","iopub.status.idle":"2025-11-28T20:24:38.600990Z","shell.execute_reply.started":"2025-11-28T20:24:38.460230Z","shell.execute_reply":"2025-11-28T20:24:38.600020Z"}},"outputs":[{"name":"stdout","text":"\u001b[0m\u001b[01;34mclean-example-data\u001b[0m/         mmlu_results.json       \u001b[01;34mtokenizer\u001b[0m/\neval_generative_stats.json  \u001b[01;34mpoisoned-example-data\u001b[0m/  training_args.bin\n\u001b[01;34mfine-tuned-model\u001b[0m/           reduced-config.json     training_args.json\nground_truth.csv            round_config.json\nlog.txt                     stats.json\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"### Load Model","metadata":{"_uuid":"006ae104-fbea-41e5-aef6-f6a359596c54","_cell_guid":"5c820030-63eb-485a-9321-e016cf431a2e","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import torch\nimport json\nimport os\nimport logging\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n\ndef load_model(model_filepath: str, torch_dtype:torch.dtype=torch.float16):\n    \"\"\"Load a model given a specific model_path.\n\n    Args:\n        model_filepath: str - Path to where the model is stored\n\n    Returns:\n        model, dict, str - Torch model + dictionary representation of the model + model class name\n    \"\"\"\n\n    conf_filepath = os.path.join(model_filepath, 'reduced-config.json')\n    logging.info(\"Loading config file from: {}\".format(conf_filepath))\n    with open(conf_filepath, 'r') as fh:\n        round_config = json.load(fh)\n\n    logging.info(\"Loading model from filepath: {}\".format(model_filepath))\n    # https://huggingface.co/docs/transformers/installation#offline-mode\n    if round_config['use_lora']:\n        base_model_filepath = os.path.join(model_filepath, 'base-model')\n        logging.info(\"loading the base model (before LORA) from {}\".format(base_model_filepath))\n        model = AutoModelForCausalLM.from_pretrained(base_model_filepath, device_map = \"auto\", trust_remote_code=True, torch_dtype=torch_dtype, local_files_only=True)\n        # model = AutoModelForCausalLM.from_pretrained(round_config['model_architecture'], trust_remote_code=True, attn_implementation=\"flash_attention_2\", torch_dtype=torch_dtype)\n\n        fine_tuned_model_filepath = os.path.join(model_filepath, 'fine-tuned-model')\n        logging.info(\"loading the LORA adapter onto the base model from {}\".format(fine_tuned_model_filepath))\n        model.load_adapter(fine_tuned_model_filepath)\n    else:\n        fine_tuned_model_filepath = os.path.join(model_filepath, 'fine-tuned-model')\n        logging.info(\"Loading full fine tune checkpoint into cpu from {}\".format(fine_tuned_model_filepath))\n        model = AutoModelForCausalLM.from_pretrained(fine_tuned_model_filepath, device_map = \"auto\", trust_remote_code=True, torch_dtype=torch_dtype, local_files_only=True)\n        # model = AutoModelForCausalLM.from_pretrained(fine_tuned_model_filepath, trust_remote_code=True, attn_implementation=\"flash_attention_2\", torch_dtype=torch_dtype)\n\n    model.eval()\n\n    tokenizer_filepath = os.path.join(model_filepath, 'tokenizer')\n    tokenizer = AutoTokenizer.from_pretrained(tokenizer_filepath)\n\n    return model, tokenizer\n\n\ndef _two_gpu_max_memory(headroom_gb=2):\n    \"\"\"\n    Reserve headroom so HF sharding MUST split across both 16GB T4s.\n    \"\"\"\n    if not torch.cuda.is_available():\n        return None\n    n = torch.cuda.device_count()\n    cap = f\"{16 - headroom_gb}GiB\"  # e.g., \"14GiB\"\n    return {i: cap for i in range(n)}\n\ndef _common_from_pretrained_kwargs():\n    \"\"\"\n    Settings that reduce both CPU and GPU peak memory and use a lean attention impl.\n    \"\"\"\n    kw = dict(\n        trust_remote_code=True,\n        local_files_only=True,\n        torch_dtype=torch.float16,     # T4 → FP16\n        low_cpu_mem_usage=True,        # streaming load\n        offload_state_dict=True,       # avoid CPU spikes\n        attn_implementation=\"sdpa\",    # available by default on Kaggle\n    )\n    mm = _two_gpu_max_memory(headroom_gb=2)\n    if mm and torch.cuda.device_count() > 1:\n        kw[\"device_map\"] = \"auto\"\n        kw[\"max_memory\"] = mm\n        # Optional if host RAM is tight:\n        # kw[\"offload_folder\"] = \"/kaggle/working/offload\"\n    else:\n        kw[\"device_map\"] = {\"\": 0}\n    return kw\n\ndef load_model_and_tokenizer(model_dir: str, merge_lora: bool = True):\n    \"\"\"\n    Robust loader for full fine-tunes or LoRA adapters stored under `model_dir`.\n    Expects:\n      - reduced-config.json with {\"use_lora\": <bool>, ...}\n      - For LoRA: base-model/, fine-tuned-model/\n      - For full FT: fine-tuned-model/\n      - tokenizer/ with tokenizer files\n    Returns: (model, tokenizer)\n    \"\"\"\n    conf_path = os.path.join(model_dir, \"reduced-config.json\")\n    logging.info(f\"Loading config: {conf_path}\")\n    with open(conf_path, \"r\") as fh:\n        cfg = json.load(fh)\n\n    kw = _common_from_pretrained_kwargs()\n\n    if cfg.get(\"use_lora\", False):\n        base_dir = os.path.join(model_dir, \"base-model\")\n        lora_dir = os.path.join(model_dir, \"fine-tuned-model\")\n\n        logging.info(f\"Loading base model: {base_dir}\")\n        model = AutoModelForCausalLM.from_pretrained(base_dir, **kw)\n        logging.info(f\"Attaching LoRA adapter: {lora_dir}\")\n        # If PeftModel is missing, use .load_adapter if available\n        try:\n            model = PeftModel.from_pretrained(model, lora_dir, is_trainable=False)  # type: ignore\n        except Exception:\n            model.load_adapter(lora_dir)\n\n    else:\n        ft_dir = os.path.join(model_dir, \"fine-tuned-model\")\n        logging.info(f\"Loading full fine-tuned model: {ft_dir}\")\n        model = AutoModelForCausalLM.from_pretrained(ft_dir, **kw)\n\n    # Tokenizer hygiene\n    tok_dir = os.path.join(model_dir, \"tokenizer\")\n    tokenizer = AutoTokenizer.from_pretrained(tok_dir, use_fast=True, local_files_only=True)\n    if tokenizer.pad_token_id is None and tokenizer.eos_token_id is not None:\n        tokenizer.pad_token = tokenizer.eos_token\n    tokenizer.padding_side = \"right\"  # better for causal LMs with dynamic padding\n\n    # Runtime memory knobs for your gradient-based rollout\n    model.eval()\n    if hasattr(model.config, \"use_cache\"):\n        model.config.use_cache = False  # reduce KV/activation memory during your search\n\n    # Optional: quick sanity check of sharding\n    try:\n        print(getattr(model, \"hf_device_map\", \"no device map\"))\n    except Exception:\n        pass\n\n    return model, tokenizer\n\nmodel, tokenizer = load_model_and_tokenizer(\n    model_dir=\"/kaggle/input/trojai-rev2-00000001/id-00000001\"\n)\ntokenizer.padding_side = \"right\"","metadata":{"_uuid":"43c1c76c-c924-4533-8e28-91e12380d187","_cell_guid":"482053c7-f947-4daa-9771-7f214fb8cb22","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-11-28T20:24:38.603099Z","iopub.execute_input":"2025-11-28T20:24:38.603356Z","iopub.status.idle":"2025-11-28T20:27:11.936979Z","shell.execute_reply.started":"2025-11-28T20:24:38.603330Z","shell.execute_reply":"2025-11-28T20:27:11.936204Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n2025-11-28 20:25:03.414636: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1764361503.761502      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1764361503.878441      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30fb6d8c19dc4473829123717f9a3d83"}},"metadata":{}},{"name":"stdout","text":"{'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 0, 'model.layers.9': 0, 'model.layers.10': 0, 'model.layers.11': 0, 'model.layers.12': 0, 'model.layers.13': 0, 'model.layers.14': 0, 'model.layers.15': 0, 'model.layers.16': 1, 'model.layers.17': 1, 'model.layers.18': 1, 'model.layers.19': 1, 'model.layers.20': 1, 'model.layers.21': 1, 'model.layers.22': 1, 'model.layers.23': 1, 'model.layers.24': 1, 'model.layers.25': 1, 'model.layers.26': 1, 'model.layers.27': 1, 'model.layers.28': 1, 'model.layers.29': 1, 'model.layers.30': 1, 'model.layers.31': 1, 'model.norm': 1, 'model.rotary_emb': 1, 'lm_head': 1}\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torch.nn.utils.rnn import pad_sequence\nfrom datasets import load_dataset\n\ndef load_prompts_unpadded(tokenizer, args):\n    \"\"\"\n    Returns DataLoader where each batch is:\n      {\n        \"input_ids\": list of 1D LongTensors (prompts, no padding),\n        \"prompt_lens\": LongTensor (B,)\n      }\n    \"\"\"\n    ds = load_dataset(\"tatsu-lab/alpaca\", split=\"train\", cache_dir=args[\"data_dir\"])\n\n    # Subsample for speed\n    if \"sample_size\" in args and args[\"sample_size\"] is not None and args[\"sample_size\"] < len(ds):\n        ds = ds.shuffle(seed=42).select(range(args[\"sample_size\"]))\n\n    def collate(batch):\n        texts = [ex[\"instruction\"] for ex in batch]\n        enc = tokenizer(\n            texts,\n            padding=False,\n            truncation=True,\n            max_length=args[\"max_length\"],\n        )\n        prompts = [torch.tensor(ids, dtype=torch.long) for ids in enc[\"input_ids\"]]\n        prompt_lens = [len(p) for p in prompts]\n\n        return {\n            \"input_ids\": prompts,  # list of (Li,)\n            \"prompt_lens\": torch.tensor(prompt_lens, dtype=torch.long),\n        }\n\n    num_workers = max(2, os.cpu_count() // 2)\n    return DataLoader(\n        ds,\n        batch_size=args[\"batch_size\"],\n        shuffle=True,\n        pin_memory=True,\n        num_workers=num_workers,\n        persistent_workers=True,\n        collate_fn=collate,\n    )","metadata":{"_uuid":"82baa6b1-6aed-4e56-b92b-c6f7a51c8256","_cell_guid":"e920dfc4-351a-44fb-bed7-fd39af4920a6","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-11-28T20:27:11.937797Z","iopub.execute_input":"2025-11-28T20:27:11.938497Z","iopub.status.idle":"2025-11-28T20:27:12.970904Z","shell.execute_reply.started":"2025-11-28T20:27:11.938476Z","shell.execute_reply":"2025-11-28T20:27:12.970282Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"### Entropy Loss","metadata":{"_uuid":"d7ff0512-275d-4863-bb5e-e364092d3b09","_cell_guid":"690915cd-41f4-44f0-9cd8-4dcb0d24dc7c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nfrom torch import amp\n\ndef entropy_loss(batch_logits):\n    \"\"\"\n    batch_logits: (B, V) logits for the token of interest.\n    Returns scalar mean entropy.\n    \"\"\"\n    log_probs = F.log_softmax(batch_logits, dim=-1)\n    probs = log_probs.exp()\n    entropy = -(probs * log_probs).sum(dim=-1)  # (B,)\n    return entropy.mean()","metadata":{"_uuid":"45cc71f6-4ced-4cc0-abb2-e176d204470d","_cell_guid":"47d8270e-9447-48e9-8980-f0ce9baa5fd5","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-11-28T20:27:12.972295Z","iopub.execute_input":"2025-11-28T20:27:12.972850Z","iopub.status.idle":"2025-11-28T20:27:12.977223Z","shell.execute_reply.started":"2025-11-28T20:27:12.972829Z","shell.execute_reply":"2025-11-28T20:27:12.976591Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"### Rollout Loss","metadata":{"_uuid":"2e2a824f-b638-4eae-8651-cddfa298eb4d","_cell_guid":"0f83304f-d672-4733-8217-cdc4617b2cf4","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nfrom torch import amp\nfrom torch.nn.utils.rnn import pad_sequence\n\ndef compute_loss_for_suffix(\n    model,\n    emb_layer,\n    batch,\n    suffix_z,           # (Ls, E) nn.Parameter\n    n_tokens=10,\n    amp_dtype=torch.float16,\n    cos_reg_weight=0.1,\n    E_norm_cpu=None,    # (V, E) on CPU, fp32\n    chunk_size=1024,\n    top_k=5,\n    neg_weight=1.0,     # how strongly to push away from non-top-k\n):\n    \"\"\"\n    - For each example, build [prompt][suffix_z] in embedding space.\n    - Pad all to same length -> [prompt][suffix][PAD].\n    - Roll out n_tokens-1 tokens under inference_mode.\n    - Final forward WITH grad gives entropy loss on last generated token.\n    - Gradients flow into suffix_z only (prompts are detached).\n    - PLUS: regularizer that pulls suffix_z toward real token embeddings via cosine similarity.\n    \"\"\"\n    prompts = batch[\"input_ids\"]   # list of 1D LongTensors (Li,)\n    dev = emb_layer.weight.device\n    suffix_z = suffix_z.to(dev)    # (Ls, E)\n\n    B = len(prompts)\n    Ls, E = suffix_z.shape\n\n    base_embs = []   # each: (Li+Ls, E)\n    base_lens = []   # each: scalar length Li+Ls\n\n    # --- Build per-example [prompt][suffix] in embedding space ---\n    for p_ids in prompts:\n        p_ids_dev = p_ids.to(dev)\n        p_emb = emb_layer(p_ids_dev).detach()   # (Li, E), prompts are constants\n        base = torch.cat([p_emb, suffix_z], dim=0)  # (Li+Ls, E)\n        base_embs.append(base)\n        base_lens.append(base.size(0))\n\n    # Pad to [prompt][suffix][PAD...] across the batch\n    base = pad_sequence(base_embs, batch_first=True)   # (B, max_len, E)\n    base_lens = torch.tensor(base_lens, device=dev)    # (B,)\n    max_len = base.size(1)\n\n    # Attention mask: 1 for real tokens, 0 for pad\n    arange = torch.arange(max_len, device=dev).unsqueeze(0)  # (1, max_len)\n    base_mask = (arange < base_lens.unsqueeze(1)).long()     # (B, max_len)\n\n    # Now base has structure [prompt][suffix][PAD] per row (masked pads)\n\n    def _one_step_logits(e, m):\n        with amp.autocast(\"cuda\", dtype=amp_dtype):\n            out = model(\n                inputs_embeds=e,\n                attention_mask=m,\n                use_cache=False,\n                output_attentions=False,\n                output_hidden_states=False,\n                return_dict=True,\n            )\n        return out.logits[:, -1, :]  # (B, V)\n\n    # ---------- Rollout under no grad (from detached base) ----------\n    work_e = base.detach()  # rollout uses constants\n    work_m = base_mask\n    added_embs = []         # list of (B, E) constants\n    \n    T = max(0, n_tokens - 1)\n    with torch.inference_mode():\n        for _ in range(T):\n            logits_t = _one_step_logits(work_e, work_m)\n            probs_t  = torch.softmax(logits_t, dim=-1)\n            next_ids = torch.argmax(probs_t, dim=-1)        # (B,)\n    \n            next_emb = emb_layer(next_ids.to(dev)).detach() # (B, E)\n            added_embs.append(next_emb)\n    \n            work_e = torch.cat([work_e, next_emb.unsqueeze(1)], dim=1)\n            work_m = torch.cat(\n                [work_m, torch.ones((B, 1), dtype=work_m.dtype, device=dev)],\n                dim=1,\n            )\n    \n    # ---------- Final inputs: [prompt][suffix][PAD] + generated tokens ----------\n    if len(added_embs) > 0:\n        added = torch.stack(added_embs, dim=1)              # (B, T, E)\n        final_emb = torch.cat([base, added], dim=1)         # (B, max_len+T, E)\n        gen_mask = torch.ones((B, T), dtype=base_mask.dtype, device=dev)\n        final_mask = torch.cat([base_mask, gen_mask], dim=1)\n    else:\n        final_emb = base\n        final_mask = base_mask\n    \n    # ---------- Forward WITH grad for ALL n_tokens steps ----------\n    with amp.autocast(\"cuda\", dtype=amp_dtype):\n        out = model(\n            inputs_embeds=final_emb,\n            attention_mask=final_mask,\n            use_cache=False,\n            output_attentions=False,\n            output_hidden_states=False,\n            return_dict=True,\n        )\n    \n    logits_all = out.logits   # (B, L_total, V)\n    B, L_total, V = logits_all.shape\n    \n    # base_lens: (B,) lengths of [prompt][suffix] BEFORE generated tokens\n    # we want logits for:\n    #   step 1: position base_len - 1  (first next token)\n    #   step 2: position base_len      (second next)\n    #   ...\n    #   step n_tokens: position base_len - 1 + (n_tokens - 1) = base_len + T - 1\n    # So indices: [base_len - 1, base_len, ..., base_len + T - 1], length = n_tokens\n    \n    all_step_logits = []\n    \n    for b in range(B):\n        blen = base_lens[b].item()  # length of base for this example\n    \n        # safety: don't go past sequence length\n        # we know we have exactly T generated tokens, so there are n_tokens positions:\n        # indices from blen-1 to blen-1+T (inclusive)\n        start_idx = blen - 1\n        end_idx   = blen - 1 + T    # inclusive\n        # this yields exactly n_tokens positions when T = n_tokens-1\n    \n        idxs = torch.arange(start_idx, end_idx + 1, device=dev)  # (n_tokens,)\n        # gather logits for this example's steps: (n_tokens, V)\n        step_logits_b = logits_all[b, idxs, :]                   # (n_tokens, V)\n        all_step_logits.append(step_logits_b)\n    \n    # stack over batch: (B, n_tokens, V) -> (B*n_tokens, V)\n    logits_for_loss = torch.cat(all_step_logits, dim=0)  # (B*n_tokens, V)\n\n    # print(f\"hey: {logits_for_loss.shape}\")\n    \n    # mean entropy over all n_tokens steps for all examples\n    ent = entropy_loss(logits_for_loss)\n\n    \n    dev = suffix_z.device\n    Ls = suffix_z.size(0)\n    V = E_norm_cpu.size(0)\n    \n    # normalized suffix embeddings on GPU, fp32, with grad\n    z_norm = F.normalize(suffix_z.float(), dim=-1)  # (Ls, E)\n    \n    # running top-2 cosines across vocab, per suffix position\n    top2_vals = None  # (2, Ls)\n    \n    for start in range(0, V, chunk_size):\n        end = min(start + chunk_size, V)\n        # chunk: (c, E) fp32 on GPU, no grad\n        chunk = E_norm_cpu[start:end].to(dev, non_blocking=True)  # (c, E)\n    \n        # (c, E) @ (E, Ls) -> (c, Ls)\n        chunk_sim = torch.matmul(chunk, z_norm.T)  # (c, Ls)\n    \n        # top-2 within this chunk: (2, Ls)  (if c < 2, topk handles it)\n        chunk_top2, _ = chunk_sim.topk(\n            k=min(2, chunk_sim.size(0)), dim=0\n        )  # (k', Ls)\n    \n        if top2_vals is None:\n            # if first chunk smaller than 2, pad with very low values\n            if chunk_top2.size(0) < 2:\n                pad_rows = 2 - chunk_top2.size(0)\n                pad = torch.full(\n                    (pad_rows, Ls),\n                    -1e9,\n                    device=dev,\n                    dtype=chunk_top2.dtype,\n                )\n                top2_vals = torch.cat([chunk_top2, pad], dim=0)  # (2, Ls)\n            else:\n                top2_vals = chunk_top2  # (2, Ls)\n        else:\n            # combine global and chunk top2, then keep best 2 across all\n            combined = torch.cat([top2_vals, chunk_top2], dim=0)  # (2 + k', Ls)\n            top2_vals, _ = combined.topk(k=2, dim=0)              # (2, Ls)\n    \n        # free small temps\n        del chunk, chunk_sim, chunk_top2\n    \n    # Now top2_vals[0] = global best cosine; top2_vals[1] = global second-best\n    top1 = top2_vals[0]   # (Ls,)\n    top2 = top2_vals[1]   # (Ls,)\n    \n    # margin per position: encourage top1 >> top2\n    margin_per_pos = top1 - top2  # (Ls,)\n    mean_margin = margin_per_pos.mean()            # scalar\n    \n    # We want to MAXIMIZE mean_margin → in minimization, use negative margin\n    margin_reg = -mean_margin\n    \n    cos_reg = margin_reg  # you can scale it directly with cos_reg_weight below\n    \n    total_loss = ent + cos_reg_weight * cos_reg\n    \n    return total_loss\n","metadata":{"_uuid":"d4d5f659-e161-4fcf-8de8-576c776decea","_cell_guid":"b27218f0-6c32-44fc-aef1-082ace3c0fff","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-11-28T20:27:12.978489Z","iopub.execute_input":"2025-11-28T20:27:12.978820Z","iopub.status.idle":"2025-11-28T20:27:12.999260Z","shell.execute_reply.started":"2025-11-28T20:27:12.978794Z","shell.execute_reply":"2025-11-28T20:27:12.998633Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"### Save Suffix Embedds","metadata":{"_uuid":"bfff2ac1-e4e7-4261-b7dd-158886637fe9","_cell_guid":"f8d079be-6203-44ae-b3c3-6f954dd8b7bf","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import os\nimport torch\n\ndef save_suffix_embeddings(suffix_z, epoch, round_idx):\n    \"\"\"\n    Save optimized suffix embeddings for tracking exploration across rounds/epochs.\n    \"\"\"\n    save_dir = \"/kaggle/working/suffix_saves\"\n    os.makedirs(save_dir, exist_ok=True)\n\n    file_path = os.path.join(save_dir, f\"suffix_r{round_idx}_e{epoch}.pt\")\n    torch.save(suffix_z.detach().cpu(), file_path)\n\n    print(f\"Saved suffix for round {round_idx}, epoch {epoch} → {file_path}\")","metadata":{"_uuid":"35c2cb80-fd14-4593-a49c-9a8094825079","_cell_guid":"2731d816-58a4-4a87-ba5b-6585628250a6","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-11-28T20:27:13.000045Z","iopub.execute_input":"2025-11-28T20:27:13.000311Z","iopub.status.idle":"2025-11-28T20:27:13.033520Z","shell.execute_reply.started":"2025-11-28T20:27:13.000288Z","shell.execute_reply":"2025-11-28T20:27:13.032755Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"### Optimization","metadata":{"_uuid":"28c11a2c-9aed-4263-b4c4-fa12d937cee4","_cell_guid":"16536c9a-f6ac-485b-b53a-3c8f8c84aa40","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"from torch.optim import Adam\nfrom torch.optim.lr_scheduler import StepLR\n\ndef optimize_suffix_embeddings(\n    model,\n    tokenizer,\n    dataloader_args,\n    suffix_len=10,\n    n_tokens_rollout=10,\n    epochs=5,\n    init_lr=1e-2,\n    rounds=10,\n    amp_dtype=torch.float16,\n):\n    \"\"\"\n    Optimize a shared suffix embedding (length suffix_len) in continuous space.\n    Returns:\n      suffix_z: (Ls, E) optimized embeddings\n      emb_layer: embedding layer (for projection)\n    \"\"\"\n    model.eval()\n    if hasattr(model.config, \"use_cache\"):\n        model.config.use_cache = False\n\n    emb_layer = model.get_input_embeddings()\n    dev = emb_layer.weight.device\n    d_model = emb_layer.weight.size(1)\n    # emb_dtype = emb_layer.weight.dtype\n    # print(f\"emb_dtype: {emb_dtype}\")\n\n    # 1.5. Load Embedding Matrix\n    with torch.no_grad():\n        E_cpu = model.get_input_embeddings().weight.detach().cpu().float()  # (V, E)\n        E_norm_cpu = F.normalize(E_cpu, dim=-1)  # (V, E), fp32 on CPU\n\n\n    best_suffix_z = None\n    best_loss = float('inf')\n\n\n    for round in range(rounds):\n        print(f\"\\n=== Optimization Round {round+1}/{rounds} ===\")\n\n        # Initialize suffix embeddings (continuous)\n        suffix_z = torch.nn.Parameter(\n            0.01 * torch.randn(suffix_len, d_model, device=dev)\n        )\n        print(f\"dtype of suffix_z: {suffix_z.dtype}\")\n\n        \n        optimizer = Adam([suffix_z], lr=init_lr)\n        scheduler = StepLR(optimizer, step_size=1, gamma=0.8)  # every epoch: lr *= 0.8\n\n        for epoch in range(epochs):\n            print(f\"\\n[Epoch {epoch+1}/{epochs}]\")\n            running_loss = 0.0\n\n            dataloader = load_prompts_unpadded(tokenizer, dataloader_args)\n\n            for batch_count, batch in enumerate(dataloader):\n                optimizer.zero_grad(set_to_none=True)\n\n                loss = compute_loss_for_suffix(\n                    model,\n                    emb_layer,\n                    batch,\n                    suffix_z,\n                    E_norm_cpu=E_norm_cpu,\n                    n_tokens=n_tokens_rollout,\n                    amp_dtype=amp_dtype,\n                    cos_reg_weight=0.5*(round+1)\n                )\n\n                # Minimize entropy → maximize confidence\n                loss.backward()\n                optimizer.step()\n\n                running_loss += loss.item()\n\n                if batch_count % 5 == 0 and batch_count > 0:\n                    avg = running_loss / batch_count\n                    print(f\"  batch {batch_count} out of {len(dataloader)}, avg loss: {avg:.4f}\", end=\"\\r\")\n\n            epoch_avg = running_loss / max(1, batch_count)\n            print(f\"Epoch {epoch+1} mean loss: {epoch_avg:.4f}\")\n            # step the scheduler once per epoch\n            scheduler.step()\n            save_suffix_embeddings(suffix_z, epoch, round)\n        \n        if epoch_avg < best_loss:\n            best_loss = epoch_avg\n            best_suffix_z = suffix_z.detach().clone()\n\n        print(f\"\\nOptimization finished for round {round+1}. Suffix found: {suffix_z.detach()}\")\n\n    return best_suffix_z, emb_layer","metadata":{"_uuid":"42a1c011-1f81-49bc-aa0c-fd5314c1ac49","_cell_guid":"45ac23b2-6547-4378-ab9e-d3c858a383bf","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-11-28T20:27:13.034261Z","iopub.execute_input":"2025-11-28T20:27:13.034474Z","iopub.status.idle":"2025-11-28T20:27:13.060069Z","shell.execute_reply.started":"2025-11-28T20:27:13.034451Z","shell.execute_reply":"2025-11-28T20:27:13.059254Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"### Projection + Diagnosis","metadata":{"_uuid":"c409c004-6563-4044-8c33-67776b35d1ec","_cell_guid":"1215b6ac-165d-4174-ab4b-9b4a27dbd481","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"def project_suffix_to_tokens_and_diagnostics(\n    suffix_z,\n    emb_layer,\n    tokenizer,\n):\n    \"\"\"\n    suffix_z: (Ls, E) - optimized continuous suffix embeddings\n    emb_layer: model.get_input_embeddings()\n    \"\"\"\n    with torch.no_grad():\n        dev = emb_layer.weight.device\n        E = emb_layer.weight        # (V, E)\n        V, d = E.shape\n\n        # Move suffix to same device\n        z = suffix_z.to(dev)        # (Ls, E)\n\n        # ---- Fix dtype mismatch: work in float32 for stability ----\n        E_f = E.float()             # (V, E) fp32\n        z_f = z.float()             # (Ls, E) fp32\n\n        # Normalize for cosine similarity\n        E_norm = F.normalize(E_f, dim=-1)        # (V, E)\n        z_norm = F.normalize(z_f, dim=-1)        # (Ls, E)\n\n        # Cosine similarity: (V, E) @ (E, Ls) -> (V, Ls)\n        cos_sim = torch.matmul(E_norm, z_norm.T)  # (V, Ls)\n\n        # For each suffix position, get best matching token\n        best_token_ids = cos_sim.argmax(dim=0)    # (Ls,)\n\n        # Diagnostics: L2 distances between z[i] and E[best_token_ids[i]]\n        nearest_embs = E_f[best_token_ids]        # (Ls, E) fp32\n        l2_dists = (z_f - nearest_embs).norm(dim=-1)  # (Ls,)\n\n        print(\"L2 distance between optimized embeddings and nearest token embeddings:\")\n        print(f\"  min:  {l2_dists.min().item():.6f}\")\n        print(f\"  max:  {l2_dists.max().item():.6f}\")\n        print(f\"  mean: {l2_dists.mean().item():.6f}\")\n\n        best_cos = cos_sim.max(dim=0).values     # (Ls,)\n        print(\"Cosine similarity of optimized embeddings to nearest tokens:\")\n        print(f\"  min:  {best_cos.min().item():.6f}\")\n        print(f\"  max:  {best_cos.max().item():.6f}\")\n        print(f\"  mean: {best_cos.mean().item():.6f}\")\n\n        suffix_token_ids = best_token_ids.cpu()\n        suffix_tokens = tokenizer.convert_ids_to_tokens(suffix_token_ids.tolist())\n        suffix_text = tokenizer.decode(\n            suffix_token_ids.tolist(),\n            skip_special_tokens=False\n        )\n\n        print(\"\\nProjected discrete suffix token IDs:\", suffix_token_ids.tolist())\n        print(\"Projected discrete suffix tokens:\", suffix_tokens)\n        print(\"Projected suffix as text:\", repr(suffix_text))\n\n        return suffix_token_ids","metadata":{"_uuid":"6f31da4e-48dc-46d3-89fb-2d5855370560","_cell_guid":"716c98c7-dfee-471f-a66d-a8e8466c7afe","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-11-28T20:27:13.060996Z","iopub.execute_input":"2025-11-28T20:27:13.061304Z","iopub.status.idle":"2025-11-28T20:27:13.083461Z","shell.execute_reply.started":"2025-11-28T20:27:13.061286Z","shell.execute_reply":"2025-11-28T20:27:13.082977Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# 1. Dataloader with [prompt] only, no suffix, unpadded\nargs = {\n    \"data_dir\": \"/kaggle/working/data\",\n    \"max_length\": 512,\n    \"batch_size\": 4,\n    \"sample_size\": 1024,\n}\n\n# 2. Optimize continuous suffix\nsuffix_len = 10\nn_tokens_rollout = 5\nepochs = 10\ninit_lr = 1e-2\nrounds = 4\n\nsuffix_z, emb_layer = optimize_suffix_embeddings(\n    model,\n    tokenizer,\n    args,\n    suffix_len=suffix_len,\n    n_tokens_rollout=n_tokens_rollout,\n    epochs=epochs,\n    init_lr=init_lr,\n    rounds=rounds,\n    amp_dtype=torch.float16,\n)\n\n# 3. Project to discrete tokens + diagnostics\nsuffix_token_ids = project_suffix_to_tokens_and_diagnostics(\n    suffix_z,\n    emb_layer,\n    tokenizer,\n)","metadata":{"_uuid":"ca398184-f2a2-4d29-a5d1-4bc68f9b6c8f","_cell_guid":"e8854ab5-9201-45d2-a889-a7d7c10ec1eb","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-11-28T20:27:13.084111Z","iopub.execute_input":"2025-11-28T20:27:13.084288Z","iopub.status.idle":"2025-11-28T20:27:38.084753Z","shell.execute_reply.started":"2025-11-28T20:27:13.084275Z","shell.execute_reply":"2025-11-28T20:27:38.083629Z"}},"outputs":[{"name":"stdout","text":"\n=== Optimization Round 1/4 ===\ndtype of suffix_z: torch.float32\n\n[Epoch 1/15]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"696e7c34545a4638adb9ac160f91417b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/train-00000-of-00001-a09b74b3ef9c3b(…):   0%|          | 0.00/24.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"befed04e67464a84aa556793128fd7f8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/52002 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0100eafcc37a495693f39cd5f444d071"}},"metadata":{}},{"name":"stdout","text":"hey: torch.Size([40, 32000])\nhey: torch.Size([40, 32000])\nhey: torch.Size([40, 32000])\nhey: torch.Size([40, 32000])\nhey: torch.Size([40, 32000])\nhey: torch.Size([40, 32000])\nhey: torch.Size([40, 32000])tropy loss: 4.3304\nhey: torch.Size([40, 32000])\nhey: torch.Size([40, 32000])\nhey: torch.Size([40, 32000])\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/1434771581.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mrounds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m suffix_z, emb_layer = optimize_suffix_embeddings(\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_47/4105707562.py\u001b[0m in \u001b[0;36moptimize_suffix_embeddings\u001b[0;34m(model, tokenizer, dataloader_args, suffix_len, n_tokens_rollout, epochs, init_lr, rounds, amp_dtype)\u001b[0m\n\u001b[1;32m     76\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m                 \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbatch_count\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m5\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbatch_count\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":9}]}