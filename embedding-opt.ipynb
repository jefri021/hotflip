{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13197689,"sourceType":"datasetVersion","datasetId":8363949}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Optimize Embeddings","metadata":{"_uuid":"d495e3a7-2077-4217-9179-6872835988d6","_cell_guid":"85f2b851-1f43-4828-b7ce-c71913830a46","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"In this notebook, we're aiming to optimize embeddings directly, regardless of their values being a valid token representetive or not. We then use a similarity matrix + softmax to estimate a distribution on possible tokens for the optimized embeddings.","metadata":{"_uuid":"b30be227-5c1c-4ef2-bdfa-740471278aec","_cell_guid":"d0f4c093-c5fe-4fee-baa0-fe180de76b51","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"### Check Model","metadata":{"_uuid":"e372dddc-79e4-4aa1-b235-e77a0acaf35e","_cell_guid":"ed542c25-736e-48c1-9647-5fafb5d8bd74","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"%ls /kaggle/input/trojai-rev2-00000001/id-00000001","metadata":{"_uuid":"c8c5c7d7-8860-4f18-b34a-53556336fa7c","_cell_guid":"56b8b8c4-12dd-4b35-9465-a703ecbfb764","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Load Model","metadata":{"_uuid":"006ae104-fbea-41e5-aef6-f6a359596c54","_cell_guid":"5c820030-63eb-485a-9321-e016cf431a2e","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import torch\nimport json\nimport os\nimport logging\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n\ndef load_model(model_filepath: str, torch_dtype:torch.dtype=torch.float16):\n    \"\"\"Load a model given a specific model_path.\n\n    Args:\n        model_filepath: str - Path to where the model is stored\n\n    Returns:\n        model, dict, str - Torch model + dictionary representation of the model + model class name\n    \"\"\"\n\n    conf_filepath = os.path.join(model_filepath, 'reduced-config.json')\n    logging.info(\"Loading config file from: {}\".format(conf_filepath))\n    with open(conf_filepath, 'r') as fh:\n        round_config = json.load(fh)\n\n    logging.info(\"Loading model from filepath: {}\".format(model_filepath))\n    # https://huggingface.co/docs/transformers/installation#offline-mode\n    if round_config['use_lora']:\n        base_model_filepath = os.path.join(model_filepath, 'base-model')\n        logging.info(\"loading the base model (before LORA) from {}\".format(base_model_filepath))\n        model = AutoModelForCausalLM.from_pretrained(base_model_filepath, device_map = \"auto\", trust_remote_code=True, torch_dtype=torch_dtype, local_files_only=True)\n        # model = AutoModelForCausalLM.from_pretrained(round_config['model_architecture'], trust_remote_code=True, attn_implementation=\"flash_attention_2\", torch_dtype=torch_dtype)\n\n        fine_tuned_model_filepath = os.path.join(model_filepath, 'fine-tuned-model')\n        logging.info(\"loading the LORA adapter onto the base model from {}\".format(fine_tuned_model_filepath))\n        model.load_adapter(fine_tuned_model_filepath)\n    else:\n        fine_tuned_model_filepath = os.path.join(model_filepath, 'fine-tuned-model')\n        logging.info(\"Loading full fine tune checkpoint into cpu from {}\".format(fine_tuned_model_filepath))\n        model = AutoModelForCausalLM.from_pretrained(fine_tuned_model_filepath, device_map = \"auto\", trust_remote_code=True, torch_dtype=torch_dtype, local_files_only=True)\n        # model = AutoModelForCausalLM.from_pretrained(fine_tuned_model_filepath, trust_remote_code=True, attn_implementation=\"flash_attention_2\", torch_dtype=torch_dtype)\n\n    model.eval()\n\n    tokenizer_filepath = os.path.join(model_filepath, 'tokenizer')\n    tokenizer = AutoTokenizer.from_pretrained(tokenizer_filepath)\n\n    return model, tokenizer\n\n\ndef _two_gpu_max_memory(headroom_gb=2):\n    \"\"\"\n    Reserve headroom so HF sharding MUST split across both 16GB T4s.\n    \"\"\"\n    if not torch.cuda.is_available():\n        return None\n    n = torch.cuda.device_count()\n    cap = f\"{16 - headroom_gb}GiB\"  # e.g., \"14GiB\"\n    return {i: cap for i in range(n)}\n\ndef _common_from_pretrained_kwargs():\n    \"\"\"\n    Settings that reduce both CPU and GPU peak memory and use a lean attention impl.\n    \"\"\"\n    kw = dict(\n        trust_remote_code=True,\n        local_files_only=True,\n        torch_dtype=torch.float16,     # T4 → FP16\n        low_cpu_mem_usage=True,        # streaming load\n        offload_state_dict=True,       # avoid CPU spikes\n        attn_implementation=\"sdpa\",    # available by default on Kaggle\n    )\n    mm = _two_gpu_max_memory(headroom_gb=2)\n    if mm and torch.cuda.device_count() > 1:\n        kw[\"device_map\"] = \"auto\"\n        kw[\"max_memory\"] = mm\n        # Optional if host RAM is tight:\n        # kw[\"offload_folder\"] = \"/kaggle/working/offload\"\n    else:\n        kw[\"device_map\"] = {\"\": 0}\n    return kw\n\ndef load_model_and_tokenizer(model_dir: str, merge_lora: bool = True):\n    \"\"\"\n    Robust loader for full fine-tunes or LoRA adapters stored under `model_dir`.\n    Expects:\n      - reduced-config.json with {\"use_lora\": <bool>, ...}\n      - For LoRA: base-model/, fine-tuned-model/\n      - For full FT: fine-tuned-model/\n      - tokenizer/ with tokenizer files\n    Returns: (model, tokenizer)\n    \"\"\"\n    conf_path = os.path.join(model_dir, \"reduced-config.json\")\n    logging.info(f\"Loading config: {conf_path}\")\n    with open(conf_path, \"r\") as fh:\n        cfg = json.load(fh)\n\n    kw = _common_from_pretrained_kwargs()\n\n    if cfg.get(\"use_lora\", False):\n        base_dir = os.path.join(model_dir, \"base-model\")\n        lora_dir = os.path.join(model_dir, \"fine-tuned-model\")\n\n        logging.info(f\"Loading base model: {base_dir}\")\n        model = AutoModelForCausalLM.from_pretrained(base_dir, **kw)\n        logging.info(f\"Attaching LoRA adapter: {lora_dir}\")\n        # If PeftModel is missing, use .load_adapter if available\n        try:\n            model = PeftModel.from_pretrained(model, lora_dir, is_trainable=False)  # type: ignore\n        except Exception:\n            model.load_adapter(lora_dir)\n\n    else:\n        ft_dir = os.path.join(model_dir, \"fine-tuned-model\")\n        logging.info(f\"Loading full fine-tuned model: {ft_dir}\")\n        model = AutoModelForCausalLM.from_pretrained(ft_dir, **kw)\n\n    # Tokenizer hygiene\n    tok_dir = os.path.join(model_dir, \"tokenizer\")\n    tokenizer = AutoTokenizer.from_pretrained(tok_dir, use_fast=True, local_files_only=True)\n    if tokenizer.pad_token_id is None and tokenizer.eos_token_id is not None:\n        tokenizer.pad_token = tokenizer.eos_token\n    tokenizer.padding_side = \"right\"  # better for causal LMs with dynamic padding\n\n    # Runtime memory knobs for your gradient-based rollout\n    model.eval()\n    if hasattr(model.config, \"use_cache\"):\n        model.config.use_cache = False  # reduce KV/activation memory during your search\n\n    # Optional: quick sanity check of sharding\n    try:\n        print(getattr(model, \"hf_device_map\", \"no device map\"))\n    except Exception:\n        pass\n\n    return model, tokenizer\n\nmodel, tokenizer = load_model_and_tokenizer(\n    model_dir=\"/kaggle/input/trojai-rev2-00000001/id-00000001\"\n)","metadata":{"_uuid":"43c1c76c-c924-4533-8e28-91e12380d187","_cell_guid":"482053c7-f947-4daa-9771-7f214fb8cb22","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torch.nn.utils.rnn import pad_sequence\nfrom datasets import load_dataset\n\ndef load_prompts_unpadded(tokenizer, args):\n    \"\"\"\n    Returns DataLoader where each batch is:\n      {\n        \"input_ids\": list of 1D LongTensors (prompts, no padding),\n        \"prompt_lens\": LongTensor (B,)\n      }\n    \"\"\"\n    ds = load_dataset(\"tatsu-lab/alpaca\", split=\"train\", cache_dir=args[\"data_dir\"])\n\n    # Subsample for speed\n    if \"sample_size\" in args and args[\"sample_size\"] is not None and args[\"sample_size\"] < len(ds):\n        ds = ds.shuffle(seed=42).select(range(args[\"sample_size\"]))\n\n    def collate(batch):\n        texts = [ex[\"instruction\"] for ex in batch]\n        enc = tokenizer(\n            texts,\n            padding=False,\n            truncation=True,\n            max_length=args[\"max_length\"],\n        )\n        prompts = [torch.tensor(ids, dtype=torch.long) for ids in enc[\"input_ids\"]]\n        prompt_lens = [len(p) for p in prompts]\n\n        return {\n            \"input_ids\": prompts,  # list of (Li,)\n            \"prompt_lens\": torch.tensor(prompt_lens, dtype=torch.long),\n        }\n\n    num_workers = max(2, os.cpu_count() // 2)\n    return DataLoader(\n        ds,\n        batch_size=args[\"batch_size\"],\n        shuffle=True,\n        pin_memory=True,\n        num_workers=num_workers,\n        persistent_workers=True,\n        collate_fn=collate,\n    )","metadata":{"_uuid":"82baa6b1-6aed-4e56-b92b-c6f7a51c8256","_cell_guid":"e920dfc4-351a-44fb-bed7-fd39af4920a6","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Entropy Loss","metadata":{"_uuid":"d7ff0512-275d-4863-bb5e-e364092d3b09","_cell_guid":"690915cd-41f4-44f0-9cd8-4dcb0d24dc7c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nfrom torch import amp\n\ndef entropy_loss(batch_logits):\n    \"\"\"\n    batch_logits: (B, V) logits for the token of interest.\n    Returns scalar mean entropy.\n    \"\"\"\n    log_probs = F.log_softmax(batch_logits, dim=-1)\n    probs = log_probs.exp()\n    entropy = -(probs * log_probs).sum(dim=-1)  # (B,)\n    return entropy.mean()","metadata":{"_uuid":"45cc71f6-4ced-4cc0-abb2-e176d204470d","_cell_guid":"47d8270e-9447-48e9-8980-f0ce9baa5fd5","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Rollout Loss","metadata":{"_uuid":"2e2a824f-b638-4eae-8651-cddfa298eb4d","_cell_guid":"0f83304f-d672-4733-8217-cdc4617b2cf4","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"def compute_rollout_entropy_loss_for_suffix(\n    model,\n    emb_layer,\n    batch,\n    suffix_z,           # (Ls, E) nn.Parameter\n    n_tokens=10,\n    amp_dtype=torch.float16,\n):\n    \"\"\"\n    - For each example, build [prompt][suffix_z] in embedding space.\n    - Pad all to same length -> [prompt][suffix][PAD].\n    - Roll out n_tokens-1 tokens under inference_mode.\n    - Final forward WITH grad gives entropy loss on last generated token.\n    - Gradients flow into suffix_z only (prompts are detached).\n    \"\"\"\n    prompts = batch[\"input_ids\"]   # list of 1D LongTensors (Li,)\n    dev = emb_layer.weight.device\n    suffix_z = suffix_z.to(dev)    # (Ls, E)\n\n    B = len(prompts)\n    Ls, E = suffix_z.shape\n\n    base_embs = []   # each: (Li+Ls, E)\n    base_lens = []   # each: scalar length Li+Ls\n\n    # --- Build per-example [prompt][suffix] in embedding space ---\n    for p_ids in prompts:\n        p_ids_dev = p_ids.to(dev)\n        p_emb = emb_layer(p_ids_dev).detach()   # (Li, E), prompts are constants\n        base = torch.cat([p_emb, suffix_z], dim=0)  # (Li+Ls, E)\n        base_embs.append(base)\n        base_lens.append(base.size(0))\n\n    # Pad to [prompt][suffix][PAD...] across the batch\n    base = pad_sequence(base_embs, batch_first=True)   # (B, max_len, E)\n    base_lens = torch.tensor(base_lens, device=dev)    # (B,)\n    max_len = base.size(1)\n\n    # Attention mask: 1 for real tokens, 0 for pad\n    arange = torch.arange(max_len, device=dev).unsqueeze(0)  # (1, max_len)\n    base_mask = (arange < base_lens.unsqueeze(1)).long()     # (B, max_len)\n\n    # Now base has structure [prompt][suffix][PAD] per row (masked pads)\n\n    def _one_step_logits(e, m):\n        with amp.autocast(\"cuda\", dtype=amp_dtype):\n            out = model(\n                inputs_embeds=e,\n                attention_mask=m,\n                use_cache=False,\n                output_attentions=False,\n                output_hidden_states=False,\n                return_dict=True,\n            )\n        return out.logits[:, -1, :]  # (B, V)\n\n    # ---------- Rollout under no grad (from detached base) ----------\n    work_e = base.detach()  # rollout uses constants\n    work_m = base_mask\n    added_embs = []         # list of (B, E) constants\n\n    T = max(0, n_tokens - 1)\n    with torch.inference_mode():\n        for _ in range(T):\n            logits_t = _one_step_logits(work_e, work_m)\n            probs_t = torch.softmax(logits_t, dim=-1)\n            next_ids = torch.argmax(probs_t, dim=-1)        # (B,)\n\n            next_emb = emb_layer(next_ids.to(dev)).detach() # (B, E)\n            added_embs.append(next_emb)\n\n            work_e = torch.cat([work_e, next_emb.unsqueeze(1)], dim=1)\n            work_m = torch.cat(\n                [work_m, torch.ones((B, 1), dtype=work_m.dtype, device=dev)],\n                dim=1,\n            )\n\n    # ---------- Final inputs: [prompt][suffix][PAD] + generated tokens ----------\n    if len(added_embs) > 0:\n        added = torch.stack(added_embs, dim=1)              # (B, T, E)\n        final_emb = torch.cat([base, added], dim=1)         # (B, max_len+T, E)\n        # mask: base_mask for original, ones for generated\n        gen_mask = torch.ones((B, T), dtype=base_mask.dtype, device=dev)\n        final_mask = torch.cat([base_mask, gen_mask], dim=1)\n    else:\n        final_emb = base\n        final_mask = base_mask\n\n    # ---------- Final step WITH grad (depends on suffix_z) ----------\n    logits_last = _one_step_logits(final_emb, final_mask)\n    loss = entropy_loss(logits_last)\n\n    return loss","metadata":{"_uuid":"d4d5f659-e161-4fcf-8de8-576c776decea","_cell_guid":"b27218f0-6c32-44fc-aef1-082ace3c0fff","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Save Suffix Embedds","metadata":{"_uuid":"bfff2ac1-e4e7-4261-b7dd-158886637fe9","_cell_guid":"f8d079be-6203-44ae-b3c3-6f954dd8b7bf","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import os\nimport torch\n\ndef save_suffix_embeddings(suffix_z, epoch, round_idx):\n    \"\"\"\n    Save optimized suffix embeddings for tracking exploration across rounds/epochs.\n    \"\"\"\n    save_dir = \"/kaggle/working/suffix_saves\"\n    os.makedirs(save_dir, exist_ok=True)\n\n    file_path = os.path.join(save_dir, f\"suffix_r{round_idx}_e{epoch}.pt\")\n    torch.save(suffix_z.detach().cpu(), file_path)\n\n    print(f\"Saved suffix for round {round_idx}, epoch {epoch} → {file_path}\")","metadata":{"_uuid":"35c2cb80-fd14-4593-a49c-9a8094825079","_cell_guid":"2731d816-58a4-4a87-ba5b-6585628250a6","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Optimization","metadata":{"_uuid":"28c11a2c-9aed-4263-b4c4-fa12d937cee4","_cell_guid":"16536c9a-f6ac-485b-b53a-3c8f8c84aa40","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"def optimize_suffix_embeddings(\n    model,\n    tokenizer,\n    dataloader_args,\n    suffix_len=10,\n    n_tokens_rollout=10,\n    epochs=5,\n    init_lr=1e-2,\n    rounds=10,\n    amp_dtype=torch.float16,\n):\n    \"\"\"\n    Optimize a shared suffix embedding (length suffix_len) in continuous space.\n    Returns:\n      suffix_z: (Ls, E) optimized embeddings\n      emb_layer: embedding layer (for projection)\n    \"\"\"\n    model.eval()\n    if hasattr(model.config, \"use_cache\"):\n        model.config.use_cache = False\n\n    emb_layer = model.get_input_embeddings()\n    dev = emb_layer.weight.device\n    d_model = emb_layer.weight.size(1)\n\n    best_suffix_z = None\n    best_loss = float('inf')\n\n    for round in range(rounds):\n        print(f\"\\n=== Optimization Round {round+1}/{rounds} ===\")\n\n        # Initialize suffix embeddings (continuous)\n        suffix_z = torch.nn.Parameter(\n            0.01 * torch.randn(suffix_len, d_model, device=dev)\n        )\n        optimizer = torch.optim.Adam([suffix_z], lr=lr)\n\n        for epoch in range(epochs):\n            print(f\"\\n[Epoch {epoch+1}/{epochs}]\")\n            running_loss = 0.0\n\n            dataloader = load_prompts_unpadded(tokenizer, dataloader_args)\n\n            for batch_count, batch in enumerate(dataloader):\n                optimizer.zero_grad(set_to_none=True)\n\n                loss = compute_rollout_entropy_loss_for_suffix(\n                    model,\n                    emb_layer,\n                    batch,\n                    suffix_z,\n                    n_tokens=n_tokens_rollout,\n                    amp_dtype=amp_dtype,\n                )\n\n                # Minimize entropy → maximize confidence\n                loss.backward()\n                optimizer.step()\n\n                running_loss += loss.item()\n\n                if batch_count % 5 == 0 and batch_count > 0:\n                    avg = running_loss / batch_count\n                    print(f\"  batch {batch_count} out of {len(dataloader)}, avg entropy loss: {avg:.4f}\", end=\"\\r\")\n\n            epoch_avg = running_loss / max(1, batch_count)\n            print(f\"Epoch {epoch+1} mean entropy loss: {epoch_avg:.4f}\")\n            save_suffix_embeddings(suffix_z, epoch, round)\n        \n        if epoch_avg < best_loss:\n            best_loss = epoch_avg\n            best_suffix_z = suffix_z.detach().clone()\n\n        print(f\"\\nOptimization finished for round {round+1}. Suffix found: {suffix_z.detach()}\")\n\n    return best_suffix_z, emb_layer","metadata":{"_uuid":"42a1c011-1f81-49bc-aa0c-fd5314c1ac49","_cell_guid":"45ac23b2-6547-4378-ab9e-d3c858a383bf","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Projection + Diagnosis","metadata":{"_uuid":"c409c004-6563-4044-8c33-67776b35d1ec","_cell_guid":"1215b6ac-165d-4174-ab4b-9b4a27dbd481","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"def project_suffix_to_tokens_and_diagnostics(\n    suffix_z,\n    emb_layer,\n    tokenizer,\n):\n    \"\"\"\n    suffix_z: (Ls, E) - optimized continuous suffix embeddings\n    emb_layer: model.get_input_embeddings()\n    \"\"\"\n    with torch.no_grad():\n        dev = emb_layer.weight.device\n        E = emb_layer.weight        # (V, E)\n        V, d = E.shape\n\n        # Move suffix to same device\n        z = suffix_z.to(dev)        # (Ls, E)\n\n        # ---- Fix dtype mismatch: work in float32 for stability ----\n        E_f = E.float()             # (V, E) fp32\n        z_f = z.float()             # (Ls, E) fp32\n\n        # Normalize for cosine similarity\n        E_norm = F.normalize(E_f, dim=-1)        # (V, E)\n        z_norm = F.normalize(z_f, dim=-1)        # (Ls, E)\n\n        # Cosine similarity: (V, E) @ (E, Ls) -> (V, Ls)\n        cos_sim = torch.matmul(E_norm, z_norm.T)  # (V, Ls)\n\n        # For each suffix position, get best matching token\n        best_token_ids = cos_sim.argmax(dim=0)    # (Ls,)\n\n        # Diagnostics: L2 distances between z[i] and E[best_token_ids[i]]\n        nearest_embs = E_f[best_token_ids]        # (Ls, E) fp32\n        l2_dists = (z_f - nearest_embs).norm(dim=-1)  # (Ls,)\n\n        print(\"L2 distance between optimized embeddings and nearest token embeddings:\")\n        print(f\"  min:  {l2_dists.min().item():.6f}\")\n        print(f\"  max:  {l2_dists.max().item():.6f}\")\n        print(f\"  mean: {l2_dists.mean().item():.6f}\")\n\n        best_cos = cos_sim.max(dim=0).values     # (Ls,)\n        print(\"Cosine similarity of optimized embeddings to nearest tokens:\")\n        print(f\"  min:  {best_cos.min().item():.6f}\")\n        print(f\"  max:  {best_cos.max().item():.6f}\")\n        print(f\"  mean: {best_cos.mean().item():.6f}\")\n\n        suffix_token_ids = best_token_ids.cpu()\n        suffix_tokens = tokenizer.convert_ids_to_tokens(suffix_token_ids.tolist())\n        suffix_text = tokenizer.decode(\n            suffix_token_ids.tolist(),\n            skip_special_tokens=False\n        )\n\n        print(\"\\nProjected discrete suffix token IDs:\", suffix_token_ids.tolist())\n        print(\"Projected discrete suffix tokens:\", suffix_tokens)\n        print(\"Projected suffix as text:\", repr(suffix_text))\n\n        return suffix_token_ids","metadata":{"_uuid":"6f31da4e-48dc-46d3-89fb-2d5855370560","_cell_guid":"716c98c7-dfee-471f-a66d-a8e8466c7afe","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1. Dataloader with [prompt] only, no suffix, unpadded\nargs = {\n    \"data_dir\": \"/kaggle/working/data\",\n    \"max_length\": 512,\n    \"batch_size\": 4,\n    \"sample_size\": 2048,\n}\n\n# 2. Optimize continuous suffix\nsuffix_len = 10\nn_tokens_rollout = 5\nepochs = 3\ninit_lr = 5e-3\nrounds = 5\n\nsuffix_z, emb_layer = optimize_suffix_embeddings(\n    model,\n    tokenizer,\n    args,\n    suffix_len=suffix_len,\n    n_tokens_rollout=n_tokens_rollout,\n    epochs=epochs,\n    init_lr=init_lr,\n    rounds=rounds,\n    amp_dtype=torch.float16,\n)\n\n# 3. Project to discrete tokens + diagnostics\nsuffix_token_ids = project_suffix_to_tokens_and_diagnostics(\n    suffix_z,\n    emb_layer,\n    tokenizer,\n)","metadata":{"_uuid":"ca398184-f2a2-4d29-a5d1-4bc68f9b6c8f","_cell_guid":"e8854ab5-9201-45d2-a889-a7d7c10ec1eb","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}