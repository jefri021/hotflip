{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0314dbb",
   "metadata": {},
   "source": [
    "## T-GCG With Multiple Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96835764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def refresh_repo():\n",
    "#     %cd /kaggle/working\n",
    "#     %rm -rf hotflip\n",
    "#     !git clone https://github.com/jefri021/hotflip.git\n",
    "#     %cd /kaggle/working/hotflip/\n",
    "#     !git pull origin main\n",
    "\n",
    "# refresh_repo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4717577e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mclean-example-data\u001b[0m/         mmlu_results.json       \u001b[01;34mtokenizer\u001b[0m/\n",
      "eval_generative_stats.json  \u001b[01;34mpoisoned-example-data\u001b[0m/  training_args.bin\n",
      "\u001b[01;34mfine-tuned-model\u001b[0m/           reduced-config.json     training_args.json\n",
      "ground_truth.csv            round_config.json\n",
      "log.txt                     stats.json\n"
     ]
    }
   ],
   "source": [
    "%ls /kaggle/input/trojai-rev2-00000001/id-00000001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db8d77b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "from typing import List, Union\n",
    "\n",
    "def clear_memory(keep_vars: Union[List[str], None] = None, verbose: bool = True):\n",
    "    \"\"\"\n",
    "    Clears memory while preserving specified variables.\n",
    "    Still clears GPU memory for all CUDA objects, including kept variables.\n",
    "    \n",
    "    Args:\n",
    "        keep_vars: List of variable names to preserve in memory (will still be cleared from GPU)\n",
    "        verbose: Whether to print memory clearing information\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(\"Starting memory clearing process...\")\n",
    "    \n",
    "    # Convert keep_vars to set for faster lookups\n",
    "    keep_set = set(keep_vars) if keep_vars else set()\n",
    "    \n",
    "    # First pass: Move kept CUDA variables to CPU\n",
    "    if torch.cuda.is_available():\n",
    "        for name, var in list(globals().items()):\n",
    "            if name in keep_set and isinstance(var, torch.Tensor) and var.is_cuda:\n",
    "                if verbose:\n",
    "                    print(f\"Moving kept tensor '{name}' to CPU\")\n",
    "                globals()[name] = var.cpu()\n",
    "    \n",
    "    # Clear Python garbage collector\n",
    "    gc.collect()\n",
    "    if verbose:\n",
    "        print(\"Ran Python garbage collection\")\n",
    "    \n",
    "    # Clear CUDA memory if available\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        if verbose:\n",
    "            print(\"Cleared CUDA cache\")\n",
    "            print(f\"Current CUDA memory allocated: {torch.cuda.memory_allocated()/1024**2:.2f} MB\")\n",
    "            print(f\"Current CUDA memory cached: {torch.cuda.memory_reserved()/1024**2:.2f} MB\")\n",
    "    \n",
    "    # Try to clear TensorFlow/Keras if available\n",
    "    try:\n",
    "        import tensorflow as tf\n",
    "        tf.keras.backend.clear_session()\n",
    "        if verbose:\n",
    "            print(\"Cleared TensorFlow/Keras session\")\n",
    "    except ImportError:\n",
    "        pass\n",
    "    \n",
    "    # Delete objects not in keep_vars\n",
    "    for name, var in list(globals().items()):\n",
    "        if not name.startswith('__') and name not in keep_set:\n",
    "            if isinstance(var, (torch.Tensor, torch.nn.Module)):\n",
    "                del globals()[name]\n",
    "                if verbose:\n",
    "                    print(f\"Deleted torch object: {name}\")\n",
    "            elif isinstance(var, list) and var and isinstance(var[0], torch.Tensor):\n",
    "                del globals()[name]\n",
    "                if verbose:\n",
    "                    print(f\"Deleted list of torch tensors: {name}\")\n",
    "    \n",
    "    # Final garbage collection\n",
    "    gc.collect()\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Memory clearing complete\")\n",
    "\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "import logging\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "def load_model(model_filepath: str, torch_dtype:torch.dtype=torch.float16):\n",
    "    \"\"\"Load a model given a specific model_path.\n",
    "\n",
    "    Args:\n",
    "        model_filepath: str - Path to where the model is stored\n",
    "\n",
    "    Returns:\n",
    "        model, dict, str - Torch model + dictionary representation of the model + model class name\n",
    "    \"\"\"\n",
    "\n",
    "    conf_filepath = os.path.join(model_filepath, 'reduced-config.json')\n",
    "    logging.info(\"Loading config file from: {}\".format(conf_filepath))\n",
    "    with open(conf_filepath, 'r') as fh:\n",
    "        round_config = json.load(fh)\n",
    "\n",
    "    logging.info(\"Loading model from filepath: {}\".format(model_filepath))\n",
    "    # https://huggingface.co/docs/transformers/installation#offline-mode\n",
    "    if round_config['use_lora']:\n",
    "        base_model_filepath = os.path.join(model_filepath, 'base-model')\n",
    "        logging.info(\"loading the base model (before LORA) from {}\".format(base_model_filepath))\n",
    "        model = AutoModelForCausalLM.from_pretrained(base_model_filepath, device_map = \"auto\", trust_remote_code=True, torch_dtype=torch_dtype, local_files_only=True)\n",
    "        # model = AutoModelForCausalLM.from_pretrained(round_config['model_architecture'], trust_remote_code=True, attn_implementation=\"flash_attention_2\", torch_dtype=torch_dtype)\n",
    "\n",
    "        fine_tuned_model_filepath = os.path.join(model_filepath, 'fine-tuned-model')\n",
    "        logging.info(\"loading the LORA adapter onto the base model from {}\".format(fine_tuned_model_filepath))\n",
    "        model.load_adapter(fine_tuned_model_filepath)\n",
    "    else:\n",
    "        fine_tuned_model_filepath = os.path.join(model_filepath, 'fine-tuned-model')\n",
    "        logging.info(\"Loading full fine tune checkpoint into cpu from {}\".format(fine_tuned_model_filepath))\n",
    "        model = AutoModelForCausalLM.from_pretrained(fine_tuned_model_filepath, device_map = \"auto\", trust_remote_code=True, torch_dtype=torch_dtype, local_files_only=True)\n",
    "        # model = AutoModelForCausalLM.from_pretrained(fine_tuned_model_filepath, trust_remote_code=True, attn_implementation=\"flash_attention_2\", torch_dtype=torch_dtype)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    tokenizer_filepath = os.path.join(model_filepath, 'tokenizer')\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_filepath)\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "import os, json, logging, torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "def _two_gpu_max_memory(headroom_gb=2):\n",
    "    \"\"\"\n",
    "    Reserve headroom so HF sharding MUST split across both 16GB T4s.\n",
    "    \"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        return None\n",
    "    n = torch.cuda.device_count()\n",
    "    cap = f\"{16 - headroom_gb}GiB\"  # e.g., \"14GiB\"\n",
    "    return {i: cap for i in range(n)}\n",
    "\n",
    "def _common_from_pretrained_kwargs():\n",
    "    \"\"\"\n",
    "    Settings that reduce both CPU and GPU peak memory and use a lean attention impl.\n",
    "    \"\"\"\n",
    "    kw = dict(\n",
    "        trust_remote_code=True,\n",
    "        local_files_only=True,\n",
    "        torch_dtype=torch.float16,     # T4 → FP16\n",
    "        low_cpu_mem_usage=True,        # streaming load\n",
    "        offload_state_dict=True,       # avoid CPU spikes\n",
    "        attn_implementation=\"sdpa\",    # available by default on Kaggle\n",
    "    )\n",
    "    mm = _two_gpu_max_memory(headroom_gb=2)\n",
    "    if mm and torch.cuda.device_count() > 1:\n",
    "        kw[\"device_map\"] = \"auto\"\n",
    "        kw[\"max_memory\"] = mm\n",
    "        # Optional if host RAM is tight:\n",
    "        # kw[\"offload_folder\"] = \"/kaggle/working/offload\"\n",
    "    else:\n",
    "        kw[\"device_map\"] = {\"\": 0}\n",
    "    return kw\n",
    "\n",
    "def load_model_and_tokenizer(model_dir: str, merge_lora: bool = True):\n",
    "    \"\"\"\n",
    "    Robust loader for full fine-tunes or LoRA adapters stored under `model_dir`.\n",
    "    Expects:\n",
    "      - reduced-config.json with {\"use_lora\": <bool>, ...}\n",
    "      - For LoRA: base-model/, fine-tuned-model/\n",
    "      - For full FT: fine-tuned-model/\n",
    "      - tokenizer/ with tokenizer files\n",
    "    Returns: (model, tokenizer)\n",
    "    \"\"\"\n",
    "    conf_path = os.path.join(model_dir, \"reduced-config.json\")\n",
    "    logging.info(f\"Loading config: {conf_path}\")\n",
    "    with open(conf_path, \"r\") as fh:\n",
    "        cfg = json.load(fh)\n",
    "\n",
    "    kw = _common_from_pretrained_kwargs()\n",
    "\n",
    "    if cfg.get(\"use_lora\", False):\n",
    "        base_dir = os.path.join(model_dir, \"base-model\")\n",
    "        lora_dir = os.path.join(model_dir, \"fine-tuned-model\")\n",
    "\n",
    "        logging.info(f\"Loading base model: {base_dir}\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(base_dir, **kw)\n",
    "        logging.info(f\"Attaching LoRA adapter: {lora_dir}\")\n",
    "        # If PeftModel is missing, use .load_adapter if available\n",
    "        try:\n",
    "            model = PeftModel.from_pretrained(model, lora_dir, is_trainable=False)  # type: ignore\n",
    "        except Exception:\n",
    "            model.load_adapter(lora_dir)\n",
    "\n",
    "    else:\n",
    "        ft_dir = os.path.join(model_dir, \"fine-tuned-model\")\n",
    "        logging.info(f\"Loading full fine-tuned model: {ft_dir}\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(ft_dir, **kw)\n",
    "\n",
    "    # Tokenizer hygiene\n",
    "    tok_dir = os.path.join(model_dir, \"tokenizer\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tok_dir, use_fast=True, local_files_only=True)\n",
    "    if tokenizer.pad_token_id is None and tokenizer.eos_token_id is not None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"  # better for causal LMs with dynamic padding\n",
    "\n",
    "    # Runtime memory knobs for your gradient-based rollout\n",
    "    model.eval()\n",
    "    if hasattr(model.config, \"use_cache\"):\n",
    "        model.config.use_cache = False  # reduce KV/activation memory during your search\n",
    "\n",
    "    # Optional: quick sanity check of sharding\n",
    "    try:\n",
    "        print(getattr(model, \"hf_device_map\", \"no device map\"))\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return model, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "896bdadb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "2025-11-19 22:18:57.885721: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1763590738.106183      85 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1763590738.167687      85 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "933d8d139c3a48f487be8f723cf26381",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 0, 'model.layers.9': 0, 'model.layers.10': 0, 'model.layers.11': 0, 'model.layers.12': 0, 'model.layers.13': 0, 'model.layers.14': 0, 'model.layers.15': 0, 'model.layers.16': 1, 'model.layers.17': 1, 'model.layers.18': 1, 'model.layers.19': 1, 'model.layers.20': 1, 'model.layers.21': 1, 'model.layers.22': 1, 'model.layers.23': 1, 'model.layers.24': 1, 'model.layers.25': 1, 'model.layers.26': 1, 'model.layers.27': 1, 'model.layers.28': 1, 'model.layers.29': 1, 'model.layers.30': 1, 'model.layers.31': 1, 'model.norm': 1, 'model.rotary_emb': 1, 'lm_head': 1}\n"
     ]
    }
   ],
   "source": [
    "# from load_model import download_and_load\n",
    "# from load_model import load_model_and_tokenizer\n",
    "\n",
    "\n",
    "# model, tokenizer = download_and_load(\n",
    "#     file_id=\"1lwC9JLRu4Z4SSQwjNtetAymStPqQeaDc\",\n",
    "#     output_filename=\"model0.tar.gz\",\n",
    "#     load_model_path=\"/kaggle/tmp/id-00000000\")\n",
    "\n",
    "model, tokenizer = load_model_and_tokenizer(\n",
    "    model_dir=\"/kaggle/input/trojai-rev2-00000001/id-00000001\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41a9ecfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "\n",
    "suffix_ref = {\"ids\": None}  # will hold your current suffix\n",
    "\n",
    "def load_prompts(tokenizer, args, suffix_ref, sample_size=None):\n",
    "    ds = load_dataset(\"tatsu-lab/alpaca\", split=\"train\", cache_dir=args[\"data_dir\"])\n",
    "\n",
    "    if sample_size is not None and sample_size < len(ds):\n",
    "        ds = ds.shuffle(seed=42).select(range(sample_size))\n",
    "\n",
    "    pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    def collate(batch):\n",
    "        texts = [ex[\"instruction\"] for ex in batch]\n",
    "\n",
    "        # No padding here; we'll pad after appending suffix\n",
    "        enc = tokenizer(\n",
    "            texts,\n",
    "            padding=False,\n",
    "            truncation=True,\n",
    "            max_length=args[\"max_length\"],\n",
    "        )\n",
    "        prompts = [torch.tensor(ids, dtype=torch.long) for ids in enc[\"input_ids\"]]\n",
    "        prompt_lens = [len(p) for p in prompts]\n",
    "\n",
    "        suffix = suffix_ref[\"ids\"]\n",
    "        if suffix is None:\n",
    "            raise ValueError(\"suffix_ref['ids'] is None – set suffix before building dataloader.\")\n",
    "        suffix = suffix.to(torch.long)  # keep suffix on CPU; model will move as needed\n",
    "\n",
    "        # [prompt][suffix]\n",
    "        full_seqs = [torch.cat([p, suffix]) for p in prompts]\n",
    "\n",
    "        # Now pad: [prompt][suffix][PAD ...]\n",
    "        padded = pad_sequence(full_seqs, batch_first=True, padding_value=pad_token_id)\n",
    "        attention_mask = (padded != pad_token_id).long()\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": padded,                # (B, T) on CPU\n",
    "            \"attention_mask\": attention_mask,   # (B, T) on CPU\n",
    "            \"prompt_lens\": torch.tensor(prompt_lens, dtype=torch.long),  # (B,) on CPU\n",
    "        }\n",
    "\n",
    "    num_workers = max(2, os.cpu_count() // 2)\n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size=args[\"batch_size\"],\n",
    "        shuffle=False,\n",
    "        pin_memory=True,\n",
    "        num_workers=num_workers,\n",
    "        persistent_workers=True,\n",
    "        collate_fn=collate,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc77d78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def entropy_loss(batch_logits):\n",
    "    \"\"\"\n",
    "    batch_logits: (B, V) logits for the token we're interested in\n",
    "    Returns: scalar mean entropy\n",
    "    \"\"\"\n",
    "    log_probs = F.log_softmax(batch_logits, dim=-1)\n",
    "    probs = log_probs.exp()\n",
    "    entropy = -(probs * log_probs).sum(dim=-1)  # (B,)\n",
    "    return entropy.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65b73baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.amp import autocast\n",
    "\n",
    "def compute_rollout_entropy_loss(\n",
    "    model,\n",
    "    emb_layer,\n",
    "    base_embeddings,    # (B, L, E) incoming tensor\n",
    "    attention_mask,     # (B, L)\n",
    "    n_tokens=10,\n",
    "    amp_dtype=torch.float16,\n",
    "):\n",
    "    \"\"\"\n",
    "    - base_embeddings: embeddings for [prompt][suffix], any tensor\n",
    "    - We create a LEAF `base` (requires_grad=True) from it.\n",
    "    - Roll out (n_tokens - 1) steps under inference_mode, collecting CONSTANT embeddings.\n",
    "    - Then build final_emb = cat([base, added_constants], dim=1).\n",
    "    - Final forward uses final_emb; gradients flow only into `base`.\n",
    "    \"\"\"\n",
    "    dev = base_embeddings.device\n",
    "    B, L, E = base_embeddings.shape\n",
    "\n",
    "    # Make base a leaf\n",
    "    base = base_embeddings.detach().requires_grad_(True)  # (B, L, E)\n",
    "\n",
    "    def _one_step_logits(e, m):\n",
    "        # e: (B, cur_len, E), m: (B, cur_len)\n",
    "        with autocast(\"cuda\", dtype=amp_dtype):\n",
    "            out = model(\n",
    "                inputs_embeds=e,\n",
    "                attention_mask=m,\n",
    "                use_cache=False,\n",
    "                output_attentions=False,\n",
    "                output_hidden_states=False,\n",
    "                return_dict=True,\n",
    "            )\n",
    "        # logits for next-token distribution at last position\n",
    "        return out.logits[:, -1, :]  # (B, V)\n",
    "\n",
    "    # ---------- Rollout under no grad ----------\n",
    "    work_e = base           # starts as base, but ops in inference_mode don't build graph\n",
    "    work_m = attention_mask\n",
    "    added_embs = []         # list of (B, E) constants\n",
    "\n",
    "    T = max(0, n_tokens - 1)\n",
    "    with torch.inference_mode():\n",
    "        for _ in range(T):\n",
    "            logits_t = _one_step_logits(work_e, work_m)  # no grad\n",
    "            probs_t  = torch.softmax(logits_t, dim=-1)   # (B, V), no grad\n",
    "\n",
    "            # greedy choice for rollout (can also sample)\n",
    "            next_ids = torch.argmax(probs_t, dim=-1)     # (B,)\n",
    "\n",
    "            # embedding of next tokens as a constant\n",
    "            next_emb = emb_layer(next_ids.to(dev))       # (B, E)\n",
    "            next_emb = next_emb.detach()                 # explicitly detach\n",
    "\n",
    "            added_embs.append(next_emb)\n",
    "\n",
    "            # extend working sequence/mask\n",
    "            work_e = torch.cat([work_e, next_emb.unsqueeze(1)], dim=1)\n",
    "            work_m = torch.cat(\n",
    "                [work_m, torch.ones((B, 1), dtype=work_m.dtype, device=dev)],\n",
    "                dim=1\n",
    "            )\n",
    "\n",
    "    # ---------- Build final inputs: only base is differentiable ----------\n",
    "    if len(added_embs) > 0:\n",
    "        # (B, T, E) constants\n",
    "        added = torch.stack(added_embs, dim=1)\n",
    "        # cat([base (leaf), added (const)], dim=1) -> final_emb depends on base\n",
    "        final_emb = torch.cat([base, added], dim=1)   # (B, L+T, E)\n",
    "        final_msk = work_m                            # mask can be treated as const\n",
    "    else:\n",
    "        raise RuntimeError(\"No added embeddings but n_tokens > 1\")\n",
    "        # final_emb = base\n",
    "        # final_msk = attention_mask\n",
    "\n",
    "    # ---------- Final step WITH grad ----------\n",
    "    logits_last = _one_step_logits(final_emb, final_msk)  # graph includes base\n",
    "    loss = entropy_loss(logits_last)                      # scalar\n",
    "\n",
    "    return loss, base      # base is the leaf you should differentiate w.r.t."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "acdd5fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_grad_for_pos_rollout(\n",
    "    model,\n",
    "    batch,\n",
    "    pos,                # suffix coordinate index (0..len_s-1)\n",
    "    n_tokens=10,\n",
    "    amp_dtype=torch.float16,\n",
    "    score_only=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    model: HF causal LM (can be sharded with device_map='auto')\n",
    "    batch: {\n",
    "        \"input_ids\": (B, L0),\n",
    "        \"attention_mask\": (B, L0),\n",
    "        \"prompt_lens\": (B,)\n",
    "    }\n",
    "    pos: which suffix coordinate we are optimizing (0-based inside suffix)\n",
    "    n_tokens: how many future tokens to generate before measuring entropy\n",
    "    \"\"\"\n",
    "    input_ids = batch[\"input_ids\"]          # CPU\n",
    "    attention_mask = batch[\"attention_mask\"]\n",
    "    prompt_lens = batch[\"prompt_lens\"]\n",
    "\n",
    "    emb_layer = model.get_input_embeddings()\n",
    "    dev = emb_layer.weight.device\n",
    "\n",
    "    # Move to embedding device\n",
    "    input_ids_dev = input_ids.to(dev)\n",
    "    attention_mask_dev = attention_mask.to(dev)\n",
    "\n",
    "    # Base embeddings for [prompt][suffix]\n",
    "    base_emb = emb_layer(input_ids_dev)  # (B, L, E)\n",
    "\n",
    "    # Rollout-based entropy loss; base is a leaf requiring grad\n",
    "    model.zero_grad(set_to_none=True)\n",
    "    loss, base = compute_rollout_entropy_loss(\n",
    "        model,\n",
    "        emb_layer,\n",
    "        base_emb,\n",
    "        attention_mask_dev,\n",
    "        n_tokens=n_tokens,\n",
    "        amp_dtype=amp_dtype,\n",
    "    )\n",
    "\n",
    "    if score_only:\n",
    "        return loss.item()\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    grads = base.grad  # (B, L, E) — this was None before; now it should be real\n",
    "    if grads is None:\n",
    "        raise RuntimeError(\"base.grad is None – graph did not connect to base.\")\n",
    "\n",
    "    B, L, E = grads.shape\n",
    "    prompt_lens_dev = prompt_lens.to(dev)\n",
    "\n",
    "    grads_at_pos = []\n",
    "    for b in range(B):\n",
    "        idx = prompt_lens_dev[b].item() + pos  # absolute index of suffix[pos]\n",
    "        if 0 <= idx < L:\n",
    "            grads_at_pos.append(grads[b, idx])\n",
    "        else:\n",
    "            raise RuntimeError(\"Sequence too short for the given position.\")\n",
    "\n",
    "    if not grads_at_pos:\n",
    "        # return torch.zeros(E, device=dev)\n",
    "        raise RuntimeError(\"No valid gradients found for the given position.\")\n",
    "\n",
    "    mean_grad = torch.stack(grads_at_pos, dim=0).mean(dim=0)  # (E,)\n",
    "    return mean_grad.detach()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4c8841d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_suffix_coordinate(model, suffix, mean_grad, pos, T, tokenizer):\n",
    "    emb_weight = model.get_input_embeddings().weight  # (V, d_model), typically fp16\n",
    "    V, d = emb_weight.shape\n",
    "\n",
    "    # 1) Move grad to same device as embeddings\n",
    "    mean_grad = mean_grad.to(emb_weight.device)\n",
    "\n",
    "    # 2) Do computations in float32 to avoid fp16 overflow/NaN\n",
    "    emb_f = emb_weight.float()            # (V, d) fp32\n",
    "    grad_f = mean_grad.float()            # (d,)   fp32\n",
    "\n",
    "    current_tok = suffix[pos].item()\n",
    "    current_emb_f = emb_f[current_tok]    # (d,) fp32\n",
    "\n",
    "    # 3) First-order approx of loss change\n",
    "    delta = emb_f - current_emb_f         # (V, d)\n",
    "    approx_delta_L = torch.matmul(delta, grad_f)  # (V,) fp32\n",
    "\n",
    "    # 4) Convert to scores (we want to minimize loss → maximize -ΔL)\n",
    "    scores = -approx_delta_L              # (V,)\n",
    "\n",
    "    # 5) Stabilize: subtract max to avoid huge exponents\n",
    "    scores = scores - scores.max()\n",
    "\n",
    "    # 6) Optional: mask special tokens\n",
    "    pad_id = tokenizer.pad_token_id\n",
    "    if pad_id is not None:\n",
    "        scores[pad_id] = -1e9\n",
    "    # you can also mask other specials if you want\n",
    "\n",
    "    # 7) Softmax with temperature\n",
    "    temp = max(float(T), 1e-5)\n",
    "    probs = torch.softmax(scores / temp, dim=-1)  # (V,)\n",
    "\n",
    "    # 8) Sanity checks: if probs are invalid, fall back to argmax\n",
    "    if (not torch.isfinite(probs).all()) or probs.sum() <= 0:\n",
    "        print(\"Warning: probs invalid, falling back to argmax\")\n",
    "        # fall back to deterministic best token\n",
    "        new_tok = scores.argmax().item()\n",
    "    # else:\n",
    "        # multinomial expects probs >= 0 and sum > 0\n",
    "    new_tok = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "    suffix[pos] = new_tok\n",
    "    return suffix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23a6c2c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suffix:  ਾaux heuresexplক seu ah anal энциклопеди\n",
      "Suffix entropy score: 3.9082\n"
     ]
    }
   ],
   "source": [
    "# \"\"\"\n",
    "# Now we perform Greedy Coordinate Descent with temperature to optimize the suffix.\n",
    "# In collate function, we will append the suffix to each prompt in the batch, and pad the sequences.\n",
    "# We mask unrelated tokens in the loss computation, only keeping the suffix tokens.\n",
    "# We use cyclic order in between epochs. Once we reach the end of the suffix, we start again from the beginning.\n",
    "# \"\"\"\n",
    "# # Hyperparameters\n",
    "# len_s = 10\n",
    "# epochs = 10\n",
    "# T = 1.0\n",
    "# n_steps_ahead = 5\n",
    "\n",
    "# # Start from a random suffix\n",
    "# suffix = torch.randint(2, tokenizer.vocab_size, (len_s,), dtype=torch.long)\n",
    "# suffix_ref[\"ids\"] = suffix\n",
    "\n",
    "# # args of dataloader\n",
    "# args = {\n",
    "#     \"data_dir\": \"/kaggle/working/data\",\n",
    "#     \"max_length\": 512,\n",
    "#     \"batch_size\": 16,\n",
    "# }\n",
    "\n",
    "\n",
    "# model.eval()\n",
    "# if hasattr(model.config, \"use_cache\"):\n",
    "#     model.config.use_cache = False\n",
    "\n",
    "# for epoch in range(epochs):\n",
    "#     print(f\"Epoch {epoch}, T={T:.4f}, suffix: {tokenizer.decode(suffix.tolist())}\")\n",
    "\n",
    "#     for pos in range(len_s):\n",
    "#         print(f\"  Optimizing suffix position {pos}\")\n",
    "#         grad_accum = None\n",
    "\n",
    "#         dataloader = load_prompts(tokenizer, args, suffix_ref, sample_size=1280)\n",
    "\n",
    "#         for batch_count, batch in enumerate(dataloader):\n",
    "#             mean_grad_batch = get_mean_grad_for_pos_rollout(\n",
    "#                 model,\n",
    "#                 batch,\n",
    "#                 pos,\n",
    "#                 n_tokens=n_steps_ahead,\n",
    "#                 amp_dtype=torch.float16,\n",
    "#             )\n",
    "\n",
    "#             if grad_accum is None:\n",
    "#                 grad_accum = mean_grad_batch\n",
    "#             else:\n",
    "#                 grad_accum = grad_accum + mean_grad_batch\n",
    "\n",
    "#             print(f\"    Processed batch {batch_count + 1} from total {len(dataloader)}\", end=\"\\r\")\n",
    "\n",
    "#         mean_grad = grad_accum / batch_count\n",
    "\n",
    "#         update_suffix_coordinate(model, suffix, mean_grad, pos, T, tokenizer)\n",
    "#         suffix_ref[\"ids\"] = suffix  # make collate see updated suffix\n",
    "#         print(f\"Next token at position {pos}: {tokenizer.decode([suffix[pos].item()])}\")\n",
    "#     print(f\"Suffix after epoch {epoch}: {tokenizer.decode(suffix.tolist())}\")\n",
    "\n",
    "#     T *= 0.8\n",
    "\n",
    "suffix = \" ਾaux heuresexplক seu ah anal энциклопеди\"\n",
    "suffix_ids = tokenizer(suffix, return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "suffix_attention_mask = torch.ones_like(suffix_ids)\n",
    "suffix_score = get_mean_grad_for_pos_rollout(\n",
    "    model, (\n",
    "        {\n",
    "            \"input_ids\": suffix_ids.unsqueeze(0),\n",
    "            \"attention_mask\": suffix_attention_mask.unsqueeze(0),\n",
    "            \"prompt_lens\": torch.tensor([0], dtype=torch.long),\n",
    "        }\n",
    "    ),\n",
    "    pos=0,\n",
    "    n_tokens=10,\n",
    "    amp_dtype=torch.float16,\n",
    "    score_only=True,\n",
    ")\n",
    "print(f\"Suffix: {suffix}\")\n",
    "print(f\"Suffix entropy score: {suffix_score:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
