{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0314dbb",
   "metadata": {},
   "source": [
    "## T-GCG With Multiple Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96835764",
   "metadata": {},
   "outputs": [],
   "source": [
    "def refresh_repo():\n",
    "    %cd /kaggle/working\n",
    "    %rm -rf hotflip\n",
    "    !git clone https://github.com/jefri021/hotflip.git\n",
    "    %cd /kaggle/working/hotflip/\n",
    "    !git pull origin main\n",
    "\n",
    "refresh_repo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896bdadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_model import download_and_load\n",
    "\n",
    "model, tokenizer = download_and_load(\n",
    "    file_id=\"1lwC9JLRu4Z4SSQwjNtetAymStPqQeaDc\",\n",
    "    output_filename=\"model0.tar.gz\",\n",
    "    load_model_path=\"/kaggle/tmp/id-00000000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a9ecfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "\n",
    "suffix_ref = {\"ids\": None}  # will hold your current suffix\n",
    "\n",
    "def load_prompts(tokenizer, args, suffix_ref):\n",
    "    ds = load_dataset(\"tatsu-lab/alpaca\", split=\"train\", cache_dir=args[\"data_dir\"])\n",
    "\n",
    "    pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    def collate(batch):\n",
    "        texts = [ex[\"instruction\"] for ex in batch]\n",
    "\n",
    "        # No padding here; we'll pad after appending suffix\n",
    "        enc = tokenizer(\n",
    "            texts,\n",
    "            padding=False,\n",
    "            truncation=True,\n",
    "            max_length=args[\"max_length\"],\n",
    "        )\n",
    "        prompts = [torch.tensor(ids, dtype=torch.long) for ids in enc[\"input_ids\"]]\n",
    "        prompt_lens = [len(p) for p in prompts]\n",
    "\n",
    "        suffix = suffix_ref[\"ids\"]\n",
    "        if suffix is None:\n",
    "            raise ValueError(\"suffix_ref['ids'] is None â€“ set suffix before building dataloader.\")\n",
    "        suffix = suffix.to(torch.long)  # keep suffix on CPU; model will move as needed\n",
    "\n",
    "        # [prompt][suffix]\n",
    "        full_seqs = [torch.cat([p, suffix]) for p in prompts]\n",
    "\n",
    "        # Now pad: [prompt][suffix][PAD ...]\n",
    "        padded = pad_sequence(full_seqs, batch_first=True, padding_value=pad_token_id)\n",
    "        attention_mask = (padded != pad_token_id).long()\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": padded,                # (B, T) on CPU\n",
    "            \"attention_mask\": attention_mask,   # (B, T) on CPU\n",
    "            \"prompt_lens\": torch.tensor(prompt_lens, dtype=torch.long),  # (B,) on CPU\n",
    "        }\n",
    "\n",
    "    num_workers = max(2, os.cpu_count() // 2)\n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size=args[\"batch_size\"],\n",
    "        shuffle=False,\n",
    "        pin_memory=True,\n",
    "        num_workers=num_workers,\n",
    "        persistent_workers=True,\n",
    "        collate_fn=collate,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3aaa232",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_collate_fn(suffix_ref, pad_token_id):\n",
    "    \"\"\"\n",
    "    Batch is a list of items from the dataset, where each item has:\n",
    "      {\"input_ids\": 1D LongTensor of prompt tokens (no pad, no suffix yet)}\n",
    "    \"\"\"\n",
    "    def collate(batch):\n",
    "        prompts = [item[\"input_ids\"] for item in batch]  # list of 1D tensors\n",
    "        prompt_lens = [len(p) for p in prompts]\n",
    "\n",
    "        suffix = suffix_ref[\"ids\"].to(prompts[0].device)  # (len_s,)\n",
    "        full_seqs = [torch.cat([p, suffix]) for p in prompts]\n",
    "\n",
    "        padded = pad_sequence(full_seqs, batch_first=True, padding_value=pad_token_id)\n",
    "        attention_mask = (padded != pad_token_id).long()\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": padded,          # (B, T)\n",
    "            \"attention_mask\": attention_mask,  # (B, T)\n",
    "            \"prompt_lens\": torch.tensor(prompt_lens, device=padded.device),  # (B,)\n",
    "        }\n",
    "\n",
    "    return collate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b73baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def loss_for_suffix_pos(logits, prompt_lens, pos, n_steps_ahead=1):\n",
    "    \"\"\"\n",
    "    logits: (B, T, V) on some device\n",
    "    prompt_lens: (B,) on same device as logits or broadcastable\n",
    "    \"\"\"\n",
    "    B, T, V = logits.shape\n",
    "    device = logits.device\n",
    "\n",
    "    probs = F.softmax(logits, dim=-1)  # (B, T, V)\n",
    "    entropies = []\n",
    "\n",
    "    for b in range(B):\n",
    "        base_idx = prompt_lens[b].item() + pos         # suffix[pos] position\n",
    "        t_idx = base_idx + (n_steps_ahead - 1)         # n-th token after\n",
    "\n",
    "        if t_idx < 0 or t_idx >= T:\n",
    "            continue\n",
    "\n",
    "        p = probs[b, t_idx]                            # (V,)\n",
    "        entropy = -(p * (p + 1e-12).log()).sum()\n",
    "        entropies.append(entropy)\n",
    "\n",
    "    if not entropies:\n",
    "        return torch.tensor(0.0, device=device)\n",
    "\n",
    "    return torch.stack(entropies).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdd5fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_grad_for_pos(model, batch, pos, n_steps_ahead=1):\n",
    "    \"\"\"\n",
    "    model: HF causal LM with device_map='auto'\n",
    "    batch: collation dict from load_prompts (all CPU)\n",
    "    pos: suffix index\n",
    "    \"\"\"\n",
    "    input_ids = batch[\"input_ids\"]          # CPU\n",
    "    attention_mask = batch[\"attention_mask\"]\n",
    "    prompt_lens = batch[\"prompt_lens\"]\n",
    "\n",
    "    model.zero_grad()\n",
    "\n",
    "    embed_layer = model.get_input_embeddings()\n",
    "    grads_holder = {}\n",
    "\n",
    "    def emb_hook(module, inp, out):\n",
    "        # out: (B, T, d_model) on some device (maybe GPU 0)\n",
    "        def out_hook(grad):\n",
    "            grads_holder[\"grads\"] = grad    # (B, T, d_model)\n",
    "        out.register_hook(out_hook)\n",
    "\n",
    "    handle = embed_layer.register_forward_hook(emb_hook)\n",
    "\n",
    "    # Forward; HF will move input_ids/attention_mask to appropriate devices internally\n",
    "    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    logits = outputs.logits    # (B, T, V) on some device\n",
    "\n",
    "    # Move prompt_lens to same device as logits\n",
    "    prompt_lens_dev = prompt_lens.to(logits.device)\n",
    "\n",
    "    loss = loss_for_suffix_pos(logits, prompt_lens_dev, pos, n_steps_ahead=n_steps_ahead)\n",
    "    loss.backward()\n",
    "\n",
    "    handle.remove()\n",
    "\n",
    "    grads = grads_holder[\"grads\"]  # (B, T, d_model) on some device\n",
    "    B, T, d = grads.shape\n",
    "\n",
    "    grads_at_pos = []\n",
    "    for b in range(B):\n",
    "        idx = prompt_lens_dev[b].item() + pos\n",
    "        if idx < T:\n",
    "            grads_at_pos.append(grads[b, idx])\n",
    "\n",
    "    if not grads_at_pos:\n",
    "        return torch.zeros(d, device=grads.device)\n",
    "\n",
    "    mean_grad = torch.stack(grads_at_pos, dim=0).mean(dim=0)  # (d_model,)\n",
    "    return mean_grad.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c8841d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_suffix_coordinate(model, suffix, mean_grad, pos, T, tokenizer):\n",
    "    emb_weight = model.get_input_embeddings().weight  # (V, d_model) on some device\n",
    "    V, d = emb_weight.shape\n",
    "\n",
    "    # Move grad to same device as embeddings if needed\n",
    "    mean_grad = mean_grad.to(emb_weight.device)\n",
    "\n",
    "    current_tok = suffix[pos].item()\n",
    "    current_emb = emb_weight[current_tok]  # (d,)\n",
    "\n",
    "    delta = emb_weight - current_emb       # (V, d)\n",
    "    approx_delta_L = torch.matmul(delta, mean_grad)  # (V,)\n",
    "    scores = -approx_delta_L\n",
    "\n",
    "    # you can mask special tokens here if you want\n",
    "    # scores[tokenizer.pad_token_id] = -1e9\n",
    "\n",
    "    probs = torch.softmax(scores / max(T, 1e-5), dim=-1)\n",
    "    new_tok = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "    suffix[pos] = new_tok\n",
    "    return suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a6c2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Now we perform Greedy Coordinate Descent with temperature to optimize the suffix.\n",
    "In collate function, we will append the suffix to each prompt in the batch, and pad the sequences.\n",
    "We mask unrelated tokens in the loss computation, only keeping the suffix tokens.\n",
    "We use cyclic order in between epochs. Once we reach the end of the suffix, we start again from the beginning.\n",
    "\"\"\"\n",
    "# Hyperparameters\n",
    "len_s = 10\n",
    "epochs = 10\n",
    "T = 1.0\n",
    "n_steps_ahead = 5\n",
    "\n",
    "# Start from a random suffix\n",
    "suffix = torch.randint(2, tokenizer.vocab_size, (len_s,), dtype=torch.long)\n",
    "suffix_ref[\"ids\"] = suffix\n",
    "\n",
    "# args of dataloader\n",
    "args = {\n",
    "    \"data_dir\": \"kaggle/working/data\",\n",
    "    \"max_length\": 512,\n",
    "    \"batch_size\": 16,\n",
    "}\n",
    "\n",
    "\n",
    "model.eval()\n",
    "if hasattr(model.config, \"use_cache\"):\n",
    "    model.config.use_cache = False\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch}, T={T:.4f}, suffix: {suffix.tolist()}\")\n",
    "\n",
    "    for pos in range(len_s):\n",
    "        print(f\"  Optimizing suffix position {pos}\")\n",
    "        grad_accum = None\n",
    "        batch_count = 0\n",
    "\n",
    "        dataloader = load_prompts(tokenizer, args, suffix_ref)\n",
    "\n",
    "        for batch in dataloader:\n",
    "            mean_grad_batch = get_mean_grad_for_pos(model, batch, pos, n_steps_ahead=n_steps_ahead)\n",
    "\n",
    "            if grad_accum is None:\n",
    "                grad_accum = mean_grad_batch\n",
    "            else:\n",
    "                grad_accum = grad_accum + mean_grad_batch\n",
    "\n",
    "            batch_count += 1\n",
    "            print(f\"    Processed batch {batch_count}\", end=\"\\r\")\n",
    "\n",
    "\n",
    "        if batch_count == 0:\n",
    "            continue\n",
    "\n",
    "        mean_grad = grad_accum / batch_count\n",
    "\n",
    "        update_suffix_coordinate(model, suffix, mean_grad, pos, T, tokenizer)\n",
    "        suffix_ref[\"ids\"] = suffix  # make collate see updated suffix\n",
    "\n",
    "    T *= 0.8"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
